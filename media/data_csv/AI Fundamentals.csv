course,sub_course,module,sub_module,content_html_list,img_list,video_url
AI Fundamentals,AI Fundamentals,Lesson 1: Welcome to the AI Fundamentals Program!,1.1  Welcome to Udacity,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Welcome to Udacity!</h2>\n<p class=""chakra-text css-o3oz8b"">We are so pleased that you\'ve chosen to start your journey with us. Our mission at Udacity is to power your career for the world\'s best tech education.</p>\n<p class=""chakra-text css-o3oz8b"">Tech is transforming the world in so many ways.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Artificial Intelligence</li><li class=""css-cvpopp"">Cloud Services</li><li class=""css-cvpopp"">Mobile Apps</li></ul>\n<p class=""chakra-text css-o3oz8b"">If you are curious and want to power your career, Udacity is the team that is on your side.</p>\n<p class=""chakra-text css-o3oz8b"">Welcome again and we can\'t wait to see where this journey will take you.</p></div>']",[],https://www.youtube.com/embed/9QadFJRKrEA
AI Fundamentals,AI Fundamentals,Lesson 1: Welcome to the AI Fundamentals Program!,1.2  Career chat with your Instructors,[],[],https://www.youtube.com/embed/iaeNwg6HMRA
AI Fundamentals,AI Fundamentals,Lesson 1: Welcome to the AI Fundamentals Program!,1.3  How to Succeed,[],[],https://www.youtube.com/embed/hVwUijsLKzw
AI Fundamentals,AI Fundamentals,Lesson 2: Introduction to AI Fundamentals with Azure,2.1  Meet Your Instructor,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Welcome to the AI Fundamentals course! Your instructors in this course are <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.linkedin.com/in/ciprianjichici/"">Ciprian Jichici<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.linkedin.com/in/daronyondem/"">Daron Yondem<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.linkedin.com/in/kevin-feasel-5047167/"">Kevin Feasel<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, and <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.linkedin.com/in/kylebunting/"">Kyle Bunting<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>. In this next video, you can learn a little about your instructors\' background and how they use AI and ML in their work.</p></div>']",[],https://www.youtube.com/embed/kkthAcYm_9E
AI Fundamentals,AI Fundamentals,Lesson 2: Introduction to AI Fundamentals with Azure,2.2  Introduction to AI Fundamentals,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">In this course, we will focus on Microsoft\'s two primary cloud-based AI and ML services. Those are Azure Machine Learning service and Azure Cognitive Services.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Azure Machine Learning service</strong> provides tools to build and deploy machine learning models for all skill levels.</li><li class=""css-cvpopp""><strong>Azure Cognitive Services</strong> offer specialized AI services for specific AI workloads such as Computer Vision, Prediction / Forecasting, Anomaly Detection, Natural Language Processing, and Conversational AI.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What would be a better fit if you were looking for more out-of-the-box functionality to deliver specialized services for specific AI workloads?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Machine Learning service</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Cognitive Services</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">New Terms</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Anomaly Detection:</strong> Detecting data points that do not align with a recognized pattern across a set of data points.</li><li class=""css-cvpopp""><strong>API:</strong> Application programming interfaces that are exposed to help multiple applications and systems interact with each other.</li><li class=""css-cvpopp""><strong>Azure Cognitive Services:</strong> A suite of AI Services targeting specialized AI workloads to help developers implement AI capabilities into their solution with minimal AI and ML expertise.</li><li class=""css-cvpopp""><strong>Azure Machine Learning:</strong> A cloud-based service that delivers a platform supporting the creation and deployment of Machine Learning Models.</li><li class=""css-cvpopp""><strong>Computer Vision:</strong> A domain of artificial intelligence focused on processing and understanding visual inputs through complex machine learning algorithms.</li><li class=""css-cvpopp""><strong>Intent:</strong> An intent represents the action the user wants to execute. It usually refers to a preferred action when a user delivers a sentence to be interpreted by the machine.</li><li class=""css-cvpopp""><strong>Jupyter Notebook:</strong> A virtual notebook environment where developers can write notes and combine live code and visualizations into a single document.</li><li class=""css-cvpopp""><strong>Machine Learning Model:</strong> A machine learning model is a logic that has been trained to recognize certain types of patterns.</li><li class=""css-cvpopp""><strong>Machine Learning Operations (MLOps):</strong> The practice involves the automation of processes required to build and deliver end-to-end Machine Learning Models and experiences.</li><li class=""css-cvpopp""><strong>Natural Language Processing:</strong> A domain of artificial intelligence focused on understanding human language and helping computers and humans interact through human language.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional Resources</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Visit <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/learn/modules/azure-artificial-intelligence/"">Microsoft\'s AI Strategy and Solutions learning path<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> from Microsoft if you are interested in learning more about Azure AI services and Microsoft\'s AI Vision.</li></ul></div>']",[],https://www.youtube.com/embed/P3-hqgwfLz4
AI Fundamentals,AI Fundamentals,Lesson 2: Introduction to AI Fundamentals with Azure,2.3  Course Overview,"['<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Lesson 1: Introduction to AI Fundamentals</h3>\n<p class=""chakra-text css-o3oz8b"">In this lesson we\'ll learn:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Microsoft\'s cloud-based solutions for AI and Machine Learning.</li><li class=""css-cvpopp"">The organization of an AI and ML Team and various stakeholders that take part in AI and ML initiatives.</li><li class=""css-cvpopp"">A brief history of AI</li><li class=""css-cvpopp"">The prerequisites for the rest of the course</li><li class=""css-cvpopp"">When to use AI and ML, and when not?</li></ul>\n<p class=""chakra-text css-o3oz8b"">By the end of this lesson, you\'ll be ready and prepared for the rest of the course.</p>\n<h3 class=""chakra-heading css-k57syw"">Lesson 2: AI and ML Core Concepts</h3>\n<p class=""chakra-text css-o3oz8b"">In this lesson we\'ll learn:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">The relationship between AI and ML.</li><li class=""css-cvpopp"">The five workloads for AI and ML and have an understanding of the common use cases.</li><li class=""css-cvpopp"">Responsible AI and the application of ethics to the topic of AI.</li><li class=""css-cvpopp"">Responsible ML and the ethical responsibilities of researchers, data scientists, and employees at organizations performing machine learning tasks.</li></ul>\n<p class=""chakra-text css-o3oz8b"">By the end of this lesson, you will have a good idea of the main fundamental concepts we need to be able to start using AI and ML tools in Microsoft Azure.</p>\n<h3 class=""chakra-heading css-k57syw"">Lesson 3: Machine Learning</h3>\n<p class=""chakra-text css-o3oz8b"">In this lesson we\'ll learn:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Key Machine Learning Concepts to crafting a foundation for following lessons.</li><li class=""css-cvpopp"">Different approaches to ML, such as Supervised, Unsupervised, Semi-Supervised and Reinforcement Learning</li><li class=""css-cvpopp"">Core tasks in building AI and ML solutions including Automated ML and Azure ML.</li><li class=""css-cvpopp"">An overview of Azure ML around Azure ML Designer and ML Pipelines.</li></ul>\n<p class=""chakra-text css-o3oz8b"">By the end of this lesson, you will have created three separate machine learning models using Azure Machine Learning and deployed two of them to production.</p>\n<h3 class=""chakra-heading css-k57syw"">Lesson 4: Computer Vision</h3>\n<p class=""chakra-text css-o3oz8b"">With this lesson, we will switch gears and focus on specialized AI services for specific AI and ML workloads. We\'ll learn:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Object Detection and Classification with Azure Computer Vision</li><li class=""css-cvpopp"">Image Classification with Azure Custom Vision</li><li class=""css-cvpopp"">Facial Recognition with Azure Face service.</li><li class=""css-cvpopp"">Read text (OCR) with Azure Computer Vision</li><li class=""css-cvpopp"">Extracting information from forms with Azure Form Recognizer</li></ul>\n<p class=""chakra-text css-o3oz8b"">By the end of this lesson, you will be ready to implement various capabilities of Azure Computer vision into your applications.</p>\n<h3 class=""chakra-heading css-k57syw"">Lesson 5: Natural Language Processing</h3>\n<p class=""chakra-text css-o3oz8b"">In this lesson, we\'ll focus on AI workloads that require Natural Human Language Understanding. We will learn:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Key Phrase and Entity extraction with Text Analytics to get insights about documents.</li><li class=""css-cvpopp"">Intent extraction through Language Understanding to figure out what a sentence\'s goal is.</li><li class=""css-cvpopp"">Speech recognition and synthesis of speech from text.</li><li class=""css-cvpopp"">Text and Speech Translation</li></ul>\n<p class=""chakra-text css-o3oz8b"">By the end of this lesson, you will be ready to implement various NLP capabilities of Azure Cognitive Services into your applications. Some of these capabilities will be used in the next lesson when building conversational AI systems.</p>\n<h3 class=""chakra-heading css-k57syw"">Lesson 6: Conversational AI</h3>\n<p class=""chakra-text css-o3oz8b"">In this lesson, we\'ll build a knowledge base and craft conversational experiences between users and computers. We will learn:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">What Conversational AI is.</li><li class=""css-cvpopp"">Creating a knowledge base with Azure QnA Maker</li><li class=""css-cvpopp"">Deploying an automated bot to Azure Bot Service</li><li class=""css-cvpopp"">Using Azure Bot Service channels.</li></ul>\n<p class=""chakra-text css-o3oz8b"">By the end of this lesson, you will build a chatbot backed by a knowledge base from start to finish.</p></div>']",[],https://www.youtube.com/embed/cDyKxoGa--4
AI Fundamentals,AI Fundamentals,Lesson 2: Introduction to AI Fundamentals with Azure,2.4  Stakeholders,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">Artificial Intelligence and Machine Learning are team sport. The number of roles involved in AI and ML initiatives can vary based on the organizations\' size. Most organizations start with a Machine Learning team and complement with an AI team moving forward.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Key stakeholders</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>External Stakeholders:</strong> Executives, customers, and managers from other groups in the organization.</li><li class=""css-cvpopp""><strong>Data Scientists:</strong> They identify use cases and determine the suitable datasets and algorithms for experiments. They build the AI models.</li><li class=""css-cvpopp""><strong>Data Engineers:</strong> They focus on making the data available and accessible to data scientists integrating various data sources, optimizing access to data.</li><li class=""css-cvpopp""><strong>AI Architects:</strong> They connect data scientists, data engineers, software developers, and developer operations to get the solution up and running, including the assessment of infrastructure requirements.</li><li class=""css-cvpopp""><strong>ML Engineers:</strong> Subject matter experts in their area and make sure AI models are suitable for production use.</li><li class=""css-cvpopp""><strong>AI Developer:</strong> Focuses on the development and implementation of AI solutions.</li><li class=""css-cvpopp""><strong>Data Analyst:</strong> Uses machine learning to extract insights from data.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">How do data scientists get to the data?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">They ask the executives to give them access to data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Data Engineers make the available data accessible to data scientists.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Data Analysts can provide access to data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">New Terms</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Experiment:</strong> In the context of AI and ML, experiment refers to using the AI and ML processes to see if a particular question can be answered with AI and ML, or a hypothesis can be supported or validated.</li><li class=""css-cvpopp""><strong>Dataset:</strong> A dataset is a collection of data that is used to train the machine during the AI and ML processes.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional Resources</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">If you are interested in learning more about evaluating and prioritizing AI investments in your organization, <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/learn/modules/implement-ai-organization/"">Microsoft has a learning path for you<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li></ul></div>']",['https://video.udacity-data.com/topher/2021/March/604f627c_20210315-aiml-team/20210315-aiml-team.png'],https://www.youtube.com/embed/bLAtPAwFft0
AI Fundamentals,AI Fundamentals,Lesson 2: Introduction to AI Fundamentals with Azure,2.5  History of AI and ML,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">Artificial Intelligence and Machine Learning adoption is increasing every day. The principles behind AI and ML, such as Mathematics, Economics, Neuroscience, Philosophy, Linguistics, and Computer Engineering, all have their fair share in today\'s AI and ML history.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>1943:</strong> McCulloch and Pitts created a model of a neuron. The thinking was that the ability to model a neuron would help create an artificial human brain and intelligence.</li><li class=""css-cvpopp""><strong>1950:</strong> The first Turing test was announced. A machine hidden behind a curtain was tested by a human on the side of the curtain. If the tester could not identify the identity as a machine, it would have been a success for AI. No one passed a full Turing test so far, but just limited versions.</li><li class=""css-cvpopp""><strong>1956:</strong> The first Artificial Intelligence conference in Dartmouth happened.</li><li class=""css-cvpopp""><strong>The 1960s:</strong> The first robot Shakey that incorporated robotics, computer vision, and natural language processing, was able to move boxes in a room.</li><li class=""css-cvpopp""><strong>The 1970s:</strong> Lack of computing resources resulted in a decrease in interest in AI. This period in AI History is now called AI Winter.</li><li class=""css-cvpopp""><strong>The 1980s-90s:</strong> The first autonomous trip from New York to LA was the ""No Hands Across America"". DeepBlue for Chess brought a lot of interest in AI.</li><li class=""css-cvpopp""><strong>The 2010s:</strong> AI defeated the human world champion at GO.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Considering AI\'s history, which one is a slow down in progress and a reduction of interest in AI and ML?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The effort to model the human brain.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">AI Winter</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Trying to beat humans in Chess</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The difficulty of the Turing test.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">New Terms</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Autonomous:</strong> An autonomous system is a system that can accomplish a task without human involvement during the execution of the task.</li><li class=""css-cvpopp""><strong>Go:</strong> An abstract strategy-based board game.</li><li class=""css-cvpopp""><strong>Turing Test:</strong> A test where a human tested interacts with two entities, one machine, and one human, without knowing which one is which. The goal of the test is to assess if the tester can identify which one is the machine. If the tester can not determine the machine or is not sure, the machine passes the test.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional Resources</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.youtube.com/watch?v=7bsEN8mwUB8"">Watch the history of Shakey<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, the first Robot to Embody Artificial Intelligence</li><li class=""css-cvpopp"">If you are looking for more granular details of Machine Learning History<a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://labelyourdata.com/articles/history-of-machine-learning-how-did-it-all-start/""> here is an additional timeline with milestones<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li></ul></div>']",[],https://www.youtube.com/embed/0htCBfiwVDA
AI Fundamentals,AI Fundamentals,Lesson 2: Introduction to AI Fundamentals with Azure,2.6  Prerequisites,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">You are expected to have fundamental programming knowledge in a programming language.</li><li class=""css-cvpopp"">You will need an active Azure Subscription and an Azure Resource Group. Below you can find instructions on how to get both.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Prerequisite 1: An Active Azure Subscription</h2>\n<p class=""chakra-text css-o3oz8b"">You need an active Azure Subscription to complete the lessons in this course. If you don\'t have one, you can sign up for a free trial at <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free"">https://azure.microsoft.com/free<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Prerequisite 2: Create an Azure resource group</h2>\n<p class=""chakra-text css-o3oz8b"">During the course, you will create various Azure resources to be used in the lessons. You will need an Azure Resource Group to organize the resources you create. A resource group in Azure acts as a hosting container that can hold multiple Azure resources. The following steps will help you create a resource group in Azure.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Open a web browser, navigate to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com/"">Azure portal<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, and sign in using the account associated with your Azure subscription.</li><li class=""css-cvpopp"">On the home page of the Azure portal, select <strong>Resource groups</strong> under the Azure services heading.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Resource groups</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">On the Resource groups blade, select <strong>Add</strong> on the toolbar to create a new resource group.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add resource group</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""4"" class=""css-13a5a39""><li class=""css-cvpopp"">On the <strong>Create a resource group</strong> Basics tab, enter the following:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Subscription</strong>: Select the subscription you are using for this course.</li><li class=""css-cvpopp""><strong>Resource group</strong>: Enter <code class=""chakra-code css-1u83yg1"">udacity-exercises</code> or another name unique within your subscription.</li><li class=""css-cvpopp""><strong>Region</strong>: Select any available region, but preferably one close to your location. <strong>Note</strong>, you will use this same region for all other resources you create during this course.</li></ul>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a resource group basics tab</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""5"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Review + create</strong> and on the Review + create tab, ensure the <code class=""chakra-code css-1u83yg1"">validation passed</code> message is displayed and then select <strong>Create</strong>.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">""Review new resource group settings""</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s check if you have everything you need for this course.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Have an active Azure Subscription</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Resource Group</p></div>']","['https://video.udacity-data.com/topher/2021/February/60359490_resource-groups/resource-groups.png', 'https://video.udacity-data.com/topher/2021/February/603608f1_resource-groups-add/resource-groups-add.png', 'https://video.udacity-data.com/topher/2021/February/60364966_create-a-resource-group-basics/create-a-resource-group-basics.png', 'https://video.udacity-data.com/topher/2021/February/603649bb_create-resource-group-review-create/create-resource-group-review-create.png']",https://www.youtube.com/embed/BdhLDVUr2RU
AI Fundamentals,AI Fundamentals,Lesson 2: Introduction to AI Fundamentals with Azure,2.7  When To Use and Not To Use AI and ML,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<h4 class=""chakra-heading css-1dlhxqh"">When not to use AI and ML?</h4>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">The lack of good, high-quality data would be an issue to get high confidence results. In this case, we suggest getting the data first.</li><li class=""css-cvpopp"">Do not use AI and ML if you are looking to get 100% correct answers at all times. Predictions and pattern recognition methods will not get you what you want. You have to be ok with wrong answers with varying confidence levels.</li><li class=""css-cvpopp"">The world is changing. Make sure your data and predictions help you within the confines of the nonstationary world. You can’t use data from last year to predict healthcare or finance situations post COVID.</li></ul>\n<h4 class=""chakra-heading css-1dlhxqh"">When to use AI and ML?</h4>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">If finding patterns is your primary goal, you are on the right path. Make sure your patterns are generalizable. You might be able to find a pattern of rainy days for the last year. But if that does not provide a good prediction for tomorrow, who cares about that pattern?</li><li class=""css-cvpopp"">You have to be ok with approximate solutions instead of exact solutions.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In which scenario AI and ML might not be a good fit?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Extracting sentiment from human speech.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Predicting when someone will get cancer.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Predicting how long a player will play a game.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Predicting what books a book reader would like reading.</p></div>']",[],https://www.youtube.com/embed/fS4nd4YzfJU
AI Fundamentals,AI Fundamentals,Lesson 2: Introduction to AI Fundamentals with Azure,2.8  Lesson Review,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Lesson Review</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">In this lesson, we took a peek at the world of AI and ML and prepared for what\'s next in the upcoming lessons. We learned:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">The tools and services that are available in Azure such as Azure Machine Learning and Azure Cognitive Services.</li><li class=""css-cvpopp"">The course agenda including topics ranging from AI and ML Core Concepts to Machine Learning, Computer Vision. Natural Language Processing and Conversational AI.</li><li class=""css-cvpopp"">Various stakeholders involved in AI and ML initiatives and the roles that are commonly found to be part of an AI and ML Team.</li><li class=""css-cvpopp"">The history of AI and ML.</li><li class=""css-cvpopp"">When we should use AI and ML and when we should not.</li></ul>\n<p class=""chakra-text css-o3oz8b"">Now, you are ready to go to the next level. Enjoy the course.</p></div>']",['https://video.udacity-data.com/topher/2021/March/60507005_20210316-lesson-review/20210316-lesson-review.png'],https://www.youtube.com/embed/kEcQCv1L0Gw
AI Fundamentals,AI Fundamentals,Lesson 2: Introduction to AI Fundamentals with Azure,2.9  Glossary,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">For your reference, here are all the new terms we introduced in this lesson:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Anomaly Detection:</strong> Detecting data points that do not align with a recognized pattern across a set of data points.</li><li class=""css-cvpopp""><strong>API:</strong> Application programming interfaces that are exposed to help multiple applications and systems interact with each other.</li><li class=""css-cvpopp""><strong>Autonomous:</strong> An autonomous system is a system that can accomplish a task without human involvement during the execution of the task.</li><li class=""css-cvpopp""><strong>Azure Cognitive Services:</strong> A suite of AI Services targeting specialized AI workloads to help developers implement AI capabilities into their solution with minimal AI and ML expertise.</li><li class=""css-cvpopp""><strong>Azure Machine Learning:</strong> A cloud-based service that delivers a platform supporting the creation and deployment of Machine Learning Models.</li><li class=""css-cvpopp""><strong>Computer Vision:</strong> A domain of artificial intelligence focused on processing and understanding visual inputs through complex machine learning algorithms.</li><li class=""css-cvpopp""><strong>Dataset:</strong> A dataset is a collection of data that is used to train the machine during the AI and ML processes.</li><li class=""css-cvpopp""><strong>Experiment:</strong> In the context of AI and ML, experiment refers to using the AI and ML processes to see if a particular question can be answered with AI and ML, or a hypothesis can be supported or validated.</li><li class=""css-cvpopp""><strong>Go:</strong> An abstract strategy-based board game.</li><li class=""css-cvpopp""><strong>Intent:</strong> An intent represents the action the user wants to execute. It usually refers to a preferred action when a user delivers a sentence to be interpreted by the machine.</li><li class=""css-cvpopp""><strong>Jupyter Notebook:</strong> A virtual notebook environment where developers can write notes and combine live code and visualizations into a single document.</li><li class=""css-cvpopp""><strong>Machine Learning Model:</strong> A machine learning model is a logic that has been trained to recognize certain types of patterns.</li><li class=""css-cvpopp""><strong>Machine Learning Operations (MLOps):</strong> The practice involves the automation of processes required to build and deliver end-to-end Machine Learning Models and experiences.</li><li class=""css-cvpopp""><strong>Natural Language Processing:</strong> A domain of artificial intelligence focused on understanding human language and helping computers and humans interact through human language.</li><li class=""css-cvpopp""><strong>Turing Test:</strong> A test where a human tested interacts with two entities, one machine, and one human, without knowing which one is which. The goal of the test is to assess if the tester can identify which one is the machine. If the tester can not determine the machine or is not sure, the machine passes the test.</li></ul></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.1  Introduction and Lesson Overview,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Introduction and Lesson Overview</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Welcome to the AI Fundamentals lesson about core concepts relating to artificial intelligence (AI) and machine learning (ML).  Artificial intelligence is a branch of computer science dedicated to the notion of agents:  computer systems with the ability to simulate thought.  Since its genesis in the 1950s, artificial intelligence has remained a topic of interest in computer science, developing out into a number of sub-topics focused around search and optimization, pattern recognition, inference, and learning from experience.  Implementations of artificial intelligence may be as simple as small, rule-based systems of behavior to move a character on a screen in a video game or as complex as multi-stage neural networks taking thousands of inputs to generate forecasts of tomorrow\'s stock market movements.</p>\n<p class=""chakra-text css-o3oz8b"">Throughout the lesson, we will gain an understanding of what Artificial Intelligence and Machine Learning are, as well as how they relate to one another.  We will review common workloads for AI and ML, including examples of how they are used in practice today.  We will additionally cover Responsible AI and Responsible ML guidelines, sets of practices to help promote ethics in artificial intelligence and machine learning, respectively.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">This lesson will introduce you to some of the key concepts in artificial intelligence and machine learning, acting as a springboard to the remaining lessons in this nanodegree.  In this lesson, we will learn about:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">What artificial intelligence and machine learning are, as well as how they relate to one another.</li><li class=""css-cvpopp"">Common artificial intelligence and machine learning workloads.</li><li class=""css-cvpopp"">The Responsible Artificial Intelligence project</li><li class=""css-cvpopp"">The Responsible Machine Learning project</li></ul>\n<p class=""chakra-text css-o3oz8b"">This key information will act as the foundation for each subsequent lesson, as well as providing a primer on the topic of artificial intelligence.</p></div>']",[],https://www.youtube.com/embed/8OdL7Bnca1Q
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.2  Introducing AI and ML,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Introducing AI and ML</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">Artificial intelligence is a branch of computer science dedicated to simulating human cognitive behavior.  In the realm of artificial intelligence, we have machine learning, which allows a computer to learn without human intervention.</p>\n<p class=""chakra-text css-o3oz8b"">The process of developing an artificial intelligence system has four key steps:</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Data scientists build an artificial intelligence system using techniques.  These techniques can include, but are not limited to, machine learning.</li><li class=""css-cvpopp"">The artificial intelligence system then trains machine learning models by studying patterns in the data.</li><li class=""css-cvpopp"">Data scientists observe the results of these machine learning models and subsequently optimizes them, most commonly by modifying the artificial intelligence system which generates these models.</li><li class=""css-cvpopp"">The process of training and optimization--steps 2 and 3--repeats until the model becomes accurate enough for its intended purpose.  At this point, the model is capable of going into production.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New Terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Artificial Intelligence</strong>:  the ability for a computer to simulate human cognitive behavior.</li><li class=""css-cvpopp""><strong>Weak Artificial Intelligence</strong>:  a narrow application of intelligence, focused on solving one problem.  An example of this is a system which counts people who enter or leave a bus.  All artificial intelligence systems today are considered weak AI.</li><li class=""css-cvpopp""><strong>Strong Artificial Intelligence</strong>:  a system which exhibits a human\'s ability to generalize and solve a variety of problems, including learning how to solve new problems without direct human programming.  As of today, there are no strong AI systems in the world, and there remains debate in the academic community around whether strong AI is achievable.</li><li class=""css-cvpopp""><strong>Super Artificial Intelligence</strong>:  a system which surpasses a human\'s ability to generalize and solve problems.  This is like strong AI, but the expectation is that the system is superior in every domain of problem-solving capability.</li><li class=""css-cvpopp""><strong>Machine Learning</strong>:  the process of combining algorithms and data to allow a computer to learn without human intervention.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional Resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Professor John McCarthy has <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""http://jmc.stanford.edu/artificial-intelligence/index.html"">a great set of questions and answers<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> around the concept of Artificial Intelligence.  One of these questions pertains to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""http://jmc.stanford.edu/artificial-intelligence/what-is-ai/branches-of-ai.html"">the branches of AI<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, showing the depth of what is available in this space.  Note that these answers were written in 2007, prior to the resurgence of neural networks.</li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/en-us/overview/artificial-intelligence-ai-vs-machine-learning/"">The differences between artificial intelligence and machine learning<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  This page explains how artificial intelligence and machine learning relate to one another.  It also emphasizes the AI and ML process, as well as explaining some of the capabilities of AI and ML.</li><li class=""css-cvpopp"">Artificial intelligence and machine learning are useful in a large number of industries.  Dirk Mayer and Olaf Enge-Rosenblatt provide a high-level review of <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://semiengineering.com/artificial-intelligence-for-industrial-applications/"">AI solutions in semi-conductor engineering and fabrication<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li></ul></div>']",[],https://www.youtube.com/embed/kGsjxeLxMXg
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.3  Quizzes: Introducing AI and ML,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Quizzes: Introducing AI and ML</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following statements are accurate?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Artificial intelligence is the ability for a computer to simulate human cognitive behavior.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Machine learning is the ability for a computer to simulate human cognitive behavior.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Artificial intelligence and machine learning are the same thing.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Artificial intelligence is a subset of machine learning.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Machine learning is a subset of artificial intelligence.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Below are the steps for the process of training for AI and ML.  Place them in the correct order.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Data scientists choose the single best model</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The AI system trains ML models</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The AI system adapts itself to changing circumstances</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Build an AI system using techniques which include ML</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Data scientists optimize the ML models</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Continue training until accuracy is sufficient</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Step 1</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Step 2</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Step 3</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Step 4</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Choose an industry familiar to you and research three examples of ways that artificial intelligence or machine learning are shaping that industry.  Briefly describe one of these projects and its impact on the relevant industry.</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.4  Common AI and ML Workloads,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Common AI and ML Workloads</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Workloads</h2>\n<p class=""chakra-text css-o3oz8b"">A <strong>workload</strong> is a type of problem we want to solve.  For example, one workload is <strong>computer vision</strong>, and an example of computer vision to review a folder full of images and tag those images which include pictures of either cats or dogs.</p>\n<p class=""chakra-text css-o3oz8b"">There are five common workloads for artificial intelligence.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Computer Vision</h2>\n<p class=""chakra-text css-o3oz8b""><strong>Computer vision</strong> is the process of interpreting the world visually.  We train models on videos or images, typically to recognize some object, state, or interaction.  Six common implementations of computer vision are:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Image classification</strong>:  Define the subject of an image.  For example, an image classification system could pick out the most prominent actor or thing in a photograph, such as a person, a fruit bowl, or London\'s Big Ben tower clock.</li><li class=""css-cvpopp""><strong>Object detection</strong>:  Determine whether a particular thing is in an image.  An example of this would be a camera placed near a chicken coop which detects foxes or other predators and raises an alert based on this.</li><li class=""css-cvpopp""><strong>Semantic segmentation</strong>:  Classify individual pixels in a photo as belonging to a particular thing.  For example, a camera monitoring a busy road would generate an image, and then a semantic segmentation model could identify the individual vehicles on the road and highlight each vehicle with a separate color.</li><li class=""css-cvpopp""><strong>Image analysis</strong>:  Image analysis provides us a textual description of who or what is in a photo.  For example, suppose we have a photo of a man playing catch with a dog.  An image analysis model might tell us that the man is throwing a tennis ball, that the man is wearing a blue cap, and that the dog is a yellow Labrador Retriever.</li><li class=""css-cvpopp""><strong>Facial detection and recognition</strong>:  Identify whether there is a face in a photo.  For example, you might want to count how many people enter a store.  Using a video camera monitoring the entrance to the store, a facial detection model can identify the number of people who pass through into the store based on observing faces.</li><li class=""css-cvpopp""><strong>Optical character recognition</strong>:  Read text from an image.  For example, you might open a phone application which reads a menu written in a language.  That application uses optical character recognition to read letters and words on the image.  It can further connect to a service which translates those letters and words into another language, showing the menu items in your preferred language.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Prediction, Forecasting, and Anomaly Detection</h2>\n<p class=""chakra-text css-o3oz8b""><strong>Prediction, forecasting, and anomaly detection</strong> are techniques intended to analyze data--typically numeric values--and generate an estimation.  In the case of prediction and forecasting, we attempt to draw conclusions from the data.  For example, suppose we want to estimate the value of a house.  We might use input data such as the number of bedrooms, number of bathrooms, and square footage, as well as information on the school district, whether it has certain amenities, and the year it was built.</p>\n<p class=""chakra-text css-o3oz8b"">Anomaly detection intends to find errors or unusual activity in a system.  It tends to be focused on a single measure, such as the temperature of a machine or the number of products listed on a marketplace.  This kind of data is usually--but not always--time series in nature, and we emphasize trends and changes rather than specific values.  That the temperature is 80 degrees Celsius is not necessarily a problem; the problem is that it should normally be 30 degrees Celsius.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Natural Language Processing</h2>\n<p class=""chakra-text css-o3oz8b""><strong>Natural language processing (NLP)</strong> is the process of interpreting written or spoken language.  We train models based on written documents or audio clips of speech.  Three common implementations of natural language processing are:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Text analytics</strong>:  Analyze documents.  There are several methods of analysis.  One method is to extract phrases, such as finding every time a character in a novel uses a particular word.  We can also enumerate the characters in a novel or pick out the places these characters visit over the course of a story.  We can even evaluate the sentiment of text, understanding the moods of characters over the course of the book.</li><li class=""css-cvpopp""><strong>Text translation</strong>:  Translate text between languages.  This allows a person speaking English to present to an audience of non-English speakers.  The translation service interprets the speaker\'s text, translates it to another language, and displays that text on the screen as live captions.</li><li class=""css-cvpopp""><strong>Text to speech</strong>:  Interpret written text and speak it aloud.  This kind of service has been around for decades, with improvements over time coming primarily in conveying meaning and nuance in the text using inflection and tone.</li></ul>\n<p class=""chakra-text css-o3oz8b"">In addition, Microsoft has a cloud service called the <strong>Language Understanding Intelligent Service (LUIS)</strong>, which accepts written or spoken inputs, processes those inputs, and allows developers to perform some action based on those inputs.  For example, a person may speak a command to open the curtains in the living room.  LUIS would interpret this command and could call a function to activate a physical device inside the house, opening the living room curtains.</p>\n<h2 class=""chakra-heading css-fz7yxd"">Knowledge Mining</h2>\n<p class=""chakra-text css-o3oz8b""><strong>Knowledge mining</strong> is the process of extracting knowledge from vast amounts of information.  In Azure, <strong>Azure Cognitive Search</strong> is the primary tool.  It tags information in documents, allowing for easy, detailed searches of those documents.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Conversational AI</h2>\n<p class=""chakra-text css-o3oz8b""><strong>Conversational AI</strong> is another common workload.  With this workload, we develop software agents intended to communicate directly with humans, typically in conversational format.  One common example of this is a chatbot which interacts with you on a website.  Another example of a conversational AI system would be a virtual agent, who might help you search on the market for a particular house based on the criteria most important to you.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional Resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">The Microsoft <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aidemos.microsoft.com/computer-vision"">hands-on Computer Vision demo<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> provides an overview of services available in their Cognitive Services product which relate to vision.  This includes analyzing and describing images, reading text from images, and recognizing celebrities or landmarks.</li><li class=""css-cvpopp"">Another hands-on demo from Microsoft allows you to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aidemos.microsoft.com/luis/demo"">use the Language Understanding Intelligent Service to control lights in a house<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  Use natural language commands to operate different lights in rooms based on context, such as ""I would like to watch a movie now."" or ""Time to go to sleep.""  The demo also shows the limitations of language understanding, as there are many ways to phrase a problem which developers did not think about.</li><li class=""css-cvpopp"">Microsoft has a <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/Microsoft/DataStoriesSamples/tree/master/samples/WarAndPeaceSentimentAnalysis"">data story which uses sentiment analysis<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> to observe the emotional states and relationships of characters throughout Leo Tolstoy\'s novel, <em class=""chakra-text css-o3oz8b"">War and Peace</em>.</li><li class=""css-cvpopp"">Another example of computer vision in practice is <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/Azure/pixel_level_land_classification"">this demo on pixel-level land cover classification<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  Data scientists trained a neural network to accept an aerial image as an input and return a lad cover label for each pixel in the image.</li></ul></div>']",[],https://www.youtube.com/embed/CQgFNJpVZCU
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.5  Quizzes: Common AI and ML Workloads,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Quizzes: Common AI and ML Workloads</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following is not a common AI or ML workload?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Natural Language Processing</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Knowledge Mining</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Conversational AI</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Literate Programming</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Computer Vision</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Match the AI or ML operation to its specific workload.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Natural Language Processing</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Anomaly Detection</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Conversational AI</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Knowledge Mining</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Prediction and Forecasting</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Computer Vision</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Semantic segmentation</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Classifying objects</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Tracking trends and changes</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Text to speech</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Cognitive Search</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">QnA maker</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.6  Responsible AI,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Responsible AI</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">The Responsible AI project is intended to serve as a framework for promoting ethical behavior when working with and deploying artificial intelligence systems.  The project starts from the premise that artificial intelligence systems can affect our livelihoods and lives in significant fashion--at the extreme, we put our lives into self-driving cars and aircraft auto-pilot systems.  Even in more pedestrian endeavors, an AI system can unintentionally harm humans.  The goal of the Responsible AI project is to provide guidance to data scientists, data engineers, and operations specialists on how to create, deploy, and manage artificially intelligent code in a way which does not cause harm to others.</p>\n<p class=""chakra-text css-o3oz8b"">The Responsible AI project consists of six guidelines:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Fairness</strong>:  AI systems should treat all people fairly and not affect similarly situated groups in different ways.</li><li class=""css-cvpopp""><strong>Reliability and Safety</strong>:  Customers should be able to trust that AI solutions will perform reliably and safely within a clear set of parameters, as well as respond safely to unanticipated situations.</li><li class=""css-cvpopp""><strong>Privacy and Security</strong>:  AI systems should be secure and respect existing privacy laws.</li><li class=""css-cvpopp""><strong>Inclusiveness</strong>:  AI systems should engage and empower people and use inclusive design practices to eliminate unintentional barriers.</li><li class=""css-cvpopp""><strong>Transparency</strong>:  People should know how AI systems work and how they interact with data to make decisions.</li><li class=""css-cvpopp""><strong>Accountability</strong>:  Those who design and deploy AI systems are accountable for how their systems operate.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional Resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Microsoft has provided <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.microsoft.com/en-us/ai/responsible-ai-resources"">a set of resources around Responsible AI<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  This includes descriptions of each guideline as well as tools to put these guidelines into practice.</li></ul></div>']",[],https://www.youtube.com/embed/HBygSGptb8s
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.7  Quizzes: Responsible AI,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Quizzes: Responsible AI</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following are not guiding principles of responsible AI?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Transparency</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Accuracy</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Accountability</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Reliability</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Completeness</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Understanding the risk of failure helps fulfill which guiding principle of responsible AI?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Fairness</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Reliability and safety</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Privacy and security</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Inclusiveness</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Transparency</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Accountability</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Consider the following scenario:</p>\n<p class=""chakra-text css-o3oz8b"">Your team has created an AI system which helps price business loans to entrepreneurs.  The system includes a model which takes in detailed demographic information about the entrepreneur (including race, ethnicity, age, political beliefs, and religion) as well as several hundred other inputs.  The model returns a single output:  the interest rate for the loan.  Because of the level of complexity, it is not possible for anybody to explain how the system works in its entirety and the data science team want to put it out into production, making it available for use but not maintaining or monitoring the system.  In addition, all data, including the entrepreneur\'s tax data for the past three years, is stored in Azure Blob Storage in a publicly accessible blob.</p>\n<p class=""chakra-text css-o3oz8b"">Which of the following responsible AI guidelines would this system violate?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Fairness</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Reliability and safety</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Privacy and security</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Inclusiveness</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Transparency</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Accountability</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.8  Exercise: Responsible AI in Practice,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Exercise: Responsible AI in Practice</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">We have looked at the responsible AI approach with its six key tenets:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Fairness</li><li class=""css-cvpopp"">Reliability and safety</li><li class=""css-cvpopp"">Privacy and security</li><li class=""css-cvpopp"">Inclusiveness</li><li class=""css-cvpopp"">Transparency</li><li class=""css-cvpopp"">Accountability</li></ul>\n<p class=""chakra-text css-o3oz8b"">These are important principles to keep in mind when building artificially intelligent agents, but applying these in practice may not be straightforward.  It can be easy to focus on the ""machine"" part of artificial intelligence to the detriment of humans who deal directly with the agent or indirectly through the agent\'s actions.</p>\n<p class=""chakra-text css-o3oz8b"">Keep in mind that this human-AI interaction is critical for developing artificial intelligence responsibly.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Guidelines for Human-AI Interaction</h2>\n<p class=""chakra-text css-o3oz8b"">Navigate to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aidemos.microsoft.com/guidelines-for-human-ai-interaction/demo"">the Microsoft AI Guidlines for Human-AI Interaction demo<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</p>\n<p class=""chakra-text css-o3oz8b"">This demo takes us through four phases of interaction between humans and artifically intelligent agents.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">For each guideline, review the card text on the left-hand side as well as examples on the right-hand side.  Then, select the next card to continue through the demo.</p>\n<p class=""chakra-text css-o3oz8b"">You\'ll use the knowledge from each of these guidelines to answer the following quizzes.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following is an example of supporting efficient correction?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Making it easy for a user to reroute around unexpected road closures</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Notifying a user who has missed an important meeting</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Having a spell checker which marks misspellings while a user types a word rather than waiting for the complete word</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">A navigation system rerouting a user automatically if the user misses a turn</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following are examples of showing contextually relevant information to a user?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Recommending accessories when a person adds a product to a shopping cart</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Showing the current price of a stock when searching by the ticker name</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Loading a home page when a user opens a new browser tab</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Providing the ability to dismiss suggestions by clicking an X</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Displaying prospective meeting attendees\' calendars when planning a meeting</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Match the system activity with its most relevant human-AI interaction guideline.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Learn from user behavior</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Scope services when in doubt</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Mitigate social biases</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Notify users about changes</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Convey the consequences of user actions</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Match relevant social norms</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">If a user likes a particular post, explain how that changes the recommender algorithm.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Display a warning message if a user tries to navigate to a point with no possible routing.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Based on the ""difficulty level"" of functionality used in a product, tailor automated help to the user\'s perceived level of experience.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">When correcting user behavior, use a tone and style which is appropriate for the user.</p></div>']",['https://video.udacity-data.com/topher/2021/February/602dbd56_card-1/card-1.png'],
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.9  Responsible ML,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Responsible ML</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">As an analog to the Responsible AI project, the Responsible ML project is an effort to control machine learning models and protect users.  The Responsible ML project consists of three key guidelines:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Understand</strong>:  We should understand our models, including knowing which factors play a role and to what extent they affect the outcome of the model.  This ties in quite closely to the Responsible AI guidelines of <strong>transparency</strong> and <strong>fairness</strong>.</li><li class=""css-cvpopp""><strong>Protect</strong>:  We want to use tools and processes which protect data privacy at all times, even during training.</li><li class=""css-cvpopp""><strong>Control</strong>:  We should create audit trails and track the lineage of our models.  This will help us ensure that the right model is producing the expected results in a production environment.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New Terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Differential privacy</strong>:  A technique in which individual data points are modified with some constant error term.  The end result is that the data points can no longer be tied back to specific locations but the distribution of this data remains the same in aggregate.  For example, a dataset of police incidents may use differential privacy to protect accidental disclosure of information, such as knowledge of a domestic disturbance.  The dataset would have each point (in latitude and longitude) enclosed in a circle of radius <em class=""chakra-text css-o3oz8b"">r</em>, where <em class=""chakra-text css-o3oz8b"">r</em> might be approximately one city block.  We choose a random point within the circle and define that point as the location of the incident.  On net, we know how many crimes there are in a particular neighborhood, but we do not necessarily know at which houses the incidents occur.</li><li class=""css-cvpopp""><strong>Homomorphic encryption</strong>:  A style of encryption which allows a developer or data scientist to perform calculations on encrypted data without decrypting it first.  The result of these calculations will also be in an encrypted form and the decrypted result will be exactly the same as if we performed all of the operations on unencrypted data.  This ensures that data scientists can work on data sets while maintaining maximum privacy, as they will not see the unencrypted data at any time.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional Resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Microsoft has <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/en-us/services/machine-learning/responsibleml/"">a page dedicated to Responsible ML guidelines<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  This page includes introductory videos, webinars, and examples of tools which promote responsible use of machine learning in practice.</li><li class=""css-cvpopp"">Microsoft Research has a page containing <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.microsoft.com/en-us/research/project/homomorphic-encryption/"">information on homomorphic encryption<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  This includes information about the topic at a high level, as well as information on specific libraries like Microsoft SEAL, which makes homomorphic encryption available to developers.</li></ul></div>']",[],https://www.youtube.com/embed/8zNLgzO-9UE
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.10  Quizzes: Responsible ML,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Quizzes:  Responsible ML</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The responsible ML guideline of understanding is a combination of which responsible AI guidelines?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Fairness</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Reliability and safety</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Privacy and security</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Inclusiveness</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Transparency</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Accountability</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In your own words, describe why it is important for responsible machine learning specialists to have audit trails and track the lineage of models.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following are examples of techniques to promote the responsible ML guideline of protection?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Use differential privacy</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Encrypt data at rest</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Use confidential machine learning techniques</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Use homomorphic encryption to prevent data scientists from seeing the actual data</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.11  Edge Cases,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Edge Cases</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">What we have covered in this lesson is an incomplete survey of artificial intelligence.  There are additional workloads which are less common but still fall into the realm of artificial intelligence.  Some of these include:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Autonomous systems, which control things like lighting, ventilation, and security.</li><li class=""css-cvpopp"">Optimization of difficult problems.  One example of this is the use of a technique known as <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.hindawi.com/journals/ijap/2014/729208/"">genetic algorithms to design smart antennas<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  Antenna design is a complicated problem and genetic algorithms provide an opportunity to search within the space of possible solutions for something better than what we have today.</li><li class=""css-cvpopp"">Planning systems, which take a set of general facts about the effects of specific actions, facts about a particular situation, and an end goal.  The system then generates a strategy to achieve the goal.</li></ul>\n<p class=""chakra-text css-o3oz8b"">Regardless of the workload, however, it is important to keep the Responsible AI and Responsible ML guidelines in mind.  It is also important to note that these are starting points, not conclusions;  they are something we keep purposefully in mind throughout the development, deployment, and maintenance of artificial intelligence solutions.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional Resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Microsoft has <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/en-us/services/machine-learning/responsibleml/"">a page dedicated to Responsible ML guidelines<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  This page includes introductory videos, webinars, and examples of tools which promote responsible use of machine learning in practice.</li><li class=""css-cvpopp"">The editors at Brainz include a series of cases in which researchers, engineers, and developers have <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.brainz.org/15-real-world-applications-genetic-algorithms/"">used genetic algorithms<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> to solve problems in a variety of fields.</li><li class=""css-cvpopp"">John Holland provides <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""http://www2.econ.iastate.edu/tesfatsi/holland.gaintro.htm"">an introduction to the concept behind genetic algorithms<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  This approach, based on tenets of evolutionary biology, works well in a variety of constrained optimization scenarios.</li><li class=""css-cvpopp"">John Koza provides <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""http://www.genetic-programming.com/gpanimatedtutorial.html"">an introduction to genetic programming<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, a variant on John Holland\'s concept of genetic algorithms.</li></ul></div>']",[],https://www.youtube.com/embed/m5b2mCLG8RM
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.12  Lesson Review,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Lesson Review</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">This lesson introduced you to key terminology in artificial intelligence and machine learning.  In the lesson, we learned:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">What artificial intelligence and machine learning are, as well as how machine learning is a subset of artificial intelligence.</li><li class=""css-cvpopp"">There are five common artificial intelligence and machine learning workloads, including:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Computer vision</li><li class=""css-cvpopp"">Prediction, forecasting, and anomaly detection</li><li class=""css-cvpopp"">Natural language processing</li><li class=""css-cvpopp"">Knowledge mining</li><li class=""css-cvpopp"">Conversational AI</li></ul>\n</li><li class=""css-cvpopp"">The Responsible Artificial Intelligence project provides six guidelines for ethical artificial intelligence projects:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Fairness</li><li class=""css-cvpopp"">Reliability and safety</li><li class=""css-cvpopp"">Privacy and security</li><li class=""css-cvpopp"">Inclusiveness</li><li class=""css-cvpopp"">Transparency</li><li class=""css-cvpopp"">Accountability</li></ul>\n</li><li class=""css-cvpopp"">The Responsible Machine Learning project has three pillars for ethical machine learning projects:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Understand the model</li><li class=""css-cvpopp"">Protect the data</li><li class=""css-cvpopp"">Control model deployment</li></ul>\n</li></ul>\n<p class=""chakra-text css-o3oz8b"">With this information, we now have the foundation that we need in order to jump into the remaining lessons of this nanodegree.</p></div>']",[],https://www.youtube.com/embed/e5JdIA7U-hw
AI Fundamentals,AI Fundamentals,Lesson 3: AI and ML Core Concepts,3.13  Glossary,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Glossary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">For your reference, here are all the new terms we introduced in this lesson:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Artificial Intelligence</strong>:  the ability for a computer to simulate human cognitive behavior.</li><li class=""css-cvpopp""><strong>Computer vision:</strong>  the process of interpreting the world visually.  We train models on videos or images, typically to recognize some object, state, or interaction.</li><li class=""css-cvpopp""><strong>Differential privacy</strong>:  A technique in which individual data points are modified with some constant error term.  The end result is that the data points can no longer be tied back to specific locations but the distribution of this data remains the same in aggregate.  For example, a dataset of police incidents may use differential privacy to protect accidental disclosure of information, such as knowledge of a domestic disturbance.  The dataset would have each point (in latitude and longitude) enclosed in a circle of radius r, where r might be approximately one city block.  We choose a random point within the circle and define that point as the location of the incident.  On net, we know how many crimes there are in a particular neighborhood, but we do not necessarily know at which houses the incidents occur.</li><li class=""css-cvpopp""><strong>Homomorphic encryption</strong>:  A style of encryption which allows a developer or data scientist to perform calculations on encrypted data without decrypting it first.  The result of these calculations will also be in an encrypted form and the decrypted result will be exactly the same as if we performed all of the operations on unencrypted data.  This ensures that data scientists can work on data sets while maintaining maximum privacy, as they will not see the unencrypted data at any time.</li><li class=""css-cvpopp""><strong>Knowledge mining:</strong>  the process of extracting knowledge from vast amounts of information.  In Azure, <strong>Azure Cognitive Search</strong> is the primary tool.  It tags information in documents, allowing for easy, detailed searches of those documents.</li><li class=""css-cvpopp""><strong>Language Understanding Intelligent Service (LUIS)</strong>:  a service which accepts written or spoken inputs, processes those inputs, and allows developers to perform some action based on those inputs.</li><li class=""css-cvpopp""><strong>Machine Learning</strong>:  the process of combining algorithms and data to allow a computer to learn without human intervention.</li><li class=""css-cvpopp""><strong>Natural language processing (NLP)</strong> is the process of interpreting written or spoken language.  We train models based on written documents or audio clips of speech.</li><li class=""css-cvpopp""><strong>Responsible AI:</strong>  The Responsible AI project is intended to serve as a framework for promoting ethical behavior when working with and deploying artificial intelligence systems.  It consists of six guidelines:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Fairness</strong>:  AI systems should treat all people fairly and not affect similarly situated groups in different ways.</li><li class=""css-cvpopp""><strong>Reliability and Safety</strong>:  Customers should be able to trust that AI solutions will perform reliably and safely within a clear set of parameters, as well as respond safely to unanticipated situations.</li><li class=""css-cvpopp""><strong>Privacy and Security</strong>:  AI systems should be secure and respect existing privacy laws.</li><li class=""css-cvpopp""><strong>Inclusiveness</strong>:  AI systems should engage and empower people and use inclusive design practices to eliminate unintentional barriers.</li><li class=""css-cvpopp""><strong>Transparency</strong>:  People should know how AI systems work and how they interact with data to make decisions.</li><li class=""css-cvpopp""><strong>Accountability</strong>:  Those who design and deploy AI systems are accountable for how their systems operate.</li></ul>\n</li><li class=""css-cvpopp""><strong>Responsible ML</strong>:  The Responsible ML project is an effort to control machine learning models and protect users.  It consists of three key guidelines:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Understand</strong>:  We should understand our models, including knowing which factors play a role and to what extent they affect the outcome of the model.  This ties in quite closely to the Responsible AI guidelines of <strong>transparency</strong> and <strong>fairness</strong>.</li><li class=""css-cvpopp""><strong>Protect</strong>:  We want to use tools and processes which protect data privacy at all times, even during training.</li><li class=""css-cvpopp""><strong>Control</strong>:  We should create audit trails and track the lineage of our models.  This will help us ensure that the right model is producing the expected results in a production environment.</li></ul>\n</li><li class=""css-cvpopp""><strong>Strong Artificial Intelligence</strong>:  a system which exhibits a human\'s ability to generalize and solve a variety of problems, including learning how to solve new problems without direct human programming.  As of today, there are no strong AI systems in the world, and there remains debate in the academic community around whether strong AI is achievable.</li><li class=""css-cvpopp""><strong>Super Artificial Intelligence</strong>:  a system which surpasses a human\'s ability to generalize and solve problems.  This is like strong AI, but the expectation is that the system is superior in every domain of problem-solving capability.</li><li class=""css-cvpopp""><strong>Weak Artificial Intelligence</strong>:  a narrow application of intelligence, focused on solving one problem.  An example of this is a system which counts people who enter or leave a bus.  All artificial intelligence systems today are considered weak AI.</li></ul></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.1  Introduction and Overview,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Welcome to this AI Fundamentals lesson about machine learning.  As we learned in the prior lesson, machine learning is the process of combining algorithms and data to allow a computer to learn without human intervention.</p>\n<p class=""chakra-text css-o3oz8b"">Throughout this lesson, we will engage with this concept and lay out important considerations around how machine learning works and the appropriate process for designing and implementing a machine learning solution.  We will tie this in with the data science process and gain an understanding of key data science terminology which will be important for machine learning.  We will also learn how to tie business user questions to the choice of appropriate machine learning algorithms.</p>\n<p class=""chakra-text css-o3oz8b"">The lesson contains four key components.</p>\n<h2 class=""chakra-heading css-fz7yxd"">1. Machine Learning Concepts</h2>\n<p class=""chakra-text css-o3oz8b"">The first component deals with key machine learning concepts.  Here, we will gain an understanding of some of the fundamental ideas which underpin machine learning.  This starts with the data science process, a general structure for machine learning.  From there, we will understand the importance of training and validation datasets and why both are necessary.  We will move on from there to a discussion of the classes of algorithms and finally, evaluation metrics for some of these algorithms.</p>\n<h2 class=""chakra-heading css-fz7yxd"">2. Approaches to Machine Learning</h2>\n<p class=""chakra-text css-o3oz8b"">After that, we will look at the main approaches to machine learning, learning about the difference between supervised and unsupervised machine learning, as well as less common techniques like semi-supervised and reinforcement learning.</p>\n<h2 class=""chakra-heading css-fz7yxd"">3. Core Tasks in Building a Solution</h2>\n<p class=""chakra-text css-o3oz8b"">Then, we will review the core tasks of building a machine learning solution and apply them to Azure Machine Learning.  We will start with Azure Machine Learning\'s Automated Machine Learning product, which offers a step-by-step experience for people with limited exposure to machine learning to train and deploy valid machine learning models.  We will use this product to ingest and prepare data for a machine learning task.  This preparation includes performing operations known as feature engineering and feature selection--that is, creating new features for machine learning, and removing features from an analysis, respectively.</p>\n<p class=""chakra-text css-o3oz8b"">After defining features, we will train and evaluate a model.  Supposing the model quality is sufficient, we will then deploy and manage that model in Azure Machine Learning.  We will wrap up this part of the lesson by performing testing against the deployed model.</p>\n<h2 class=""chakra-heading css-fz7yxd"">4. An Overview of Azure Machine Learning</h2>\n<p class=""chakra-text css-o3oz8b"">From there, we will look at the Azure Machine Learning designer, a separate product which expands the scope of possibility for developers and budding data scientists.  By the end of this lesson, you will have created three separate machine learning models using Azure Machine Learning and deployed two of them to production.</p></div>']",[],https://www.youtube.com/embed/wNAXnVm87wo
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.2  Introducing Machine Learning Concepts,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Over the next several pages, we\'ll introduce to several key concepts that will be necessary to understand before you build out a machine learning model:</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">The data science process</li><li class=""css-cvpopp"">Training and evaluation</li><li class=""css-cvpopp"">Evaluation metrics and choosing a model</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">The Data Science Process</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">First, let\'s begin with an overview of the typical <strong>data science process</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Key Phases of the Data Science Process</h3>\n<p class=""chakra-text css-o3oz8b"">There are five key steps to the data science process:</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp""><strong>Collect data</strong>.  We need to be able to collect reliable data and ensure that we understand its origin, quality, and meaning.</li><li class=""css-cvpopp""><strong>Prepare data</strong>.  The next phase in the data science process is data preparation.  This can take up an enormous percentage of total time--estimates range upwards of 80-90%.</li><li class=""css-cvpopp""><strong>Train a model</strong>.  In this phase, we apply appropriate algorithms to our data, resulting in a trained model.  Depending on the algorithm, training may take a considerable amount of time.  For example, certain applications of neural networks may take over a month to train fully.  Generally, however, model training does not take that much time.</li><li class=""css-cvpopp""><strong>Evaluate the model</strong>.  After we have trained a model, we want to ensure that it meets our expectations in terms of quality.  Because the real world is always more complicated than our training data set, we want to make sure that the results look reasonable on non-training before pushing a model out to production.  This helps us avoid overfitting to the training data.</li><li class=""css-cvpopp""><strong>Deploy the model</strong>.  Having a model is great, but having it available for use is the natural next step.  Historically, deploying a model typically meant rewriting it into a ""production"" development language like C or C++.  Today, it is easy to run a microservice in a language like R or Python and handle prediction needs.</li></ol></div>']",[],https://www.youtube.com/embed/GzQRoNyBu94
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.3  Training and Evaluation,"['<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Labels and Features</h3>\n<p class=""chakra-text css-o3oz8b""><strong>Labels</strong> are the thing we want to predict.  In a regression algorithm, this is the numeric value we want to predict.  In a classification algorithm, this is the class we want to choose.  <strong>Features</strong> help us determine the labels.  Features are inputs which help us understand what affects the label.  If we want to predict the sale price of a house, we would want to collect historical information on home sales.  In our data set, the sale price would be the label and anything we believe ties to the sale price, such as number of bedrooms, number of bathrooms, and school district, would be features.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Defining Training and Validation Datasets</h3>\n<p class=""chakra-text css-o3oz8b"">We want to have separate training and validation datasets.  We use the training dataset to train the model, but we want to have a separate dataset to evaluate that model\'s quality.  This helps us understand whether our model is <strong>overfitting</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">As an example of overfitting, suppose that in the training data set, if Column A is 6, our label is Blue.  But in the real world, there is no relationship that direct--it\'s just that our data set had a limited number of cases where Column A was 6 and they coincidentally related to the label Blue.  Our model might latch onto this and expect that any time it sees Column A is 6, the answer is Blue.</p>\n<p class=""chakra-text css-o3oz8b"">If we have only a training dataset but no evaluation dataset, we are less likely to catch this sort of overfitting.  In practice, the best way to understand whether your model exhibits overfitting is to see if the accuracy against the training dataset is considerably higher than the accuracy against the evaluation dataset.  If accuracy values are similar (and here, ""similar"" is a somewhat subjective notion, as not all models in all circumstances will be equally accurate), then there is little to no overfitting observed.  If the model is considerably more accurate against the training dataset than against the evaluation dataset, we likely are overfitting to the particulars of the training dataset.  One of the easiest methods for resolving this is to add more data to the training dataset.  The larger your sample, the less likely that relationships in the data will be unique to that training data set.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Common Classes of Algorithms</h3>\n<p class=""chakra-text css-o3oz8b"">There are several sorts of algorithms commonly in use.  Each of these algorithms also answers at least one user question.  When talking to end users about the problems they would like to solve, listen for these cues to gain an understanding of the class of algorithm to choose.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Regression</strong>:  How much should a database administrator earn?  How many foot pounds of torque should we use to tighten these lugs?</li><li class=""css-cvpopp""><strong>Classification</strong>:  To which of these product categories does this item belong?</li><li class=""css-cvpopp""><strong>Clustering</strong>:  What groups exist within my customer base?  I would like to understand how to market to them individually.</li><li class=""css-cvpopp""><strong>Recommendations</strong>:  After a user has watched this video, which videos might they want to watch next?</li><li class=""css-cvpopp""><strong>Text analytics</strong>:  Based on customer reviews for our product, which aspects of the product do they like and which do they dislike?</li><li class=""css-cvpopp""><strong>Image classification</strong>:  In which style of art is this painting?</li><li class=""css-cvpopp""><strong>Anomaly detection</strong>:  Is our boiler system operating normally?</li></ul></div>']",[],https://www.youtube.com/embed/bsy3Gh3smXE
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.4  Evaluation Metrics,"['<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Model Evaluation Metrics for Regression</h3>\n<p class=""chakra-text css-o3oz8b"">Based on the type of model, there are several evaluation measures available.  The most common classes of algorithm we want to test are <strong>regression</strong> and <strong>classification</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">For regression, we have several evaluation metrics available, including:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">R^2 (R-squared)</li><li class=""css-cvpopp"">Mean Absolute Error (MAE)</li><li class=""css-cvpopp"">Mean Absolute Percent Error (MAPE)</li><li class=""css-cvpopp"">Root Mean Squared Error (RMSE)</li><li class=""css-cvpopp"">Root Mean Squared Log Error (RMSLE)</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Model Evaluation Metrics for Classification</h3>\n<p class=""chakra-text css-o3oz8b"">Based on the type of model, there are several evaluation measures available.  The most common classes of algorithm we want to test are <strong>regression</strong> and <strong>classification</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">For classification, we have the <strong>confusion matrix</strong>, which allows us to define a variety of important measures.  Three of these important measures are:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Accuracy</li><li class=""css-cvpopp"">Precision</li><li class=""css-cvpopp"">Recall</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">The Importance of Measures Other than Accuracy</h3>\n<p class=""chakra-text css-o3oz8b"">Accuracy in a classification problem is not the only important measure. In some situations, accuracy can be quite misleading.  For example, in a simple two-class problem, if one class is far more common than the other, a model might simply always predict the popular class.  If 99.9% of the data points belong to Class 1 and the model simply picks Class 1 all the time, that model will have an accuracy of 99.9%.  But it will be a terrible model for differentiating Class 1 from Class 2.  We can see this with measures like <strong>precision</strong> and <strong>recall</strong>.</p>\n<h3 class=""chakra-heading css-k57syw"">Receiver Operating Characteristic Curve</h3>\n<p class=""chakra-text css-o3oz8b"">In addition, for a two-class classification problem, we can define a <strong>Receiver Operating Characteristic</strong> (ROC) curve, which lets us plot the tradeoff between false positives and false negatives as a curve.  The <strong>Area Under the Curve</strong> (AUC) is a number which represents how good the model is at making accurate predictions.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New Terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Microservice</strong>:   A lightweight, independent service.  Typically, microservices have one job and communicate with each other using well-defined operations.</li><li class=""css-cvpopp""><strong>Label</strong>:  The thing we want to predict.</li><li class=""css-cvpopp""><strong>Feature</strong>:  Inputs which help us understand what affects the label.</li><li class=""css-cvpopp""><strong>Overfitting</strong>:  A situation which happens when a trained model latches onto the particular relationships within a training data set, but those particulars are not always indicative of the broader world.</li><li class=""css-cvpopp""><strong>R^2 (R-squared)</strong>:  An evaluation measure for linear regression models which ranges from 0-1, where 1 is the highest possible score.</li><li class=""css-cvpopp""><strong>Mean Absolute Error (MAE)</strong>:  An evaluation measure for any regression model.  It is the average difference between predicted and actual values.  This works well when dealing with small ranges of numbers.</li><li class=""css-cvpopp""><strong>Mean Absolute Percent Error (MAPE)</strong>:  An evaluation measure for any regression model.  It is the percentage difference between the predicted and actual values.  If the actual value is 0, MAPE will fail with a divide by 0 error, so it is not a good measure if the actual value can be 0.  MAPE works best when you have large ranges of numbers.</li><li class=""css-cvpopp""><strong>Root Mean Square Error (RMSE)</strong>:  An evaluation measure for any regression model.  RMSE works best when you are concerned with large differences between the predicted and actual values.</li><li class=""css-cvpopp""><strong>Root Mean Square Log Error (RMSLE)</strong>:  An evaluation measure for any regression model.  RMSLE works best when you are concerned with large percentage differences between the predicted and actual values.</li><li class=""css-cvpopp""><strong>Confusion matrix</strong>:  A table representing predicted versus actual values for a classification problem.  A classic two-class confusion matrix has four boxes.  Using ""Yes"" and ""No"" as the two classes, these four boxes are:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">True Positive: we predicted Yes correctly</li><li class=""css-cvpopp"">False Positive: we predicted Yes but it was really No</li><li class=""css-cvpopp"">False Negative: we predicted No but it was really Yes</li><li class=""css-cvpopp"">True Negative: we predicted No correctly</li></ul>\n</li><li class=""css-cvpopp""><strong>Accuracy (classification)</strong>:  A measure which is defined as the number of correct predictions divided by the total number of predictions.</li><li class=""css-cvpopp""><strong>Precision</strong>:  A measure which calculates how frequently our predicted value is correct.  It is defined as True Positives / (True Positives + False Positives).</li><li class=""css-cvpopp""><strong>Recall</strong>:  A measure which calculates how frequently we correctly predict a value.  It is defined as True Positives / (True Positives + False Negatives).</li><li class=""css-cvpopp""><strong>Receiver Operating Characteristic (ROC) curve</strong>:  A plot which represents true positive versus false positive rates for a two-class model.</li><li class=""css-cvpopp""><strong>Area Under the Curve</strong>:  the percentage of area underneath the ROC curve.  This is a measure of how accurate the two-class model is, with numbers closer to 1 being better.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional Resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">If you want to learn more about the data science process, Microsoft has their <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/overview"">Team Data Science Process<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, which is an approach for launching a data science project.</li><li class=""css-cvpopp"">Microsoft has <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-cheat-sheet"">a cheat sheet for algorithms in Azure Machine Learning<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  This sheet focuses on algorithms available in Azure Machine Learning, so it is necessarily not a listing of all possible algorithms.  It is, however, a good entry point and provides an easy way to remember the most common classes of machine learning algorithms.</li><li class=""css-cvpopp"">Wikipedia has a solid set of information around concepts in statistics, including <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://en.wikipedia.org/wiki/Confusion_matrix"">the confusion matrix<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  Having a deep familiarity with the confusion matrix makes evaluating classification models significantly easier, as it is possible to derive most of the evaluation measures for classification models from this matrix.</li><li class=""css-cvpopp"">Microsoft also has an article which explains <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml"">how to evaluate experiment results<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  This ties to their Automated ML product, but the concepts are not unique to Automated ML.</li></ul></div>']",[],https://www.youtube.com/embed/4RPDuUd-X7c
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.5  Quizzes: Introducing Machine Learning Concepts,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following is <strong>not</strong> a reason why we want separate training and validation datasets?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">We want to see how a model performs on new data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">A model might train the peculiarities of the training dataset.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Providing too many examples when training a model can lead to training failure.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Accuracy on the training data set might be higher than against new data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">I run a company which uses equipment to etch renderings of famous art works on metal plates using lasers.  I want to keep my customer base, which skews heavily toward social media users, happy by making sure I choose the right images for reproduction and creating faithful renderings.  To that end, I want to understand if images come out looking weird, and I can tell that based on how precise my laser cuts are.  What I want to know is how precise those cuts are and how factors such as humidity, machine temperature, and temperature of the metal plate affect this.</p>\n<p class=""chakra-text css-o3oz8b"">Which of the following class of algorithms fits best this scenario?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Anomaly detection</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Classification</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Clustering</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Image classification</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Recommendation</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Regression</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Text analytics</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following measures are useful when solving a regression problem?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">R^2</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Precision</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Root Mean Squared Error</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Mean Absolute Error</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Area Under the ROC Curve (AUC)</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.6  Main Approaches to Machine Learning,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">There are four approaches to machine learning:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Supervised learning</strong>:  We have a known good answer for our label.  The most common examples of this include classification and regression.  This is by far the most common type of machine learning in the business world, as we generally develop models to solve known business problems, such as forecasting how much revenue the company will earn in the next quarter.</li><li class=""css-cvpopp""><strong>Unsupervised learning</strong>:  We do not have labels for our data.  We use unsupervised learning techniques to try to discover what those labels should be.  Clustering is the most common example of this.</li><li class=""css-cvpopp""><strong>Semi-supervised learning:</strong>  We have a small percentage of data with labels and a large percentage of unlabeled data.  Perform supervised learning against the labeled data and then cluster the unlabeled data to find the nearest labeled points.</li><li class=""css-cvpopp""><strong>Reinforcement learning</strong>:  We train an agent to observe its environment and use those environmental clues to make a decision.  For example, we might train a robot to sweep through a house in the least amount of time without getting stuck or sweeping over the same spots too frequently.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional Resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">The Algorithmia blog provides <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://algorithmia.com/blog/semi-supervised-learning"">an overview of semi-supervised learning<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  It also explains how semi-supervised learning differs from reinforcement learning.</li><li class=""css-cvpopp"">Paul Pinard shows an example of reinforcement learning by <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://towardsdatascience.com/a-beginners-guide-to-reinforcement-learning-with-a-mario-bros-example-fa0e0563aeb7"">training a neural network to play Super Mario Brothers<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li></ul></div>']",[],https://www.youtube.com/embed/FGknX3VtC6A
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.7  Quizzes: Main Approaches to Machine Learning,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Match the machine learning approach to its relationship with labeled data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Semi-Supervised Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Supervised Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Reinforcement Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Self-Supervised Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Unsupervised Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">All training data is labeled</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">No training data is labeled</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">A small amount of training data is labeled, but the vast majority is unlabeled</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following is <strong>not</strong> an example of supervised learning?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Classification</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Clustering</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Feature Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Similarity Learning</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.8  Core Tasks of Building a Machine Learning Solution,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">There are five core tasks involved in building a machine learning solution:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Ingest and prepare data</li><li class=""css-cvpopp"">Feature selection and feature engineering</li><li class=""css-cvpopp"">Model training and evaluation</li><li class=""css-cvpopp"">Model deployment and management</li><li class=""css-cvpopp"">Testing deployed models</li></ul>\n<p class=""chakra-text css-o3oz8b"">These tasks elaborate upon the steps in the data science process.  With Azure Machine Learning\'s Automated Machine Learning product, we can perform each of these steps in a web interface with limited knowledge of machine learning techniques.</p>\n<p class=""chakra-text css-o3oz8b"">We are able to <strong>ingest data</strong> from a variety of sources, including sources like our local filesystems or files available by URL.  <strong>Feature selection</strong> is as simple as toggling on and off the set of features to determine if it will play a role in model training.  <strong>Feature engineering</strong>, however, is quite limited with Automated ML; therefore, we perform feature engineering prior to generating the data files we want to use.  As far as <strong>training and evaluation</strong> go, we select the type of problem we want to solve:  classification, regression, or time series forecasting.  From there, Automated ML chooses different algorithms appropriate for the task and evaluates a variety of different algorithms.  Once we have a <strong>desirable model</strong>, we can <strong>deploy</strong> it using the same web interface.  Finally, we can <strong>test the model</strong> using Azure Machine Learning\'s built-in endpoint tester, or we can consume the endpoint in any language over HTTP.  Azure Machine Learning automatically generates code for you in C#, Python, and R, three common languages.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New Terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Feature engineering</strong>:  Creating new features from existing data.  This might include calculating new features, translating a street address into latitude and longitude, or parsing passages of text for meaning.</li><li class=""css-cvpopp""><strong>Feature selection</strong>:  Removing a column from consideration when training a model.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional Resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">If you want to learn more about Azure Automated ML, Microsoft has <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml"">an overview article<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> which covers the basics</li><li class=""css-cvpopp"">Although feature engineering in Azure Machine Learning Automated ML is limited, there is <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features"">some data featurization<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> which does happen.  And as we will see later on in the lesson, the Azure ML designer has a stronger story for feature engineering.</li></ul></div>']",[],https://www.youtube.com/embed/b3D1qBgEQVU
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.9  Quizzes: Core Tasks of Building a Machine Learning Solution,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following data sources are acceptable for creating a dataset in Azure Machine Learning?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Data stored locally on your computer</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Data stored in Azure Blob Storage</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Data accessible via a URL</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Data stored in Azure Open Datasets</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Removing a feature from consideration prior to modeling is also known as which of the following?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Feature removal</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Feature winnowing</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Feature engineering</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Feature selection</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Feature dismissal</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Place the following Azure ML Automated ML steps in their order of operations.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Model training</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Feature engineering + selection</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Model restructuring</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Model deployment</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Feature mitigation</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Endpoint testing</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Endpoint removal</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Model evaluation</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Step one:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Step two:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Step three:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Step four:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Step five:</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.10  Exercise: Model Training With AutoML,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">At this point in the lesson, we have reviewed critical machine learning concepts, approaches, and tasks.  With that foundation in place, we can create our first machine learning model!</p>\n<p class=""chakra-text css-o3oz8b"">For this exercise, we will use Microsoft\'s Automated Machine Learning, or AutoML.  This capability allows us to develop supervised machine learning models based on a specific set of use cases and using a specific set of algorithms.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Run an Automated Machine Learning Experiment</h2>\n<p class=""chakra-text css-o3oz8b"">For this exercise, as well as most exercises throughout the course of this nanodegree, we will use Azure Machine Learning.</p>\n<p class=""chakra-text css-o3oz8b"">Please ensure that you have a valid Azure account.  If you do not have one, you can <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/en-us/free/"">sign up for a one-month free trial<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, which includes enough in Azure credits to complete this nanodegree.</p>\n<p class=""chakra-text css-o3oz8b"">Navigate to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">Azure Machine Learning studio<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  After filling in your Azure account\'s e-mail address and password, you will be prompted to select a Machine Learning workspace.  If you have created a workspace in the past, you may continue to the task list below.  The following walkthrough assumes that you have not created a new Azure Machine Learning workspace.</p>\n<h3 class=""chakra-heading css-k57syw"">Part 1 - Create a New Workspace</h3>\n<p class=""chakra-text css-o3oz8b"">Select <strong>+ Create a new workspace</strong> to create a new workspace if you have not already created one.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new Azure Machine Learning workspace.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the <strong>Basics</strong> tab, select your subscription and then create a new resource group.  Give the workspace a memorable name.  You may choose a region closest to you or default to East US, as this is typically one of the first regions to get new features.  Create a new storage account, Key vault, Application insights, and Container registry.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new resource group, storage account, Key Vault, Application Insights, and container registry.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The dialogs for resource group, Key Vault, and Application Insights will be fairly simple, accepting a name and offering a <strong>Save</strong> or <strong>OK</strong> button.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new Key Vault.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">For the storage account dialog, choose <strong>Locally-redundant storage</strong> as the replication option.  This is a low-cost replication option intended for development work.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new storage account, using locally-redundant storage.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">When creating the container registry, choose the <strong>Basic</strong> SKU, as this will reduce costs.  The Basic SKU is intended for development.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new container registry, using the Basic SKU.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once you have filled out all of the sections, select <strong>Review + create</strong> and then <strong>Create</strong> to create the Azure Machine Learning workspace and its component parts.  After you have created everything, navigate to Azure Machine Learning studio.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Reviewing Azure Machine Learning studio.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Part 2 - Create an Automated ML experiment</h3>\n<p class=""chakra-text css-o3oz8b"">In this exercise, you will create and use a dataset of bike rentals.  The end goal is to use Automated ML to predict the number of bikes rented on a given day, solving a regression problem.</p>\n<p class=""chakra-text css-o3oz8b""><strong>Step 1:</strong> Select <strong>Automated ML</strong> from the <strong>Author</strong> menu.  Then, select <strong>+ New Automated ML run</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new Automated ML run.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Step 2:</strong> Select <strong>Create dataset</strong> and choose <strong>From web files</strong>.  Fill in a web URL of <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aka.ms/bike-rentals""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aka.ms/bike-rentals"">https://aka.ms/bike-rentals<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  Name the data set <strong>bike-rentals</strong> and move to the next section.  In the <strong>Column headers</strong> field, select <strong>Use headers from the first file</strong> and then go to the next section.  Include the columns aside from Path.  Move to the next section and then select <strong>Create</strong> to create the dataset.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Fill in the web URL and name the dataset <strong>bike-rentals</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Set column headers on the <strong>Settings and preview</strong> page.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select columns on the <strong>Schema</strong> page.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create the dataset.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Step 3:</strong> Select the new <strong>bike-rentals</strong> data set and continue on to the next section.  In the <strong>Configure run</strong> section, create a new experiment and name it <strong>bike-experiment</strong>.  Choose <strong>rentals</strong> as the target column and create a new compute cluster.  For the virtual machine type, choose <strong>Standard_DS2_v2</strong> from the list.  Give the compute cluster a name and continue to the next step.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new experiment and name it <strong>bike-experiment</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Set the target column to <strong>rentals</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new compute cluster.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Step 4:</strong> We want to predict a continuous, numeric value (specifically, the number of bikes rented) given our data set inputs.  To do so, we will choose <strong>Regression</strong> as the task type.</p>\n<p class=""chakra-text css-o3oz8b"">In the <strong>View additional configuration settings</strong> menu, change the primary metric to <strong>Normalized root mean square error</strong>.  In the <strong>Blocked algorithms</strong> box, select all but <code class=""chakra-code css-1u83yg1"">LightGBM</code> and <code class=""chakra-code css-1u83yg1"">RandomForest</code>.  Leave these two algorithms in place, as they are fast algorithms.  In the <strong>Exit criterion</strong> change the training job time to 1 hour.  Change the metric score threshold to 0.08.  In the Validation section, select <strong>Train-validation split</strong> as the validation type and then choose <strong>20</strong> as the percentage validation of data.  Then select <strong>Save</strong> to save these configuration settings.</p>\n<p class=""chakra-text css-o3oz8b"">In the <strong>View featurization settings</strong> menu, ensure that <strong>Enable featurization</strong> is selected and all columns are selected and set to <strong>Auto</strong> for feature type and impute with.  Then select <strong>Save</strong> to save these featurization settings.</p>\n<p class=""chakra-text css-o3oz8b"">Select <strong>Finish</strong> to create the new AutoML run.  After a few moments, the status should change from <strong>Not started</strong> to <strong>Running</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Choose the <strong>Regression</strong> task type.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Change the configuration settings to use <strong>Normalized root mean square error</strong> as the primary metric, block all algorithms but <strong>LightGBM</strong> and <strong>RandomForest</strong>, set the exit criteria, and choose <strong>Train-validation split</strong> with <strong>20</strong> percent of data used for validation.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Review the featurization settings to ensure that <strong>Enable featurization</strong> is on and all columns are set for <strong>Auto</strong> feature type and imputation.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Begin the AutoML run and ensure that the status changes from <strong>Not started</strong> to <strong>Running</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Wait for the experiment to complete.  This may take a while, so please be patient as AutoML runs.  Based on our exit criteria, it may run for over one hour before completing.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Step 6:</strong> After the experiment completes, review the <strong>Best model summary</strong> on the <strong>Details</strong> tab to see which algorithm generated the best model in this case.  Select the <strong>View all other metrics</strong> link and review the metrics for this model.</p>\n<p class=""chakra-text css-o3oz8b"">After reviewing the metrics, select the algorithm name to review model details.  Navigate through the tabs and familiarize yourself with the outputs of each tab, particularly the <strong>Model</strong>, <strong>Explanations</strong>, and <strong>Metrics</strong> tabs.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Review the <strong>Best model summary</strong> on the <strong>Details</strong> tab.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Review model, explanation, and metrics details for the winning model.</p></div>']","['https://video.udacity-data.com/topher/2021/February/602efbc7_azure-ml-create-workspace/azure-ml-create-workspace.png', 'https://video.udacity-data.com/topher/2021/February/602efdad_azure-ml-create-workspace-basics/azure-ml-create-workspace-basics.png', 'https://video.udacity-data.com/topher/2021/February/602efe2f_azure-ml-create-workspace-kv/azure-ml-create-workspace-kv.png', 'https://video.udacity-data.com/topher/2021/February/602efe7f_azure-ml-create-workspace-storage/azure-ml-create-workspace-storage.png', 'https://video.udacity-data.com/topher/2021/February/602efeda_azure-ml-create-workspace-cr/azure-ml-create-workspace-cr.png', 'https://video.udacity-data.com/topher/2021/February/602effef_azure-ml-studio/azure-ml-studio.png', 'https://video.udacity-data.com/topher/2021/February/60305a0f_azure-ml-new-automated-ml-run/azure-ml-new-automated-ml-run.png']",
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.11  Solution: Model Training With AutoML,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Create an Automated Machine Learning Run</h2>\n<p class=""chakra-text css-o3oz8b"">This first part of the solution pertains to creating an Automated Machine Learning run.  In this, we configure the dataset and create a new compute instance to execute our code.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Configure and Run an Automated Machine Learning Run</h2>\n<p class=""chakra-text css-o3oz8b"">The second part of this solution covers configuration details.  This includes selecting the type of problem to solve, defining the metrics for success, and choosing which algorithms the Automated Machine Learning process should try.  We then reviewed the best-fit model, including how well it performed on our data set.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">In this exercise, we created a new Automated ML run using Azure Machine Learning.  We set up some of the boundary parameters for what AutoML was allowed to do, gave it server resources it could use, and let it run wild.  What we got back was a model which serves our needs reasonably well.</p>\n<p class=""chakra-text css-o3oz8b"">AutoML is great as a first step in machine learning:  determining whether it is worth continuing down a path.  If AutoML finds a solution which does well enough given your desired measure (for example, normalized root mean square error), this is an indication that you have a high likelihood of a viable machine learning product.  From there, you can use your machine learning expertise to fine-tune the model and make it even better.  If it fails to find a reasonable solution, this is an indicator that the problem may be too big of a challenge given the available data.  It should bring us back to the planning stage where we can decide how (or even whether) to proceed.</p></div>']",[],https://www.youtube.com/embed/WCXUzyB8IVc
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.12  Exercise: Deploying And Testing A Model,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the prior exercise, we used Automated ML to find the best model over a fixed amount of time.  Now that we have a candidate model to work with, we can use Azure Machine Learning to host that model using either Azure Container Instances (ACI) or Azure Kubernetes Service (AKS).  In a production scenario, we generally prefer to use Azure Kubernetes Service because it allows us to scale up to production workloads more easily and will be more resilient to failure than Azure Container Instances.  For a development workload, Azure Container Instances are less expensive and easier to use, as there are fewer administrative considerations.  Therefore, for this exercise, we will use Azure Container Instances to deploy the bike rental model we created in the previous exercise.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Deploying a Model</h3>\n<p class=""chakra-text css-o3oz8b"">Navigate to the <strong>Automated ML</strong> option on the <strong>Author</strong> menu.  Then, choose the <strong>bike-experiment</strong> run you completed in the prior exercise.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Choose the run associated with your bike experiment.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">From there, select the algorithm associated with the best model.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Choose the algorithm associated with your best model.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select the <strong>Deploy</strong> option to deploy this model via Azure Container Instances.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Deploy the best model.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the <strong>Deploy a model</strong> menu, enter the name <strong>predict-rentals</strong>.  Change the compute type to <strong>Azure Container Instance</strong> and select the <strong>Enable authentication</strong> option.  Select <strong>Deploy</strong> to deploy the model.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Deploy a model to an Azure Container Instance.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select the Run associated with the prior exercise\'s experiment.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select the best model.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Deploy the model to Azure Container Instances.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once the model has deployed, you should see that the deploy status is <strong>Running</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The deployed model is running and soon will be ready to make predictions.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Wait for the model status to change to <strong>Succeeded</strong>.  Once the status changes, the model is now active and ready for use.  Note that you may need to select <strong>Refresh</strong> periodically to update the status.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Creating a Compute Instance</h3>\n<p class=""chakra-text css-o3oz8b"">Before we can test the predictive model, we need to create a compute instance that we can use for executing code in notebooks.  To do this, navigate to the <strong>Compute</strong> tab on the Manage menu and select <strong>+ New</strong> under <strong>Compute instances</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new compute instance.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the <strong>Virtual Machine</strong> menu, leave all of the settings alone:  use <strong>CPU</strong> as the virtual machine type and choose <strong>Standard_DS3_v2</strong> (or whatever the default value is) and then select <strong>Next</strong>.  Provide a name for this compute instance and then select <strong>Create</strong>.  After you have created this compute instance, the status should read <strong>Running</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Set up a virtual machine using the default size.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Name the compute instance and wait for it to be created.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Testing the Model</h3>\n<p class=""chakra-text css-o3oz8b"">Now that we have a model and a compute instance in place, we can test that model.  Navigate to the <strong>Endpoints</strong> item in the <strong>Assets</strong> menu.  You should see <strong>predict-rentals</strong> in the endpoints list.  Select it.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select the predict-rentals endpoint.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">From there, navigate to the <strong>Consume</strong> tab.  You will need to copy the REST endpoint and the Primary key for your service.  Copy each of those values and paste them into a text file.  Then, navigate to the <strong>Notebooks</strong> tab of the Author menu.  In the <strong>+ Create</strong> menu, select <strong>Create new file</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new file.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Name the file <strong>Test-Bikes</strong> and choose <strong>Notebook (*.ipynb)</strong> to create a new notebook named Test-Bikes.ipynb.  Select <strong>Create</strong> to create the notebook.  Ensure that your <strong>Compute</strong> instance is selected and in a Running state.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Ensure that the Compute is set to a status of Running.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Copy and paste the following code into the first cell of your notebook.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>endpoint </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'YOUR_ENDPOINT\'</span><span> </span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#Replace with your endpoint</span><span>\n</span><span>key </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'YOUR_KEY\'</span><span> </span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#Replace with your key</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> json\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> requests\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#An array of features based on five-day weather forecast</span><span>\n</span><span>x </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">2022</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">6</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">2</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.344167</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.363625</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.805833</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.160446</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(2, 124, 124);"">2</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">2022</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">2</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.363478</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.353739</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.696087</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.248539</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(2, 124, 124);"">3</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">2022</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.196364</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.189405</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.437273</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.248309</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(2, 124, 124);"">4</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">2022</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">2</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.2</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.212122</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.590435</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.160296</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(2, 124, 124);"">5</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">2022</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">3</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.226957</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.22927</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.436957</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0.1869</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#Convert the array to JSON format</span><span>\n</span><span>input_json </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> json</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>dumps</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(15, 43, 61);"">{</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""data""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> x</span><span class=""token"" style=""color: rgb(15, 43, 61);"">}</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#Set the content type and authentication for the request</span><span>\n</span><span>headers </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">{</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Content-Type""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""application/json""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>        </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Authorization""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Bearer ""</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">+</span><span> key</span><span class=""token"" style=""color: rgb(15, 43, 61);"">}</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#Send the request</span><span>\n</span><span>response </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> requests</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>post</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> input_json</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> headers</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>headers</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#If we got a valid response, display the predictions</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">if</span><span> response</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>status_code </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">==</span><span> </span><span class=""token"" style=""color: rgb(2, 124, 124);"">200</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>    y </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> json</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>loads</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>response</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>json</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Predictions:""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> i </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> </span><span class=""token"" style=""color: rgb(0, 121, 162);"">range</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(0, 121, 162);"">len</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>x</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>        </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">"" Day: {}. Predicted rentals: {}""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span class=""token"" style=""color: rgb(0, 121, 162);"">format</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>i</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">+</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(0, 121, 162);"">max</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(0, 121, 162);"">round</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>y</span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""result""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span>i</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">else</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>response</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Replace <code class=""chakra-code css-1u83yg1"">YOUR_ENDPOINT</code> with the endpoint URL and <code class=""chakra-code css-1u83yg1"">YOUR_KEY</code> with your primary key.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Fill in the notebook cell with Python-based test code.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">This code generates sample data for five days and then calls our prediction endpoint.  Run the notebook by pressing the play button to the left of the notebook cell.  This will generate predictions for each of the days.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Predicted rental counts for five sample days.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Note that, depending on the algorithm and parameters of your model, the results may look slightly different from these numbers.</p>\n<h3 class=""chakra-heading css-k57syw"">Cleanup</h3>\n<p class=""chakra-text css-o3oz8b"">After you are done, return to the <strong>Compute</strong> tab in the Manage menu, select your compute instance, and select <strong>Stop</strong> to shut down the instance.  This will stop the existing compute instance, which also stops you from being charged for that compute power.</p>\n<p class=""chakra-text css-o3oz8b"">Then, navigate to the <strong>Endpoints</strong> tab in the Assets menu, select the <strong>predict-rentals</strong> endpoint, and select <strong>Delete</strong> to delete the endpoint.  This will destroy the endpoint, preventing you from incurring further costs for it.</p>\n<p class=""chakra-text css-o3oz8b"">Note that the compute cluster you created can still remain--it shuts down idle nodes after 120 seconds by default, so you will not incur any extra charges once those nodes shut themselves down.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Review the endpoint and copy the REST endpoint and Primary key to a text file.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new notebook entitled Test-Bikes.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Paste in the provided Python code and replace the endpoint and key variables with your values.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Run the notebook and observe the predicted rental counts.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Stop the compute instance.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Delete the predict-rentals endpoint.</p></div>']","['https://video.udacity-data.com/topher/2021/February/603062bb_azure-ml-run-12/azure-ml-run-12.png', 'https://video.udacity-data.com/topher/2021/February/60306318_azure-ml-deploy-best-algorithm/azure-ml-deploy-best-algorithm.png', 'https://video.udacity-data.com/topher/2021/February/6030635a_azure-ml-deploy/azure-ml-deploy.png', 'https://video.udacity-data.com/topher/2021/February/603063e4_azure-ml-deploy-model/azure-ml-deploy-model.png', 'https://video.udacity-data.com/topher/2021/February/603064a2_azure-ml-deployment-running/azure-ml-deployment-running.png', 'https://video.udacity-data.com/topher/2021/February/60306829_azure-ml-new-compute-instance/azure-ml-new-compute-instance.png', 'https://video.udacity-data.com/topher/2021/February/603065d0_azure-ml-endpoints/azure-ml-endpoints.png', 'https://video.udacity-data.com/topher/2021/February/6030670d_azure-ml-create-new-notebook/azure-ml-create-new-notebook.png', 'https://video.udacity-data.com/topher/2021/February/60306bf5_azure-ml-compute/azure-ml-compute.png', 'https://video.udacity-data.com/topher/2021/February/60306cbe_azure-ml-notebook-test/azure-ml-notebook-test.png', 'https://video.udacity-data.com/topher/2021/February/60306e01_azure-ml-prediction/azure-ml-prediction.png']",
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.13  Solution: Deploying And Testing A Model,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Deploying an Automated Machine Learning Run</h2>\n<p class=""chakra-text css-o3oz8b"">This part of the solution covers the process of deploying the most successful model from an Automated Machine Learning run.  We used an Azure Container Instance to host the model and created a compute instance to perform our testing.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Testing a Deployed Model</h2>\n<p class=""chakra-text css-o3oz8b"">In this part of the solution, we created an Azure Machine Learning notebook to interact with the deployed model.  This allowed us to test the endpoint hosting our model and ensure that applications will be able to interact with it.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">In this exercise, we deployed a model that we created using Azure Machine Learning\'s Automated Machine Learning product.  We created a compute cluster to host the code, and subsequently created a compute instance in order to test the deployment.</p>\n<p class=""chakra-text css-o3oz8b"">The good news is that the process we followed for deploying an Automated Machine Learning model is exactly the same for other Azure Machine Learning models.  We can apply these same techniques in the next part of the lesson, in which we work with the Azure Machine Learning designer.</p></div>']",[],https://www.youtube.com/embed/TZKZOtzM6Es
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.14  A Lap Around Azure Machine Learning,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">Azure Machine Learning studio provides an integrated development environment for each step of the machine learning process.  The home screen is made up of three sections:  quick actions along the top, a menu with major elements of Azure ML on the left-hand side, and recent runs along the bottom.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The Azure ML studio home screen, annotated.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Creating a dataset is possible from the <strong>Datasets</strong> menu.  As we learned in the <strong>Model Training with AutoML</strong> exercise, Azure Machine Learning can load data from several types of sources.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Creating a dataset from Azure Open Datasets.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once a data set has been loaded, we can select the <strong>Explore</strong> option to review summary statistics for each column in the dataset.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Summary statistics for each column in the penguin measurements data set.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure ML works from a metaphor of pipelines:  we create training pipelines to train our models and inference pipelines to generate predictions.  Building a pipeline in the Azure ML designer is straightforward:  start by drag components onto the canvas.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Dragging the penguin data component onto the Azure ML designer canvas.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Then, after you have multiple components, link them together.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Connect penguin data to the Select Columns in Dataset component by linking the output node from one into the input node of the other.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Not all nodes are interchangeable.  For example, some components take a dataset in one node and something else in a different node, and you can\'t mix the two up.  If there is some incompatibility, Azure ML will not allow you to link an output to an invalid input.</p>\n<p class=""chakra-text css-o3oz8b"">Data transformation can happen through built-in Azure ML components or by creating your own scripts in either Python or R.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Python and R are both supported as scripting languages in Azure ML.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">After performing data transformation, we are able to train and evaluate models.  The upcoming exercises will take you through the process of doing this for structured data (in the form of a classification model) and unstructured data (clustering).</p>\n<p class=""chakra-text css-o3oz8b"">Once a model has been trained, we can deploy it as a predictive service using either Azure Container Instances or Azure Kubernetes Service.  The best advice is to use Azure Container Instances for development or when simply trying it out on your own, as this reduces the level of administrative effort required and will be less expensive.  In production, Azure Kubernetes Service is a better idea because it will be more robust to service failure and can scale up and down more effectively than Azure Container Instances.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Creating a real-time endpoint.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New Terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Azure Machine Learning Studio</strong>:  The integrated development environment (IDE) for Azure Machine Learning.</li><li class=""css-cvpopp""><strong>Pipeline (Azure ML)</strong>:  A collection of components connected together in a defined order.  The metaphor represents how data moves from a source (an initial dataset) and flows through components until it reaches a destination.  There are two types of pipeline:  <strong>training pipelines</strong> and <strong>inference pipelines</strong>.</li><li class=""css-cvpopp""><strong>Run (Azure ML)</strong>:  An attempt to train a model in Azure Machine Learning.  This can be done through a pipeline in the Azure ML designer or through Automated ML.</li><li class=""css-cvpopp""><strong>Experiment (Azure ML)</strong>:  A collection of trials used to validate a user\'s hypothesis.  An experiment may contain multiple runs of pipelines.</li><li class=""css-cvpopp""><strong>Compute (Azure ML)</strong>:  Virtual machine resources which are dedicated to performing tasks in Azure Machine Learning.  Compute may include individual virtual machines (VMs), typically configured as data science VMs, or it may include a cluster of VMs intended for training and inference pipeline executions.</li><li class=""css-cvpopp""><strong>Data Labeling</strong>:  This functionality allows you to label images as part of an image classification project.</li><li class=""css-cvpopp""><strong>Linked Services</strong>:  This functionality allows you to integrate Azure Machine Learning with other Azure services.  At present, the only linked service offering is to connect to Azure Synapse Analytics, which is a modern data warehousing offering on Azure.</li><li class=""css-cvpopp""><strong>Pipeline Asset:</strong>  A component available within Azure Machine Learning.  This includes datasets you have imported, sample datasets which come with the service, and different components to transform, train, evaluate, and deploy models.</li><li class=""css-cvpopp""><strong>Node (input, output)</strong>:  An input or output connection point on a component.  Each component will have 0 to 3 input nodes and 0 to 3 output nodes.  Each input or output node has a specific type, such as <strong>DataFrameDirectory</strong>, <strong>TransformationDirectory</strong>, or <strong>UntrainedModelDirectory</strong>.  An input of <strong>DataFrameDirectory</strong> can only attach to an output of the same type.</li><li class=""css-cvpopp""><strong>Source node</strong>:  A node with no inputs.  An example of a source node is any dataset you bring onto the canvas.</li><li class=""css-cvpopp""><strong>Sink node</strong>:  A node with no outputs.  An example of a sink node is <strong>Web Service Output</strong>.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional Resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">If you want to dive into the documentation for Azure Machine Learning, <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/machine-learning/"">it is available on Microsoft Docs<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  This also includes tutorials and integration with other Microsoft products, such as Visual Studio Code and Power BI.</li></ul></div>']","['https://video.udacity-data.com/topher/2021/March/604187e6_azure-ml-home/azure-ml-home.png', 'https://video.udacity-data.com/topher/2021/March/6041887d_azure-ml-create-dataset/azure-ml-create-dataset.png', 'https://video.udacity-data.com/topher/2021/March/604188d1_azure-ml-explore-penguin-data/azure-ml-explore-penguin-data.png', 'https://video.udacity-data.com/topher/2021/March/6041893e_azure-ml-pipeline-component/azure-ml-pipeline-component.png', 'https://video.udacity-data.com/topher/2021/March/6041897d_azure-ml-pipeline-nodes/azure-ml-pipeline-nodes.png', 'https://video.udacity-data.com/topher/2021/March/60418a32_azure-ml-python-r/azure-ml-python-r.png', 'https://video.udacity-data.com/topher/2021/March/604197bd_azure-ml-real-time-endpoint/azure-ml-real-time-endpoint.png']",https://www.youtube.com/embed/m5Ea5XYZRTE
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.15  Quizzes: A Lap Around Azure Machine Learning,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Machine Learning uses the metaphor of pipelines to describe the data science process.  From the list below, choose the TWO pipelines you can create.</p>\n<p class=""chakra-text css-o3oz8b"">(Select the two that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Inference pipelines</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Deployment pipelines</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Training pipelines</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Data preparation pipelines</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Retraining pipelines</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.16  Exercise: Create A Classification Model,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In this exercise, we will use the Azure Machine Learning designer to create a model to classify data.</p>\n<p class=""chakra-text css-o3oz8b"">First, navigate to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">Azure Machine Learning studio<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> and select the workspace you created.</p>\n<h3 class=""chakra-heading css-k57syw"">Create a Compute Cluster</h3>\n<p class=""chakra-text css-o3oz8b"">Select the <strong>Compute</strong> tab from the Manage menu and then choose <strong>Compute clusters</strong>.  Ensure that you have at least one compute cluster.  If you do not have one, use the <strong>+ New</strong> option to create a new cluster.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">An Azure ML compute cluster exists.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Create a Dataset</h3>\n<p class=""chakra-text css-o3oz8b"">Navigate to the <strong>Datasets</strong> tab.  Then, select <strong>+ Create dataset</strong> and choose <strong>From web files</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a dataset from web files.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">For Web URL, enter <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aka.ms/diabetes-data""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aka.ms/diabetes-data"">https://aka.ms/diabetes-data<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.  Name the dataset <strong>diabetes</strong>.  Then select <strong>Next</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">On the <strong>Settings and preview</strong> tab, change the Column headers setting to <strong>Use headers from the first file</strong>.  Then select <strong>Next</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">In the <strong>Schema</strong> tab, ensure that all columns aside from Path are included and then select <strong>Next</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">Finally, select <strong>Create</strong> to create the dataset.</p>\n<p class=""chakra-text css-o3oz8b"">Select the dataset name and then choose <strong>Explore</strong> to review the data set.  What we intend to do is determine whether a person is likely to be diabetic (that is, <strong>Diabetic = 1</strong>) based on inputs such as number of pregnancies, plasma glucose level, body mass indicator (BMI), and more.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">A sample of the diabetes data set.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a compute cluster if one does not already exist.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Import the diabetes data set.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Create a Pipeline</h3>\n<p class=""chakra-text css-o3oz8b"">Select the <strong>Designer</strong> tab on the Author menu and then select <strong>+</strong> to create a new pipeline.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new Azure ML pipeline.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Change the draft name to <strong>diabetes-pipeline</strong>.  Then, select the <strong>Select compute target</strong> option, choose your compute cluster, and select <strong>Save</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Name the pipeline and select the compute target.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Expand the <strong>Datasets</strong> menu and drag the <strong>diabetes</strong> dataset on the canvas.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Drag the diabetes dataset onto the designer canvas.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The next thing we want to do is normalize our data.  Our inputs have a wide range of values.  Some numbers, like the number of pregnancies, has a fairly narrow range, typically 0-3 but with values occasionally reaching 8 or 9.  Others, like plasma glucose level or serum insulin, span a range of hundreds of possible values.  This can potentially cause a problem with some classification algorithms, as they can be swayed by variables with larger values and larger ranges.  To combat this, we will <strong>normalize</strong> the data--that is, ensure every input ranges from 0 to 1.</p>\n<p class=""chakra-text css-o3oz8b"">To do this, go to the <strong>Data Transformation</strong> section and drag a <strong>Normalize Data</strong> component onto the canvas and connect the output from the bottom of the <code class=""chakra-code css-1u83yg1"">diabetes</code> dataset to the input for <code class=""chakra-code css-1u83yg1"">Normalize Data</code>.  Then, set the transformation method to <strong>MinMax</strong> and select <strong>Edit column</strong> in the Columns to transform menu.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Normalize data in the pipeline.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the Include down-down list, choose <strong>All columns</strong> and then select <strong>Save</strong> to normalize each column.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Normalize all columns.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the <strong>Data Transformation</strong> section, drag a <strong>Split Data</strong> component onto the canvas and connect the output from the bottom of the <code class=""chakra-code css-1u83yg1"">Normalize Data</code> component to the input for <code class=""chakra-code css-1u83yg1"">Split Data</code>.  Then, select the <code class=""chakra-code css-1u83yg1"">Split Data</code> component and change the fraction of rows in the first output dataset to <code class=""chakra-code css-1u83yg1"">0.7</code> and the random seed to <code class=""chakra-code css-1u83yg1"">1804</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Bring a Split Data component onto the pipeline.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The next step is to add pipeline components to train and score the model.  First, go to the <strong>Model Training</strong> section and drag on a <strong>Train Model</strong> component.  Then, inside <strong>Machine Learning Algorithms</strong>, select <strong>Two-Class Boosted Decision Tree</strong>.  Finally, inside the <strong>Model Scoring &amp; Evaluation</strong> tab, bring in the <strong>Score Model</strong> component.  Hook up the components as in the image below.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">All of the components are now on the canvas.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Before we can run this, we need to configure the <strong>Train Model</strong> component.  Select it and then select <strong>Edit column</strong> for the label column.  Choose <strong>Diabetic</strong> as the label column and select <strong>Save</strong>.  This will remove the yellow triangle warning for the Train Model component.</p>\n<p class=""chakra-text css-o3oz8b"">Once that is complete, select the <strong>Submit</strong> button to run the training pipeline.  For experiment, select <strong>Create new</strong> and name it <strong>diabetes-experiment</strong>.  Then select <strong>Submit</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new experiment.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Note that this may take several minutes to complete.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create and name a pipeline.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add the <strong>diabetes</strong> data set to the canvas.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add a configure a <strong>Normalize Data</strong> component.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add and configure a <strong>Split Data</strong> component.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add a <strong>Two-Class Boosted Decision Tree</strong> component.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add a configure a <strong>Train Model</strong> component.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add a <strong>Score Model</strong> component.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Submit the run and watch it to completion.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Reviewing the Score</h3>\n<p class=""chakra-text css-o3oz8b"">After the model has finished, right-click on the <strong>Score Model</strong> component.  Then select <strong>Visualize</strong> and <strong>Scored dataset</strong>.  Scroll to the right and you can see the <strong>Diabetic</strong> label as well as the <strong>Scored Labels</strong> and <strong>Scored Probabilities</strong> columns.  The Scored Labels column provides you with Azure ML\'s best guess on test data using this model, and Scored Probabilities shows you the estimated likelihood that this prediction is accurate.</p>\n<p class=""chakra-text css-o3oz8b"">In the image below, we can see that not all predictions were correct--in this case, the model predicted that a patient would have diabetes with a likelihood of 61.5% but the patient did not have diabetes.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Most of the predictions are correct, but not all of them.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">To get a better understanding of the results, add an <strong>Evaluate Model</strong> component from the <strong>Model Scoring &amp; Evaluation</strong> and connect the output of Score Model to the left input of Evaluate Model.  Then <strong>Submit</strong> the pipeline again.  Because the rest of the components have remained the same and the underlying dataset has not changed, the other components will be recycled, meaning that the process should take less time than before as we only need to run the Evaluate Model component.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Evaluate the scored model.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">After the pipeline finishes, right-click on Evaluate Model and select <strong>Visualize</strong> and then <strong>Evaluation results</strong>.  This shows how accurate the model was:  95.2% of the time, the model chose correctly.  92.3% of the time, when the model predicted that a patient had diabetes, the model was correct--this is called Precision.  93.2% of the time, when a patient had diabetes, the model discovered this--this is the definition of Recall.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Review the model, including key scores like accuracy, Precision, and Recall.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Visualize the scored dataset.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add an <strong>Evaluate Model</strong> component and re-run the pipeline.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">View evaluation results.</p></div>']","['https://video.udacity-data.com/topher/2021/February/60316941_azure-ml-compute-cluster/azure-ml-compute-cluster.png', 'https://video.udacity-data.com/topher/2021/February/60316c6d_azure-ml-create-dataset-web/azure-ml-create-dataset-web.png', 'https://video.udacity-data.com/topher/2021/February/60316da6_azure-ml-diabetes/azure-ml-diabetes.png', 'https://video.udacity-data.com/topher/2021/February/60316f61_azure-ml-new-pipeline/azure-ml-new-pipeline.png', 'https://video.udacity-data.com/topher/2021/February/60316fdc_azure-ml-pipeline-settings/azure-ml-pipeline-settings.png', 'https://video.udacity-data.com/topher/2021/February/6031705d_azure-ml-pipeline-dataset/azure-ml-pipeline-dataset.png', 'https://video.udacity-data.com/topher/2021/February/60317251_azure-ml-pipeline-normalize/azure-ml-pipeline-normalize.png', 'https://video.udacity-data.com/topher/2021/February/60317319_azure-ml-pipeline-normalize-all-columns/azure-ml-pipeline-normalize-all-columns.png', 'https://video.udacity-data.com/topher/2021/February/60317302_azure-ml-pipeline-split-data/azure-ml-pipeline-split-data.png', 'https://video.udacity-data.com/topher/2021/February/6031757b_azure-ml-pipeline-score/azure-ml-pipeline-score.png', 'https://video.udacity-data.com/topher/2021/February/60317687_azure-ml-pipeline-submit-2/azure-ml-pipeline-submit-2.png', 'https://video.udacity-data.com/topher/2021/February/60317d18_azure-ml-scored-dataset/azure-ml-scored-dataset.png', 'https://video.udacity-data.com/topher/2021/February/60317e9b_azure-ml-pipeline-evaluate/azure-ml-pipeline-evaluate.png', 'https://video.udacity-data.com/topher/2021/February/603180f1_azure-ml-pipeline-evaluation-results/azure-ml-pipeline-evaluation-results.png']",
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.17  Solution: Create A Classification Model,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Prepare the Designer</h2>\n<p class=""chakra-text css-o3oz8b"">In this first part of the solution, we perform preparatory steps, such as ensuring that a compute cluster is available, loading in the dataset, and creating a new pipeline in the Azure Machine Learning designer.  We can then link up the pipeline to our compute target.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Train a Model</h2>\n<p class=""chakra-text css-o3oz8b"">The next part of the solution covers model training using the Azure Machine Learning designer.  This includes bringing in the dataset, performing data transformations such as normalization, and training the classification model using a two-class boosted decision tree algorithm.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Evaluate the Model</h2>\n<p class=""chakra-text css-o3oz8b"">The final part of the solution covers model evaluation.  Once the training pipeline is ready, we can create a new experiment and execute the pipeline.  This will run each component, and we can review the Score Model component to get an understanding of how well the model performed.  The Evaluate Model component will then provide us with aggregate details, which we can use to determine the quality of the model we created.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">At this point, we have used the Azure Machine Learning tool to perform several steps in the data science process.  We have collected data, transformed that data into a proper shape, trained a model based on that transformed data, and evaluated the model.</p>\n<p class=""chakra-text css-o3oz8b"">From here, we could continue the process by deploying this model and making it available to end users.</p></div>']",[],https://www.youtube.com/embed/zSWTzTf-O5g
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.18  Exercise: Create A Clustering Model,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">This exercise will follow up on techniques learned in this lesson, including in the prior exercise.  In this case, we will build a clustering model.  After that, we will deploy the model as an endpoint and test it out.</p>\n<h3 class=""chakra-heading css-k57syw"">Create a Dataset</h3>\n<p class=""chakra-text css-o3oz8b"">The first step is to create a new data set.  Create the dataset <strong>From web files</strong>.  Use <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aka.ms/penguin-data""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aka.ms/penguin-data"">https://aka.ms/penguin-data<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> as the Web URL and name the data set <strong>penguin-data</strong>.  Use headers from the first file and include all columns other than Path.</p>\n<p class=""chakra-text css-o3oz8b"">This dataset has five features:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>CulmenLength</strong>:  the length of a penguin\'s bill in millimeters.</li><li class=""css-cvpopp""><strong>CulmenDepth</strong>:  the depth of a penguin\'s bill in millimeters.</li><li class=""css-cvpopp""><strong>FlipperLength</strong>:  the length of a penguin\'s flipper in millimeters.</li><li class=""css-cvpopp""><strong>BodyMass</strong>:  the weight of a penguin in grams.</li><li class=""css-cvpopp""><strong>Species</strong>:  0 for Amelie, 1 for Gentoo, 2 for Chinstrap.  Note that we will disregard this feature in the model.</li></ul>\n<h3 class=""chakra-heading css-k57syw"">Train a Model</h3>\n<p class=""chakra-text css-o3oz8b"">The next step is to train a clustering model based on this clustering data.  Create a new pipeline and name it <strong>penguin-pipeline</strong>.  Use the same compute target you used for the prior exercise.  Then, add a <strong>Select Columns in Dataset</strong> component from the <strong>Data Transformation</strong> list.  In the properties for this component, include all columns but species.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select specified columns by name.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">After this, add a <strong>Clean Missing Data</strong> component.  In the <strong>Columns to be cleaned</strong> list, include <strong>All columns</strong>.  Then, set the <strong>Cleaning mode</strong> to <strong>Remove entire row</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">The next component is a <strong>Normalize Data</strong> component.  Feed the left output of Clean Missing Data into the input for Normalize Data.  Include <strong>All columns</strong> in the <strong>Columns to transform</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">From there, connect a <strong>Split Data</strong> component to the left output of the Normalize Data component.  Set the fraction of rows in the first output dataset to <strong>0.7</strong> and change the random seed to <strong>1467</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">From the <strong>Machine Learning Algorithms</strong> section, drag in a <strong>K-Means Clustering</strong> algorithm.  In the properties for this component, set the <strong>Number of centroids</strong> to 3, as there are three species of penguin.  In a real-world scenario in which we do not know the distinct number of outputs, we would want to experiment with different numbers of centroids.</p>\n<p class=""chakra-text css-o3oz8b"">Then, in <strong>Model Training</strong>, bring in a <strong>Train Clustering Model</strong> component.  Hook the algorithm up to the left input for Train Clustering Model and the left output of Split Data into the right input for the component.  In the settings, edit the Column set and include <strong>All columns</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">Finally, in the <strong>Model Scoring &amp; Evaluation</strong> section, drag on an <strong>Assign Data to Clusters</strong> component.  The left input should be the left output from the Train Clustering Model component, and the right input should be Split Data\'s right output.  The end result should appear like the following:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">A designed pipeline for a penguin classifier model.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once you have the pipeline in place, select <strong>Submit</strong> and create a new experiment named <strong>penguin-experiment</strong>.  Select <strong>Submit</strong> once more and let Azure Machine Learning work through the process of training our penguin clustering model.  This may take several minutes.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Import the <strong>penguin-data</strong> dataset.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a new pipeline.  Select all columns but Species, remove rows with missing data, and normalize data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Split the data.  Train a clustering model using k-means clustering as the algorithm using 70% of the data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Use the trained model and the remaining 30% of the data to assign that test data to clusters.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Submit the pipeline and let it finish processing.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Create an Inference Pipeline and Deploy to an Endpoint</h3>\n<p class=""chakra-text css-o3oz8b"">Once we have trained a model, we can deploy it as an endpoint.  In order to do that, we must first create an inference pipeline.</p>\n<p class=""chakra-text css-o3oz8b"">In the <strong>Create inference pipeline</strong> drop-down list, select <strong>Real-time inference pipeline</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a real-time inference pipeline.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">This will create a new pipeline, along with a <strong>Web Service Input</strong> and <strong>Web Service Output</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The components of a real-time inference pipeline.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Delete the <strong>penguin-data</strong> dataset and replace it with <strong>Enter Data Manually</strong> from the <strong>Data Input and Output</strong> section.  In the properties, paste in the following data:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-csv"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token value"">CulmenLength</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">CulmenDepth</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">FlipperLength</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">BodyMass</span><span>\n</span><span></span><span class=""token value"">39.1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">18.7</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">181</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">3750</span><span>\n</span><span></span><span class=""token value"">49.1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">14.8</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">220</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">5150</span><span>\n</span><span></span><span class=""token value"">46.6</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">17.8</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">193</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span class=""token value"">3800</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Enter data manually.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select <strong>Submit</strong> to run the inference pipeline on your compute cluster.  Select the existing <strong>penguin-experiment</strong> experiment from the <strong>Existing experiment</strong> drop-down list and then select <strong>Submit</strong>.  This pipeline will take several minutes to complete.</p>\n<p class=""chakra-text css-o3oz8b"">After the inference pipeline completes, right-click on <strong>Assign Data to Clusters</strong> and choose <strong>Visualize</strong> and then <strong>Results dataset</strong> to see the predicted clusters for your three penguins.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Review the cluster assignments for our sample data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select <strong>Deploy</strong> from the menu bar to deploy your model.  In the deployment settings, create a new real-time endpoint named <strong>penguin-endpoint</strong> with a Compute type of <strong>Azure Container Instance</strong>.  Then select <strong>Deploy</strong>.  This will take several minutes.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Set up a real-time endpoint.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once the deployment succeeds, select the <strong>view real-time endpoint</strong> link to view the newly-created endpoint.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">View the real-time endpoint.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Navigate to the <strong>Test</strong> tab and enter values for each variable.  Then select <strong>Test</strong> to run the test.  This will return a prediction of the suggested cluster.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Run a live test against the real-time endpoint.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Teardown</h3>\n<p class=""chakra-text css-o3oz8b"">After you have finished reviewing the endpoint, navigate back to <strong><strong>the</strong> Endpoints <strong>option in the Assets menu.  Select the</strong> penguin-endpoint <strong>endpoint and select</strong> Delete</strong> to remove the endpoint.  This will stop any additional expenditures as a result of hosting the real-time endpoint.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a real-time inference pipeline, including a test with manual data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Deploy the real-time inference pipeline to an Azure Container Instance.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Test the resultant endpoint to ensure that it returns a result.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Delete the endpoint.</p></div>']","['https://video.udacity-data.com/topher/2021/February/6032cb6f_auzre-ml-select-columns/auzre-ml-select-columns.png', 'https://video.udacity-data.com/topher/2021/February/6032cd76_azure-ml-penguin-pipeline/azure-ml-penguin-pipeline.png', 'https://video.udacity-data.com/topher/2021/February/6032dccf_azure-ml-real-time-pipeline/azure-ml-real-time-pipeline.png', 'https://video.udacity-data.com/topher/2021/February/6032dd43_azure-ml-real-time-pipeline-2/azure-ml-real-time-pipeline-2.png', 'https://video.udacity-data.com/topher/2021/February/6032ddbb_azure-ml-enter-data-manually/azure-ml-enter-data-manually.png', 'https://video.udacity-data.com/topher/2021/February/6032f1a7_azure-ml-prediction-2/azure-ml-prediction-2.png', 'https://video.udacity-data.com/topher/2021/February/6032f204_azure-ml-deploy-2/azure-ml-deploy-2.png', 'https://video.udacity-data.com/topher/2021/February/6032f405_azure-ml-view-endpoint/azure-ml-view-endpoint.png', 'https://video.udacity-data.com/topher/2021/February/6032f44b_azure-ml-test-endpoint/azure-ml-test-endpoint.png']",
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.19  Solution: Create A Clustering Model,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Prepare the Designer and Transform Data</h2>\n<p class=""chakra-text css-o3oz8b"">In this first section, we load a new penguin details dataset and create a new pipeline in the Azure Machine Learning designer.  We then begin the process of transforming data, performing activities such as feature selection, cleaning missing data, and normalizing data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Complete the Training Pipeline</h2>\n<p class=""chakra-text css-o3oz8b"">After normalizing data, we move on to the rest of the training pipeline.  This includes splitting data, training a model based on the k-means clustering algorithm, and assigning data to clusters.  We end this section by reviewing the results of the <strong>Assign Data to Clusters</strong> component.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Creating an Inference Pipeline and Deploying a Model</h2>\n<p class=""chakra-text css-o3oz8b"">This final section involves creating a real-time inference pipeline, which transforms a training pipeline into one which is useful for performing predictions.  We create some quick test data and review the results as the inference pipeline runs.</p>\n<p class=""chakra-text css-o3oz8b"">After creating the inference pipeline, we deploy the pipeline using an Azure Container Instance.  We end by testing the pipeline using the built-in test page.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">The exercises in this lesson have shown how to create and deploy a machine learning model from end to end.  We started with Azure Machine Learning\'s Automated Machine Learning to do most of the work for us and then moved on to deploying its resultant model to make it available for use.  Then, we looked at Azure Machine Learning\'s designer, creating a classification model.  We concluded the exercises in this lesson by creating a clustering model and deploying it as well.</p></div>']",[],https://www.youtube.com/embed/ImzoGBM6cI0
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.20  ML Edge Cases,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">In this lesson, we provided three pieces of advice around machine learning edge cases.</p>\n<h3 class=""chakra-heading css-k57syw"">Don\'t Get Stuck on Categories</h3>\n<p class=""chakra-text css-o3oz8b"">Categorizing approaches is valuable when trying to gain an understanding of machine learning, but there are complexities in the real world that we should appreciate.  Generally, we will work on supervised learning problems, but as we saw in the lesson, there are grey areas such as semi-supervised learning and feature learning which blur these two concepts.</p>\n<p class=""chakra-text css-o3oz8b"">Similarly, it can be easy to get focused on one variant of a problem but forget about a different variant.  For example, suppose we ask the question ""How much money will this new product launch earn us?""  This sort of question indicates that we want to solve a regression problem.  But suppose that we don\'t have good enough information to get an exact valuation.  In that case, we might be able to flip the question into, ""Will this new product launch be profitable?""  Now we can turn this into a two-class classification problem, and that might suit the business well enough if your regression model is not good enough.</p>\n<h3 class=""chakra-heading css-k57syw"">There Are Always More Algorithms</h3>\n<p class=""chakra-text css-o3oz8b"">When reviewing a cheat sheet of algorithms, it is important to remember that there are a wide variety of algorithms and variants for any class of machine learning problem, and research papers come out with regularity on new techniques and approaches to solving problems.  You likely will never have an exhaustive knowledge of all algorithms within a field, but the good news is that this is rarely the most efficient use of your time--instead, spend that time cleaning up and processing data, and you\'ll see success from a wide variety of data.  The mantra is, ""features beat algorithms.""  Great features and a mediocre choice of algorithm will generate more success than mediocre features and the best algorithms.</p>\n<h3 class=""chakra-heading css-k57syw"">Tools and Algorithms</h3>\n<p class=""chakra-text css-o3oz8b"">When choosing tools for data science, keep in mind that not all products implement the same set of algorithms.  For example, Microsoft\'s Azure Machine Learning has a set of algorithms available to it.  Meanwhile, <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet"">its ML.NET product<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> has a different set of available algorithms, and its <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/machine-learning-server/what-is-machine-learning-server"">Microsoft Machine Learning Server<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> yet another set.  This certainly applies to other commercial and open source products as well.  Furthermore, algorithms might be implemented in different ways.  For example, one product might include settings for an algorithm that another product does not.  As you work with different products, become familiar with what is available and do not assume that just because an algorithm is in one product, it should necessarily be in every product.</p></div>']",[],https://www.youtube.com/embed/ak_dyEQ7waA
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.21  Lesson Review,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">This lesson had us shift focus from artificial intelligence down into machine learning.  Summarizing the lesson, we:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Reviewed key machine learning terminology, including concepts like labels and features, as well as the approaches to machine learning and common classes of algorithms.  We applied these approaches, along with the sorts of questions end users ask, to understand which classes of algorithms are best suited to meet user needs.</li><li class=""css-cvpopp"">Solved a regression problem using Azure Machine Learning\'s Automated ML.  Along the way, we used the data science process, starting with data collection and preparation, on to model training, evaluation, and deployment.</li><li class=""css-cvpopp"">Used the Azure Machine Learning designer to develop and deploy custom-built machine learning models.  We trained and evaluated a classification model, and then trained, evaluated, and deployed a clustering model.</li></ul>\n<p class=""chakra-text css-o3oz8b"">Now that we have a firmer understanding of machine learning and how to use Azure Machine Learning, we can comfortably move on to the next lesson, which covers computer vision.</p></div>']",[],https://www.youtube.com/embed/SF-bE1KJVzo
AI Fundamentals,AI Fundamentals,Lesson 4: Machine Learning,4.22  Glossary,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">For your reference, here are all the new terms we introduced in this lesson:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Accuracy (classification)</strong>:  A measure which is defined as the number of correct predictions divided by the total number of predictions.</li><li class=""css-cvpopp""><strong>Area Under the Curve</strong>:  the percentage of area underneath the ROC curve.  This is a measure of how accurate the two-class model is, with numbers closer to 1 being better.</li><li class=""css-cvpopp""><strong>Azure Machine Learning Studio</strong>:  The integrated development environment (IDE) for Azure Machine Learning.</li><li class=""css-cvpopp""><strong>Compute (Azure ML)</strong>:  Virtual machine resources which are dedicated to performing tasks in Azure Machine Learning.  Compute may include individual virtual machines (VMs), typically configured as data science VMs, or it may include a cluster of VMs intended for training and inference pipeline executions.</li><li class=""css-cvpopp""><strong>Confusion matrix</strong>:  A table representing predicted versus actual values for a classification problem.  A classic two-class confusion matrix has four boxes.  Using ""Yes"" and ""No"" as the two classes, these four boxes are:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">True Positive: we predicted Yes correctly</li><li class=""css-cvpopp"">False Positive: we predicted Yes but it was really No</li><li class=""css-cvpopp"">False Negative: we predicted No but it was really Yes</li><li class=""css-cvpopp"">True Negative: we predicted No correctly</li></ul>\n</li><li class=""css-cvpopp""><strong>Data Labeling</strong>:  This functionality allows you to label images as part of an image classification project.</li><li class=""css-cvpopp""><strong>Experiment (Azure ML)</strong>:  A collection of trials used to validate a user\'s hypothesis.  An experiment may contain multiple runs of pipelines.</li><li class=""css-cvpopp""><strong>Feature</strong>:  Inputs which help us understand what affects the label.</li><li class=""css-cvpopp""><strong>Feature engineering</strong>:  Creating new features from existing data.  This might include calculating new features, translating a street address into latitude and longitude, or parsing passages of text for meaning.</li><li class=""css-cvpopp""><strong>Feature selection</strong>:  Removing a column from consideration when training a model.</li><li class=""css-cvpopp""><strong>Label</strong>:  The thing we want to predict.</li><li class=""css-cvpopp""><strong>Linked Services</strong>:  This functionality allows you to integrate Azure Machine Learning with other Azure services.  At present, the only linked service offering is to connect to Azure Synapse Analytics, which is a modern data warehousing offering on Azure.</li><li class=""css-cvpopp""><strong>Mean Absolute Error (MAE)</strong>:  An evaluation measure for any regression model.  It is the average difference between predicted and actual values.  This works well when dealing with small ranges of numbers.</li><li class=""css-cvpopp""><strong>Mean Absolute Percent Error (MAPE)</strong>:  An evaluation measure for any regression model.  It is the percentage difference between the predicted and actual values.  If the actual value is 0, MAPE will fail with a divide by 0 error, so it is not a good measure if the actual value can be 0.  MAPE works best when you have large ranges of numbers.</li><li class=""css-cvpopp""><strong>Microservice</strong>:   A lightweight, independent service.  Typically, microservices have one job and communicate with each other using well-defined operations.</li><li class=""css-cvpopp""><strong>Node (input, output)</strong>:  An input or output connection point on a component.  Each component will have 0 to 3 input nodes and 0 to 3 output nodes.  Each input or output node has a specific type, such as <strong>DataFrameDirectory</strong>, <strong>TransformationDirectory</strong>, or <strong>UntrainedModelDirectory</strong>.  An input of <strong>DataFrameDirectory</strong> can only attach to an output of the same type.</li><li class=""css-cvpopp""><strong>Overfitting</strong>:  A situation which happens when a trained model latches onto the particular relationships within a training data set, but those particulars are not always indicative of the broader world.</li><li class=""css-cvpopp""><strong>Pipeline (Azure ML)</strong>:  A collection of components connected together in a defined order.  The metaphor represents how data moves from a source (an initial dataset) and flows through components until it reaches a destination.  There are two types of pipeline:  <strong>training pipelines</strong> and <strong>inference pipelines</strong>.</li><li class=""css-cvpopp""><strong>Pipeline Asset:</strong>  A component available within Azure Machine Learning.  This includes datasets you have imported, sample datasets which come with the service, and different components to transform, train, evaluate, and deploy models.</li><li class=""css-cvpopp""><strong>Precision</strong>:  A measure which calculates how frequently our predicted value is correct.  It is defined as True Positives / (True Positives + False Positives).</li><li class=""css-cvpopp""><strong>R^2 (R-squared)</strong>:  An evaluation measure for linear regression models which ranges from 0-1, where 1 is the highest possible score.</li><li class=""css-cvpopp""><strong>Recall</strong>:  A measure which calculates how frequently we correctly predict a value.  It is defined as True Positives / (True Positives + False Negatives).</li><li class=""css-cvpopp""><strong>Receiver Operating Characteristic (ROC) curve</strong>:  A plot which represents true positive versus false positive rates for a two-class model.</li><li class=""css-cvpopp""><strong>Reinforcement learning</strong>:  A machine learning technique in which we train an agent to observe its environment and use those environmental clues to make a decision.</li><li class=""css-cvpopp""><strong>Root Mean Square Error (RMSE)</strong>:  An evaluation measure for any regression model.  RMSE works best when you are concerned with large differences between the predicted and actual values.</li><li class=""css-cvpopp""><strong>Root Mean Square Log Error (RMSLE)</strong>:  An evaluation measure for any regression model.  RMSLE works best when you are concerned with large percentage differences between the predicted and actual values.</li><li class=""css-cvpopp""><strong>Run (Azure ML)</strong>:  An attempt to train a model in Azure Machine Learning.  This can be done through a pipeline in the Azure ML designer or through Automated ML.</li><li class=""css-cvpopp""><strong>Semi-supervised learning:</strong>  A machine learning technique in which we have a small percentage of data with labels and a large percentage of unlabeled data.</li><li class=""css-cvpopp""><strong>Sink node</strong>:  A node with no outputs.  An example of a sink node is <strong>Web Service Output</strong>.</li><li class=""css-cvpopp""><strong>Source node</strong>:  A node with no inputs.  An example of a source node is any dataset you bring onto the canvas.</li><li class=""css-cvpopp""><strong>Supervised learning</strong>:  A machine learning technique in which we have a known good answer for our label and attempt to learn from this label for inference purposes.  The most common examples of this include classification and regression.</li><li class=""css-cvpopp""><strong>Unsupervised learning</strong>:  A machine learning technique in which we do not have labels for our data.  We use unsupervised learning techniques to try to discover what those labels should be.  Clustering is the most common example of this.</li></ul></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.1  Introduction,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Introduction</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Welcome to the AI Fundamentals learn about <strong>computer vision</strong>. Computer vision is the area of artificial intelligence concerned with <strong>processing and understanding visual input</strong>. Computer vision uses complex machine learning algorithms to analyze visual information received from images, video files, and even real-time input from cameras and attempts to understand and describe the content or action contained within it. Computer vision attempts to imitate how the human brain interprets visual information, using artificial intelligence to understand the latent information present in images, videos, and other visual data.</p>\n<p class=""chakra-text css-o3oz8b"">The primary focus areas within computer vision include <strong>semantic segmentation, object detection, image classification, facial recognition,</strong> and <strong>optical character recognition</strong>. Extracting information from visual inputs allows computer systems to make decisions based on image content.</p>\n<p class=""chakra-text css-o3oz8b"">Today, we see real-world applications of computer vision in many ways. Cellular phones allow us to log on using our faces. Social media platforms make recommendations about which of our friends to tag in an image. And autonomous vehicles use multiple aspects of computer vision to make instantaneous decisions to avoid obstacles and avoid collisions while driving.</p>\n<p class=""chakra-text css-o3oz8b"">In this lesson, we take a deeper look at the computer vision capabilities and how the Computer Vision components of Azure Cognitive Services can be used to build solutions to address handling and understanding visual input. You will learn about the Azure Computer Vision, Custom Vision, and Face services, as well as the Azure Form Recognizer.</p></div>']",[],https://www.youtube.com/embed/l-0sP2ixKFk
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.2  Lesson Overview,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">This lesson focuses on the computer vision capabilities and services available in Azure Cognitive Services. We will cover the Azure Computer Vision, Face, and Custom Vision services, as well as Azure Form Recognizer, and explore each service\'s capabilities and how you can use them in your applications.</p>\n<p class=""chakra-text css-o3oz8b"">At the end of this lesson, you will be able to:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Extract insights about the visual features, objects, and characteristics contained within images</li><li class=""css-cvpopp"">Build, deploy, and consume a custom image classification model</li><li class=""css-cvpopp"">Detect, analyze, and recognize faces using the Face service</li><li class=""css-cvpopp"">Analyze images containing text using optical character recognition</li><li class=""css-cvpopp"">Read typed and handwritten documents using the Read API of Computer Vision</li><li class=""css-cvpopp"">Extract the key-value pair and tables in forms using the Form Recognizer</li></ul>\n<p class=""chakra-text css-o3oz8b"">Throughout this lesson, we will look at how to implement the Azure Custom Vision services into the solutions you build.</p></div>']",[],https://www.youtube.com/embed/J8d_U_AFsFU
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.3  Computer Vision on Azure,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Computer Vision on Azure</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Computer vision attempts to provide solutions for several core workloads. Each workload is geared towards handling the different types of data that can appear in visual inputs. These workloads include:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Content Tagging</strong> is the process of analyzing images to identify well-known objects and attaching labels to each of them.</li><li class=""css-cvpopp""><strong>Object Detection</strong> refers to the process of identifying entities contained within an image.</li><li class=""css-cvpopp""><strong>Describe Images</strong> involves using artificial intelligence to understand the salient content in a photograph.</li><li class=""css-cvpopp""><strong>Image Classification</strong> uses machine learning models to classify or categorize photos based on their primary subject matter.</li><li class=""css-cvpopp""><strong>Facial Recognition</strong> refers to detecting human faces within an image and then analyzing and identifying those faces.</li><li class=""css-cvpopp""><strong>Read Text</strong> uses artificial intelligence to extract text from images and documents using optical character recognition or OCR.</li></ul>\n<p class=""chakra-text css-o3oz8b"">Each of these capabilities delivers different insights into the contents and context of analyzed images and helps build a more robust understanding of the visual information contained in the picture.</p>\n<p class=""chakra-text css-o3oz8b"">Any discussion about core computer vision workloads must include a conversation about <strong>semantic segmentation</strong>. Semantic segmentation uses advanced machine learning models to examine and classify the individual pixels in an image. It then groups all the parts or pixels that belong to the same object.</p>\n<p class=""chakra-text css-o3oz8b"">Azure\'s computer vision capabilities are part of the <strong>Azure Cognitive Services</strong> suite of cloud-based AI services. There are presently four services that provide the primary computer vision capabilities, presenting advanced machine learning algorithms to analyze images, videos, documents, and other visual input for specific information types.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Computer Vision services</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Azure Computer Vision</strong> is a cloud-based service that provides developers access to advanced machine learning algorithms to analyze images, videos, documents, and other visual input.</li><li class=""css-cvpopp"">The <strong>Azure Custom Vision service</strong> was created for scenarios where you need to create, train, and publish custom classification and object detection models.</li><li class=""css-cvpopp"">The <strong>Azure Face service</strong> offers robust capabilities for detecting, analyzing, and recognizing human faces.</li><li class=""css-cvpopp""><strong>Azure Form Recognizer</strong> is a data extraction service that employs artificial intelligence to identify various components and patterns in documents.</li></ul>\n<p class=""chakra-text css-o3oz8b"">For <strong>application developers</strong>, the computer vision services in Azure Cognitive Services are accessible via native SDKs and REST APIs, allowing flexibility in how you can interact with the service from your applications.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Optical character recognition (OCR)</strong>: The conversion of printed or handwritten text contained within images, such as scanned documents, into a digital representation of the text characters.</li><li class=""css-cvpopp""><strong>Semantic segmentation</strong>: The process of using complicated machine learning models to examine an image pixel-by-pixel and then cluster together all the parts or pixels that belong to the same object. Semantic segmentation classifies every pixel of an image, using that information to provide a very low-level understanding of the information in an image and feed other aspects of computer vision.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional resources</h3>\n<p class=""chakra-text css-o3oz8b"">Here are some links you can use to learn more about the following topics:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/services/cognitive-services/computer-vision/"">Azure Computer Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/services/cognitive-services/custom-vision-service/"">Custom Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/services/cognitive-services/face/"">Azure Face service<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/services/cognitive-services/form-recognizer/"">Form Recognizer<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://westcentralus.dev.cognitive.microsoft.com/docs/services/computer-vision-v3-1-ga/operations/56f91f2e778daf14a499f21b"">Computer Vision API<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/computer-vision/quickstarts-sdk/client-library?pivots=programming-language-csharp&amp;tabs=visual-studio"">Using the Computer Vision native client libraries in your solutions<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://en.wikipedia.org/wiki/Image_segmentation"">Semantic segmentation<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>']",['https://video.udacity-data.com/topher/2021/March/604bc472_azure-computer-vision-services/azure-computer-vision-services.png'],https://www.youtube.com/embed/1mEUnEa1CCw
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.4  Quizzes: Computer Vision on Azure,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Quizzes: Computer Vision on Azure</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What is the focus of the computer vision area of artificial intelligence?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Reading and translating text found in documents</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Using machine learning to extract information from visual inputs</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Understanding spoken or text-based inputs</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Object detection, facial recognition, and optical character recognition all rely upon <code class=""chakra-code css-1u83yg1"">semantic segmentation</code>. What is semantic segmentation?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The area of artificial intelligence focused on processing and understanding visual inputs through complex machine learning algorithms.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The grouping together of all pixels in an image that belong to the same object</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The use of machine learning models to identify and extract printed and handwritten text from images and documents.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following is NOT a core problem addressed by computer vision?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Classifying images based on their content</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Detecting faces in images</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Reading text found in images</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Translating text found in images</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Custom Vision provides the capability to build, train, and deploy what custom machine learning models capable of performing which of the following?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Image Classification</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Content moderation</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Face detection</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Object detection</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.5  Object Detection,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Object Detection</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Object detection</strong> in computer vision is the mechanism used to <strong>identify and categorize individual entities</strong> found in images. Object detection models return arrays of object identifications consisting of a name and confidence score, which indicates the predicted probability that the entity identified is actually that object. In Azure Computer Vision, items potentially include a parent object classification as well. An object\'s pixel coordinates are also returned as a <code class=""chakra-code css-1u83yg1"">rectangle</code> object, which can be used to draw bounding boxes on the associated image.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The <strong>content tagging</strong> and <strong>describe images</strong> capabilities of computer vision are <strong>closely related to object detection</strong>. Content tagging returns a list of labels for entities found within an image, like object detection, but does not output bounding boxes. Image descriptions rely on content tagging and object detection to identify the primary subject matter of an image and generate a human-readable sentence explaining the photo.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You can use <strong>Azure Computer Vision</strong> and <strong>Custom Vision</strong> to <strong>create object detection solutions</strong>, and both allow applications to access their functionality via native SDKs or REST APIs. The Custom Vision portal also provides a web-based portal for creating, training, and deploying custom models.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Demo: Create A Custom Object Detection Model</h3>\n<p class=""chakra-text css-o3oz8b"">A guided demo of how to build and deploy a custom object classification model using Azure Custom Vision.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Demo: Consume A Custom Object Detection Model</h3>\n<p class=""chakra-text css-o3oz8b"">A walk-through of calling a deployed custom object detection model from a Jupyter notebook in Azure Machine Learning.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Custom Vision Prediction Client <code class=""chakra-code css-1u83yg1"">detect_image</code> method (Python SDK)</h3>\n<p class=""chakra-text css-o3oz8b"">In the demo, we used the native Python SDK to access the Custom Vision library. We covered using the <code class=""chakra-code css-1u83yg1"">detect_image</code> method of the Custom Vision Prediction Client and how it can be used to retrieve the details of objects detected in analyzed images.</p>\n<p class=""chakra-text css-o3oz8b"">To reiterate the steps involved in using the Custom Vision Prediction Client, we need to do the following:</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Install the Custom Vision library using a <code class=""chakra-code css-1u83yg1"">pip install</code> command, such as the following:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-bash"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>pip </span><span class=""token"" style=""color: rgb(153, 0, 0); font-weight: bold;"">install</span><span> azure-cognitiveservices-vision-customvision</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Import the required components from the Custom Vision library.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> azure</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>cognitiveservices</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>vision</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>customvision</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>prediction </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> CustomVisionPredictionClient\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> msrest</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>authentication </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> ApiKeyCredentials</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Create an <code class=""chakra-code css-1u83yg1"">ApiKeyCredentials</code> object using the key for your Custom Vision prediction resource.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>credentials </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> ApiKeyCredentials</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>in_headers</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">{</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Prediction-key""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> key</span><span class=""token"" style=""color: rgb(15, 43, 61);"">}</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Instantiate a new Custom Vision Prediction Client, referencing the credential object you created above plus the endpoint of your Custom Vision prediction service.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>client </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> CustomVisionPredictionClient</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> credentials</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>credentials</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">With your <code class=""chakra-code css-1u83yg1"">client</code> object created, you can call the <code class=""chakra-code css-1u83yg1"">detect_image</code> method on that client to analyze an image.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Provide the Project Id for your deployed object detection model</span><span>\n</span><span>projectId </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'b619674f-ec29-4f5f-b037-bdad9e4b26ce\'</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># The modelName value must exactly match the model name you set when publishing the model</span><span>\n</span><span>modelName </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'object-detection\'</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Retrieve an image from the \'images\' folder</span><span>\n</span><span>imagePath </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> os</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>path</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>join</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'images\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'image-01.jpeg\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Open the image as a stream and pass it into client.detect_image</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">with</span><span> </span><span class=""token"" style=""color: rgb(0, 121, 162);"">open</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>imagePath</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> mode</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""rb""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">as</span><span> imageData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>  results </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>detect_image</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>projectId</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> modelName</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> imageData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">The code above opens an image named <code class=""chakra-code css-1u83yg1"">image-01.jpeg</code> from the <code class=""chakra-code css-1u83yg1"">images</code> folder as a stream. The <code class=""chakra-code css-1u83yg1"">projectId</code>, <code class=""chakra-code css-1u83yg1"">modelName</code>, and <code class=""chakra-code css-1u83yg1"">imageData</code> values are passed into the <code class=""chakra-code css-1u83yg1"">client.detect_image()</code> method for analysis. The return value from the method is stored in a variable named <code class=""chakra-code css-1u83yg1"">results</code>.</p>\n</blockquote>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">The <code class=""chakra-code css-1u83yg1"">client.detect_image()</code> method returns a results object, which includes an array of predicted objects identified in the image. To access the predictions, you use the <code class=""chakra-code css-1u83yg1"">results.predictions</code> value. For example, to loop through each prediction you would do something similar to the following:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> prediction </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> results</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>predictions</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>  </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">if</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>prediction</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>probability </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">*</span><span> </span><span class=""token"" style=""color: rgb(2, 124, 124);"">100</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">&gt;</span><span> </span><span class=""token"" style=""color: rgb(2, 124, 124);"">75</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>      </span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Do some work</span></code></div></div></pre>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">In this case, we are looping through each prediction and if the prediction\'s probability exceeds 75%, we will do some work.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Bounding box</strong>: Depicts an object\'s location in an image, including the X and Y coordinates of the entity and its width and height, designated by a <code class=""chakra-code css-1u83yg1"">rectangle</code> object.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional resources</h3>\n<p class=""chakra-text css-o3oz8b"">Here are some links you can use to learn more about the following topics:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://en.wikipedia.org/wiki/Object_detection"">Object detection<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-object-detection"">Object detection with Azure Computer Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-tagging-images"">Content tagging with Azure Computer Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-describing-images"">Image descriptions in Azure Computer Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/custom-vision-service/get-started-build-detector"">Tutorial for building an object detector in Azure Custom Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-customvision/azure.cognitiveservices.vision.customvision.prediction.operations.customvisionpredictionclientoperationsmixin?view=azure-python#detect-image-project-id--published-name--image-data--application-none--custom-headers-none--raw-false----operation-config-"">Azure Custom Vision Prediction client detect_image() method<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/Azure-Samples/cognitive-services-python-sdk-samples/blob/master/samples/vision/custom_vision_object_detection_sample.py"">Azure Custom Vision Object Detection Python Sample code<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>']",[],https://www.youtube.com/embed/_Cqc1zdBtmU
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.6  Quizzes: Object Detection,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Quizzes: Object Detection</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What does the <code class=""chakra-code css-1u83yg1"">rectangle</code> property in each detected entity returned by an object detection model represent?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The size, in pixels, of the image analyzed</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The size, in pixels, of the object</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The width and height of the primary subject matter of the image</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The pixel coordinates of the object and its size in pixels</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Semantic segmentation adds a color-coded overlay mask to the bounding box of an object based on its pixel coordinates.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">True</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">False</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In part two of the object detection demo, we used the <code class=""chakra-code css-1u83yg1"">detect_image()</code> method of the <code class=""chakra-code css-1u83yg1"">CustomVisionPredictionClient</code> object to access a deployed custom object detection model. Assuming we\'ve named our <code class=""chakra-code css-1u83yg1"">CustomVisionPredictionClient</code> variable <code class=""chakra-code css-1u83yg1"">client</code>, Which of the following represents the correct syntax when calling the <code class=""chakra-code css-1u83yg1"">detect_image()</code> method using Python?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">client.detect_image(imageStream)</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">client.detect_image(projectId, imageStream)</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">client.detect_image(modelName, imageStream)</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">client.detect_image(projectId, modelName, imageStream)</code></p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.7  Exercise Resource Setup,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The tasks below provide instructions for creating the Azure resources required to complete all of the exercises you will perform throughout this lesson on <strong>computer vision</strong>.</p>\n<p class=""chakra-text css-o3oz8b"">You should follow the step-by-step instructions to provision the required resources in Azure <em class=""chakra-text css-o3oz8b"">before</em> attempting any of the exercises. You will use your Azure subscription for creating the required resources.</p>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b""><strong>Note</strong>: All the Azure resources used for this exercise should be created in the same region to minimize egress charges and network latency.</p>\n</blockquote></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Requirements</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">You must have an Azure subscription to complete this exercise. If you don\'t have one, you can sign up for a free trial at <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free"">https://azure.microsoft.com/free<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li><li class=""css-cvpopp"">You will not be using a Udacity workspace to complete this exercise.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 1: Create an Azure resource group</h2>\n<p class=""chakra-text css-o3oz8b"">In this task, you create a resource group in Azure, which acts as a hosting container for the various resources needed to create an image classification solution.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Open a web browser, navigate to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com/"">Azure portal<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, and sign in using the account associated with your Azure subscription.</li><li class=""css-cvpopp"">On the home page of the Azure portal, select <strong>Resource groups</strong> under the Azure services heading.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Resource groups</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">On the Resource groups blade, select <strong>Add</strong> on the toolbar to create a new resource group.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add resource group</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""4"" class=""css-13a5a39""><li class=""css-cvpopp"">On the <strong>Create a resource group</strong> Basics tab, enter the following:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Subscription</strong>: Select the subscription you are using for this exercise.</li><li class=""css-cvpopp""><strong>Resource group</strong>: Enter <code class=""chakra-code css-1u83yg1"">udacity-exercises</code> or another name unique within your subscription.</li><li class=""css-cvpopp""><strong>Region</strong>: Select any available region, but preferably one close to your location. <strong>Note</strong>, you will use this same region for all other resources you create during this exercise.</li></ul>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a resource group basics tab</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""5"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Review + create</strong> and on the Review + create tab, ensure the <code class=""chakra-code css-1u83yg1"">validation passed</code> message is displayed and then select <strong>Create</strong>.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">""Review new resource group settings""</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""6"" class=""css-13a5a39""><li class=""css-cvpopp"">When the resource has been successfully created, select the notifications icon on the Azure header bar and then select <strong>Go to resource group</strong> in the <strong>Resource group created</strong> notification.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 2: Provision an Azure Cognitive Services account</h2>\n<p class=""chakra-text css-o3oz8b"">In this task, you create an Azure Cognitive Services account in your Azure subscription. Cognitive Services provides the majority of the computer vision capabilities you will be using throughout the exercises in this lesson.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">On the <strong>Resource group</strong> blade, select <strong>Add</strong> on the toolbar.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add resource to resource group</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">Enter <code class=""chakra-code css-1u83yg1"">Cognitive Services</code> into the search the marketplace box and press enter or select the result that appears below your typed text.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">New Cognitive Services resource</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Create</strong> on the Cognitive Services blade.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create new Cognitive Services account</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""4"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Configure a new <strong>Cognitive Services</strong> by entering the follow settings:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Subscription</strong>: Select the subscription you are using for this exercise.</li><li class=""css-cvpopp""><strong>Resource group</strong>: Select the resource group you create previously.</li><li class=""css-cvpopp""><strong>Region</strong>: Select the same region you choose for your resource group above.</li><li class=""css-cvpopp""><strong>Name</strong>: Enter a globally unique name, such as <code class=""chakra-code css-1u83yg1"">udacity-cog-services-SUFFIX</code>, replacing <code class=""chakra-code css-1u83yg1"">SUFFIX</code> with your initials or another string to make the name unique.</li><li class=""css-cvpopp""><strong>Pricing tier</strong>: Select <strong>Standard S0</strong>.</li></ul>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create Cognitive Services</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""5"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select <strong>Review + create</strong>.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Ensure the validation passed and then select <strong>Create</strong>.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Review and create</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 3: Create a Custom Vision service</h2>\n<p class=""chakra-text css-o3oz8b"">In this task, you create a Custom Vision service in your Azure subscription. You will be using Custom Vision to create custom object detection and image classifications machine learning models.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Return to the <code class=""chakra-code css-1u83yg1"">udacity-exercises</code> resource group blade in the Azure portal and on the <strong>Resource group</strong> blade, select <strong>Add</strong> on the toolbar.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add resource to resource group</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">Enter <code class=""chakra-code css-1u83yg1"">Custom Vision</code> into the search the marketplace box and press enter or select the result that appears below your typed text.</li><li class=""css-cvpopp"">Select <strong>Create</strong> on the Custom Vision blade.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create new Custom Vision service</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""4"" class=""css-13a5a39""><li class=""css-cvpopp"">Configure a new <strong>Custom Vision</strong> by entering the follow settings:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Create options</strong>: Select <strong>Both</strong>. This will result in the creation of two <code class=""chakra-code css-1u83yg1"">Cognitive Services</code> resources in your resource group; one for training and one for prediction.</li><li class=""css-cvpopp""><strong>Subscription</strong>: Select the subscription you are using for this exercise.</li><li class=""css-cvpopp""><strong>Resource group</strong>: Select the resource group you create previously.</li><li class=""css-cvpopp""><strong>Name</strong>: Enter a globally unique name, such as <code class=""chakra-code css-1u83yg1"">udacitycustomvisionSUFFIX</code>, replacing <code class=""chakra-code css-1u83yg1"">SUFFIX</code> with your initials or another string to make the name unique.</li><li class=""css-cvpopp""><strong>Training location</strong>: Select the same location you choose for your resource group above.</li><li class=""css-cvpopp""><strong>Training pricing tier</strong>: Select <strong>Free F0</strong>.</li><li class=""css-cvpopp""><strong>Prediction location</strong>: Select the same location you choose for your resource group above.</li><li class=""css-cvpopp""><strong>Prediction pricing tier</strong>: Select <strong>Free F0</strong>.</li></ul>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b""><strong>Note</strong>: You can only have one <strong>F0</strong> Custom Vision service provisioned in your subscription, so if you already have one, select <strong>Standard S0</strong> for both the training and prediction locations above.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create Custom Vision service</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""5"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Review + create</strong> and, as you did previously, ensure the validation passed and then select <strong>Create</strong>.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 4: Create an Azure Machine Learning workspace</h2>\n<p class=""chakra-text css-o3oz8b"">In this task, you create an Azure Machine Learning workspace to provide an environment capable of hosting and running Jupyter notebooks. In the exercises, you will use Python code in Jupyter notebooks to consume the various Computer Vision services.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Return to the <code class=""chakra-code css-1u83yg1"">udacity-exercises</code> resource group blade in the Azure portal and on the <strong>Resource group</strong> blade, select <strong>Add</strong> on the toolbar.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add resource to resource group</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">On the New blade, enter <code class=""chakra-code css-1u83yg1"">Machine Learning</code> into the search the marketplace box, and press enter on your keyboard.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">New Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Create</strong> on the Machine Learning blade.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""4"" class=""css-13a5a39""><li class=""css-cvpopp"">On the Create a machine learning workspace <strong>Basics</strong> tab, enter the following:\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Subscription</strong>: Select the subscription you are using for this exercise.</li><li class=""css-cvpopp""><strong>Resource group</strong>: Select the resource group you created for this exercise.</li><li class=""css-cvpopp""><strong>Workspace name</strong>: Enter <code class=""chakra-code css-1u83yg1"">udacity-ml-workspace</code>.</li><li class=""css-cvpopp""><strong>Region</strong>: Select the region you used for your resource group above.</li><li class=""css-cvpopp""><strong>Storage account</strong>: Accept the default assigned value to create a new storage account.</li><li class=""css-cvpopp""><strong>Key vault</strong>: Accept the default assigned value to create a new key vault.</li><li class=""css-cvpopp""><strong>Application insights</strong>: Accept the default assigned value to create a new Applications Insights instance.</li><li class=""css-cvpopp""><strong>Container registry</strong>: Accept the default assigned value of <strong>None</strong>.</li></ul>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a machine learning workspace</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""5"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Review + create</strong>, ensure you see the validation passed message, and then select <strong>Create</strong>.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 5: Create a compute resource in Azure Machine Learning</h2>\n<p class=""chakra-text css-o3oz8b"">In this task, you create the virtual machine in your Azure Machine Learning workspace that will host the Jupyter environment for running Jupyter notebooks.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Once the AML workspace deployment completes, open <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">Azure Machine Learning Studio<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in a new browser window.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Machine Learning studio</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">On the home page on Azure Machine Learning studio, select <strong>Create new</strong> and then select <strong>Compute instance</strong> from the context menu.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create new compute instance</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">On the <strong>Create compute instance</strong> Virtual machine page, select the following:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Virtual machine type</strong>: Choose <strong>CPU</strong>.</li><li class=""css-cvpopp""><strong>Virtual machine size</strong>: Choose the <code class=""chakra-code css-1u83yg1"">Standard_DS3_v2</code> virtual machine.</li></ul>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create compute instance VM</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""4"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select <strong>Next</strong> on the Create compute instance dialog.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Enter <code class=""chakra-code css-1u83yg1"">udacity-ml-vm</code> for the <strong>Compute name</strong> and then select <strong>Create</strong>.</p>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b""><strong>Note</strong>: It takes a few minutes for the compute instance to start, so wait until it has completed.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create compute instance</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 6: Clone the AI Fundamentals GitHub repo on your compute instance</h2>\n<p class=""chakra-text css-o3oz8b"">In this task, you will launch the terminal running on your AML compute instance and clone the Udacity AI Fundamentals GitHub repo, which contains the Jupyter notebooks you will run in this lesson\'s exercises.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Launch a new <code class=""chakra-code css-1u83yg1"">Terminal</code> interface for your compute instance by selecting the <code class=""chakra-code css-1u83yg1"">Terminal</code> Application URI.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Terminal link</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">At the prompt, clone the Udacity AI Fundamentals GitHub repo containing the notebook for this exercise by pasting and executing the following code:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-bash"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(153, 0, 0); font-weight: bold;"">git</span><span> clone https://github.com/udacity/AI_fundamentals.git</span></code></div></div></pre>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">This will make a copy of the GitHub repository in a folder named <code class=""chakra-code css-1u83yg1"">AI_fundamentals</code> under the user folder that bears your name.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Terminal git clone command</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">When the clone operations finishes, you will see output like the following.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">git clone command output</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""4"" class=""css-13a5a39""><li class=""css-cvpopp"">Select the refresh button on the <strong>Files</strong> tab and ensure the <code class=""chakra-code css-1u83yg1"">AI_fundamentals</code> folder appears under your user folder.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">File system</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>This concludes the setup process for the exercises in this lesson.</strong></p></div>']","['https://video.udacity-data.com/topher/2021/February/60359490_resource-groups/resource-groups.png', 'https://video.udacity-data.com/topher/2021/February/603608f1_resource-groups-add/resource-groups-add.png', 'https://video.udacity-data.com/topher/2021/February/60364966_create-a-resource-group-basics/create-a-resource-group-basics.png', 'https://video.udacity-data.com/topher/2021/February/603649bb_create-resource-group-review-create/create-resource-group-review-create.png', 'https://video.udacity-data.com/topher/2021/February/60364aa7_resource-group-add-resource/resource-group-add-resource.png', 'https://video.udacity-data.com/topher/2021/March/604ce0f0_new-cognitive-services/new-cognitive-services.png', 'https://video.udacity-data.com/topher/2021/March/604ce15a_cognitive-services-create/cognitive-services-create.png', 'https://video.udacity-data.com/topher/2021/March/604ce17c_create-cognitive-services-basics/create-cognitive-services-basics.png', 'https://video.udacity-data.com/topher/2021/March/604ce19c_cognitive-services-review-create/cognitive-services-review-create.png', 'https://video.udacity-data.com/topher/2021/February/60364aa7_resource-group-add-resource/resource-group-add-resource.png', 'https://video.udacity-data.com/topher/2021/February/60364ae7_custom-vision-create/custom-vision-create.png', 'https://video.udacity-data.com/topher/2021/February/60364b27_create-custom-vision-service-basics/create-custom-vision-service-basics.png', 'https://video.udacity-data.com/topher/2021/February/60367189_resource-group-add-resource/resource-group-add-resource.png', 'https://video.udacity-data.com/topher/2021/February/603671c1_new-machine-learning/new-machine-learning.png', 'https://video.udacity-data.com/topher/2021/February/603671f2_create-machine-learning/create-machine-learning.png', 'https://video.udacity-data.com/topher/2021/February/60367240_create-machine-learning-workspace-basics/create-machine-learning-workspace-basics.png', 'https://video.udacity-data.com/topher/2021/February/6036726f_aml-studio/aml-studio.png', 'https://video.udacity-data.com/topher/2021/March/604ce3c2_create-new-compute-instance/create-new-compute-instance.png', 'https://video.udacity-data.com/topher/2021/March/604ce3e0_create-compute-instance-vm/create-compute-instance-vm.png', 'https://video.udacity-data.com/topher/2021/March/604ce405_create-compute-instance/create-compute-instance.png', 'https://video.udacity-data.com/topher/2021/March/604ce548_terminal-link/terminal-link.png', 'https://video.udacity-data.com/topher/2021/March/604cef61_git-clone/git-clone.png', 'https://video.udacity-data.com/topher/2021/March/604cefbe_git-clone-output/git-clone-output.png', 'https://video.udacity-data.com/topher/2021/March/604cefdb_files-ai-fundamentals-folder/files-ai-fundamentals-folder.png']",
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.8  Exercise: Create An Object Detection Solution,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Introduction</h2>\n<p class=""chakra-text css-o3oz8b"">You have now learned about the capabilities of the Custom Vision service and how it can be used to create custom object detection models using the images you provide. The object detection demo walked through how to use the Custom Vision portal to build, train, test, and deploy an object detection model, and now it is time for you to apply what you have learned.</p>\n<p class=""chakra-text css-o3oz8b"">In this exercise, you use the Azure Custom Vision service to train a custom object detection model, and then deploy the model to a prediction service for consumption. You will then use a Jupyter notebook in Azure Machine Learning Studio to send images into your prediction service for detecting entities within those images and examine the results.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Requirements</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">You must have an Azure subscription to complete this exercise. If you don\'t have one, you can sign up for a free trial at <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free"">https://azure.microsoft.com/free<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li><li class=""css-cvpopp"">You will not be using a Udacity workspace to complete this exercise.</li><li class=""css-cvpopp"">You must have completed the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://coco.udacity.com/mocha/programs/nd099_t/en-us/1.0.0/courses/5ddef5bf-04d1-4b9d-94af-dac454840080/lessons/839a81b8-95d8-44df-b88f-49a5202d9152/pages/005ec524-d2e5-41cd-a752-fb4a9d04f96b"">Exercise Resource Setup<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in order to complete this exercise.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Instructions</h2>\n<p class=""chakra-text css-o3oz8b"">After completing all of the tasks outlined in the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://coco.udacity.com/mocha/programs/nd099_t/en-us/1.0.0/courses/5ddef5bf-04d1-4b9d-94af-dac454840080/lessons/839a81b8-95d8-44df-b88f-49a5202d9152/pages/005ec524-d2e5-41cd-a752-fb4a9d04f96b"">Exercise Resource Setup guide<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, complete the tasks listed below to create, train, test, and publish a custom object detection model. Then, follow the steps within the indicated Jupyter notebook to consume the deployed model using Python code.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Navigate to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.customvision.ai/"">Computer Vision portal<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in a web browser and sign in using the account associated with your Azure subscription.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the Custom Vision portal, create a new project with the following settings:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Name</strong>: Enter <code class=""chakra-code css-1u83yg1"">Object detector</code>.</li><li class=""css-cvpopp""><strong>Resource</strong>: Associate the project with your Custom Vision training resource.</li><li class=""css-cvpopp""><strong>Project Type</strong>: Choose <code class=""chakra-code css-1u83yg1"">Object Detection</code>.</li><li class=""css-cvpopp""><strong>Domain</strong>: Use the <code class=""chakra-code css-1u83yg1"">General</code> domain.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Download the object detection exercise training images from <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/training-images.zip?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/training-images.zip?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Upload the images in the <code class=""chakra-code css-1u83yg1"">training-images</code> folder you downloaded to your Custom Vision project.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Tag the eating utensils in the images using the tags <code class=""chakra-code css-1u83yg1"">fork</code>, <code class=""chakra-code css-1u83yg1"">knife</code>, and <code class=""chakra-code css-1u83yg1"">spoon</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Train the custom object detection model using the Quick Training method.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Test the model using these image URLs:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/test-images/silverware-test-01.jpg?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/test-images/silverware-test-01.jpg?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/test-images/silverware-test-02.jpg?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/test-images/silverware-test-02.jpg?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Publish the model to your Custom Vision\'s prediction resource with the name <code class=""chakra-code css-1u83yg1"">object-detection-exercise</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Retrieve the <code class=""chakra-code css-1u83yg1"">Project Id</code> for your deployed object detection model.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Retrieve the key and endpoint values for your Custom Vision prediction resource.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Open <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">Azure Machine Learning Studio<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, locate the <code class=""chakra-code css-1u83yg1"">udacity-ml-vm</code> compute you created during the exercise resource setup, and open its associated <code class=""chakra-code css-1u83yg1"">Jupyter</code> environment.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the Jupyter file system, navigate to the <code class=""chakra-code css-1u83yg1"">AI_fundamentals</code> folder under your user folder, and then drill down to the <code class=""chakra-code css-1u83yg1"">object-detection-exercise.ipynb</code> notebook in the <code class=""chakra-code css-1u83yg1"">computer-vision/exercises/object-detection</code> folder. Follow the instructions within the notebook to complete the rest of the exercise.</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.9  Solution: Create An Object Detection Solution,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Solution: Create An Object Detection Solution</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The tasks below provide step-by-step instructions to walk you through the solution to the object detection exercise. Following these steps, you will upload images to the Custom Vision service and use them for training an object detection model. You will then publish the trained model to your prediction Cognitive service and consume the model from a Jupyter notebook in Azure Machine Learning.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 1: Log into the Custom Vision portal</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Open a web browser and go to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.customvision.ai"">https://www.customvision.ai<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select <strong>Sign In</strong> and enter your Azure portal credentials if prompted.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Custom Vision portal</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">You will be directed to the Custom Vision portal\'s home page.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Custom Vision portal</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 2: Create a new object detection project</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">In the Custom Vision portal, select <strong>New Project</strong>.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">New project</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">In the Create new project dialog, enter the following:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Name</strong>: Enter <code class=""chakra-code css-1u83yg1"">Object detector</code>.</li><li class=""css-cvpopp""><strong>Description</strong>: (Optional) Enter a short description, such as <code class=""chakra-code css-1u83yg1"">Object detection exercise</code>.</li><li class=""css-cvpopp""><strong>Resource</strong>: Select your training Cognitive service.</li><li class=""css-cvpopp""><strong>Project Types</strong>: Choose <code class=""chakra-code css-1u83yg1"">Object Detection</code>.</li><li class=""css-cvpopp""><strong>Domains</strong>: Choose <code class=""chakra-code css-1u83yg1"">General</code>.</li></ul>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create new Custom Vision project</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Create project</strong> in the create new project dialog.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 3: Download training image</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select the link provided and download the ZIP file containing training images.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/kylebunting/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/training-images.zip?raw=true"">https://github.com/kylebunting/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/training-images.zip?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Extract the download ZIP file to a local folder, such as <code class=""chakra-code css-1u83yg1"">C:\\udacity\\training-images\\</code>.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 4: Upload training images</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Return to your newly created Custom Vision project and select <strong>Add images</strong> from the toolbar.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add images</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">Browse to the directory where you extracted the training images you downloaded above, select all photos in the directory, and then select <strong>Open</strong> in the Open dialog.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Open files</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">On the <strong>Image upload</strong> dialog, select <strong>Upload 28 files</strong>.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Upload images</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""4"" class=""css-13a5a39""><li class=""css-cvpopp"">When the upload completes, select <strong>Done</strong> on the Image upload dialog.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Image upload</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 5: Tag uploaded images</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">In your object detection project, select the first image you see. <strong>Note</strong>, the image may be a different image than the one displayed in the screenshot below.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Untagged images</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">In the <strong>Image Detail</strong> dialog, select the bounding box suggested over the first utensil in the image and enter the appropriate label. <strong>Important</strong>, assigned labels should be either <code class=""chakra-code css-1u83yg1"">fork</code>, <code class=""chakra-code css-1u83yg1"">knife</code>, or <code class=""chakra-code css-1u83yg1"">spoon</code>.</p>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b""><strong>Note</strong>: Depending on the image, you may need to resize the bounding boxes around each object to focus on the entity you are tagging.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Object tagging</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">Select the next utensil displayed in the image and add the relevant tag.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Object tagging</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""4"" class=""css-13a5a39""><li class=""css-cvpopp"">Continue tagging each utensil displayed in the image and then select the <strong>Next</strong> arrow on the right-hand side of the dialog to move on to the following image.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Object tagging</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""5"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">For each image, tag all utensils displayed and then select next until all photos have been labeled.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">When all the images have been tagged, select the <strong>Close</strong> button on the dialog\'s top right.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Close Image Detail</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 6: Train the object detection model</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Train</strong> in the Custom Vision header bar to select a training type.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Custom Vision toolbar</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Choose the <code class=""chakra-code css-1u83yg1"">Quick training</code> training type and select <strong>Train</strong>.</p>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b""><strong>Note</strong>: Training may take several minutes to complete.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Choose training type</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 7: Test the model</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Test your model by selecting <strong>Quick Test</strong> on the toolbar.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Custom Vision toolbar</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Use the following image URLs for testing your model by copying and pasting each one into the <strong>Image URL</strong> box and selecting the box containing an arrow:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/test-images/silverware-test-01.jpg?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/test-images/silverware-test-01.jpg?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/test-images/silverware-test-02.jpg?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/test-images/silverware-test-02.jpg?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Quick Test</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">Close the <strong>Quick Test</strong> dialog when you are done testing.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 8: Publish the custom object detection model</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">From the object detection project in Custom Vision studio, select <strong>Publish</strong> on your model\'s <strong>Performance</strong> tab.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Publish</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Name the model <code class=""chakra-code css-1u83yg1"">object-detection-exercise</code>, select your <strong>-Prediction</strong> resource from the list, and then select <strong>Publish</strong>.</p>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">This will deploy your image classification model to the prediction cognitive services resource created when you provisioned the Custom Vision service.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Publish Model</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 9: Retrieve the Project Id for the deployed model</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Open your Custom Vision project\'s <code class=""chakra-code css-1u83yg1"">Settings</code> page by selecting the gear icon on the toolbar.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Project settings</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">Locate the <strong>Project Id</strong> field and then leave the page open for the next task. You will need the <code class=""chakra-code css-1u83yg1"">Project Id</code> value to complete the steps contained within the notebook below.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Project settings</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 10: Retrieve the key and endpoint for the Custom Vision prediction resource</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Open a new browser window, return to the Azure portal, and navigate back to the <code class=""chakra-code css-1u83yg1"">udacity-exercises</code> resource group. Within the resource group, select the <strong>-Prediction</strong> Cognitive service.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Resource group</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">On the prediction cognitive service blade, select <strong>Keys and Endpoints</strong> from the left-hand menu and leave this page open. You will need the <code class=""chakra-code css-1u83yg1"">Key</code> and <code class=""chakra-code css-1u83yg1"">Endpoint</code> values from your prediction cognitive service resource.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Keys and Endpoint</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 11: Open Azure Machine Learning Studio</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">In a web browser, navigate to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">https://ml.azure.com/<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> and sign in, if prompted.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select the <strong>Compute</strong> item in the left-hand navigation menu.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">On the compute tab, locate the <code class=""chakra-code css-1u83yg1"">udacity-ml-vm</code> compute resource and select the <strong>Jupyter</strong> application URI.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Jupyter link</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 12: Open the object-detection-exercise Jupyter notebook</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Navigate down to the <code class=""chakra-code css-1u83yg1"">AI_fundamentals/computer-vision/exercises/object-detection</code> folder.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select the <code class=""chakra-code css-1u83yg1"">object-detection-exercise.ipynb</code> notebook to open it.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 13: Paste the copied values into the notebook</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Scroll down to the variables cell in the notebook.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Variables cell</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Paste the Project Id value copied from the Custom Vision portal into the value for the <code class=""chakra-code css-1u83yg1"">projectId</code> variable.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Paste the <code class=""chakra-code css-1u83yg1"">Key 1</code> value copied from the prediction resource\'s Keys and Endpoint page in the Azure portal into the <code class=""chakra-code css-1u83yg1"">key</code> variable\'s value.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Paste the <code class=""chakra-code css-1u83yg1"">Endpoint</code> value copied from the prediction resource\'s Keys and Endpoint page in the Azure portal into the <code class=""chakra-code css-1u83yg1"">endpoint</code> variable\'s value.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Verify the <code class=""chakra-code css-1u83yg1"">modelName</code> variable matches the name you entered when publishing your model. This should be <code class=""chakra-code css-1u83yg1"">object-detection-exercise</code>.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Your updated variables cell should look similar to the following:</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Updated variables</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 14: Instantiate a CustomVisionPredictionClient object</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">For the TODO in this cell, you will enter <code class=""chakra-code css-1u83yg1"">CustomVisionPredictionClient(endpoint=endpoint, credentials=credentials)</code> on this line to create a new <code class=""chakra-code css-1u83yg1"">CustomVisionPredictionClient</code> object. The final line will look like:</li></ol>\n<p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">client = CustomVisionPredictionClient(endpoint=endpoint, credentials=credentials)</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 15: Complete the call to the detect_image method</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">To call the <code class=""chakra-code css-1u83yg1"">detect_image</code> method from Python is as follows.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>results </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>detect_image</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>projectId</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> modelName</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> imageData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">You can learn more by reading the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-customvision/azure.cognitiveservices.vision.customvision.prediction.operations.customvisionpredictionclientoperationsmixin?view=azure-python#detect-image-project-id--published-name--image-data--application-none--custom-headers-none--raw-false----operation-config-"">detect_image documentation<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Detect Image</p></div>']","['https://video.udacity-data.com/topher/2021/March/604d1326_custom-vision-sign-in/custom-vision-sign-in.png', 'https://video.udacity-data.com/topher/2021/March/604d1349_custom-vision-home/custom-vision-home.png', 'https://video.udacity-data.com/topher/2021/March/604d13b3_custom-vision-new-project-tile/custom-vision-new-project-tile.png', 'https://video.udacity-data.com/topher/2021/March/604d13d4_custom-vision-create-new-project-object-detection/custom-vision-create-new-project-object-detection.png', 'https://video.udacity-data.com/topher/2021/March/604d14e9_custom-vision-add-images/custom-vision-add-images.png', 'https://video.udacity-data.com/topher/2021/March/604d1518_edge-open-training-images-object-detection/edge-open-training-images-object-detection.png', 'https://video.udacity-data.com/topher/2021/March/604d1538_upload-silverware-files/upload-silverware-files.png', 'https://video.udacity-data.com/topher/2021/March/604d1555_image-upload-done-object-detection/image-upload-done-object-detection.png', 'https://video.udacity-data.com/topher/2021/March/604d1633_object-detection-untagged-images/object-detection-untagged-images.png', 'https://video.udacity-data.com/topher/2021/March/604d164f_object-detection-image-detail-fork/object-detection-image-detail-fork.png', 'https://video.udacity-data.com/topher/2021/March/604d166c_object-detection-image-detail-knife/object-detection-image-detail-knife.png', 'https://video.udacity-data.com/topher/2021/March/604d168b_object-detection-image-detail-next/object-detection-image-detail-next.png', 'https://video.udacity-data.com/topher/2021/March/604d16a1_object-detection-image-detail-close/object-detection-image-detail-close.png', 'https://video.udacity-data.com/topher/2021/March/604d16ed_train-classification-model/train-classification-model.png', 'https://video.udacity-data.com/topher/2021/March/604d170b_choose-training-type/choose-training-type.png', 'https://video.udacity-data.com/topher/2021/March/604d1a62_toolbar-quick-test/toolbar-quick-test.png', 'https://video.udacity-data.com/topher/2021/March/604d1a7d_object-detection-quick-test-01/object-detection-quick-test-01.png', 'https://video.udacity-data.com/topher/2021/March/604d1b75_object-detection-publish/object-detection-publish.png', 'https://video.udacity-data.com/topher/2021/March/604d1b90_object-detection-publish-model/object-detection-publish-model.png', 'https://video.udacity-data.com/topher/2021/March/604d1bd4_project-settings/project-settings.png', 'https://video.udacity-data.com/topher/2021/March/604d1bef_object-detection-project-settings/object-detection-project-settings.png', 'https://video.udacity-data.com/topher/2021/March/604d1c27_resource-group/resource-group.png', 'https://video.udacity-data.com/topher/2021/March/604d1c47_prediction-keys-and-endpoints/prediction-keys-and-endpoints.png', 'https://video.udacity-data.com/topher/2021/March/604d1c77_jupyter-link/jupyter-link.png', 'https://video.udacity-data.com/topher/2021/March/604d1ccd_object-detection-variables-cell/object-detection-variables-cell.png', 'https://video.udacity-data.com/topher/2021/March/604d1cd4_object-detection-variables/object-detection-variables.png', 'https://video.udacity-data.com/topher/2021/March/604d1d38_client-detect-image/client-detect-image.png']",
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.10  Image Classification,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Image Classification: What It Is And How It Works</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Image classification</strong> is the process of using machine learning models to <strong>classify or categorize images</strong> based on the <strong>primary subject matter</strong> they contain. For example, an image classification system selects the most prominent feature detected within a photograph, such as a dog, a herd of horses, or a famous landmark, such as the Grand Canyon, and classifies the image on that. Image classification models are created by training them with a set of images that have already been evaluated and labeled with the desired classification.</p>\n<p class=""chakra-text css-o3oz8b""><strong>Azure Computer Vision</strong> uses the name <strong>image categorization</strong> for this process, and it returns a categories property in its JSON response. The machine learning models used by Computer Vision have been <strong>trained to recognize 86 possible categories</strong>. The classes are structured in a taxonomy-based parent-child hereditary hierarchy.</p>\n<p class=""chakra-text css-o3oz8b"">The <strong>Azure Custom Vision service</strong> is designed for situations where your solutions need to <strong>define custom classifications</strong>. Many applications require custom image classification capabilities because the 86 categories defined within the Azure Computer Vision category taxonomy do not include the desired labels or because they need domain or industry-specific labels. Building custom image classifiers using Azure Custom Vision allows you to use the Computer Vision image recognition machine learning algorithm and train it using images and tags you provide.</p>\n<p class=""chakra-text css-o3oz8b"">Both Azure Computer Vision and Custom Vision provide developers with REST APIs and native SDKs for using their functionality in your solutions.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Demo: Building a custom image classification model with Azure Custom Vision</h3></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Custom Vision Prediction Client <code class=""chakra-code css-1u83yg1"">classify_image</code> method (Python SDK)</h3>\n<p class=""chakra-text css-o3oz8b"">In the demo, we published a custom image classification model to a Custom Vision prediction resource. To consume the image classifier from a Jupyter notebook, we would use the native Python SDK to access the Custom Vision library. The <code class=""chakra-code css-1u83yg1"">classify_image</code> method of the Custom Vision Prediction Client provides access to the model. The steps below detail how it can be used to retrieve the classification tag for an analyzed image.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">The first step required to use the Custom Vision Prediction Client, is to install it in the Jupyter environment we are using to execute a notebook. You install the Custom Vision library using a <code class=""chakra-code css-1u83yg1"">pip install</code> command, such as the following:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-bash"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>pip </span><span class=""token"" style=""color: rgb(153, 0, 0); font-weight: bold;"">install</span><span> azure-cognitiveservices-vision-customvision</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Next, import the required components from the Custom Vision library.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> azure</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>cognitiveservices</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>vision</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>customvision</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>prediction </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> CustomVisionPredictionClient\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> msrest</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>authentication </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> ApiKeyCredentials</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Create an <code class=""chakra-code css-1u83yg1"">ApiKeyCredentials</code> object using the key for your Custom Vision prediction resource.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>credentials </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> ApiKeyCredentials</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>in_headers</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">{</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Prediction-key""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> key</span><span class=""token"" style=""color: rgb(15, 43, 61);"">}</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Instantiate a new Custom Vision Prediction Client, referencing the credential object you created above plus the endpoint of your Custom Vision prediction service.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>client </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> CustomVisionPredictionClient</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> credentials</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>credentials</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">With your <code class=""chakra-code css-1u83yg1"">client</code> object created, you can call the <code class=""chakra-code css-1u83yg1"">classify_image</code> method on that client to classify an image.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Provide the Project Id for your deployed image classification model</span><span>\n</span><span>projectId </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'b619674f-ec29-4f5f-b037-bdad9e4b26ce\'</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># The modelName value must exactly match the model name you set when publishing the model</span><span>\n</span><span>modelName </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'image-classification\'</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Retrieve an image from the \'images\' folder</span><span>\n</span><span>imagePath </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> os</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>path</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>join</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'images\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'image-01.jpeg\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Open the image as a stream and pass it into client.detect_image</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">with</span><span> </span><span class=""token"" style=""color: rgb(0, 121, 162);"">open</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>imagePath</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> mode</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""rb""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">as</span><span> imageData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>  results </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>classify_image</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>projectId</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> modelName</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> imageData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">The code above opens an image named <code class=""chakra-code css-1u83yg1"">image-01.jpeg</code> from the <code class=""chakra-code css-1u83yg1"">images</code> folder as a stream. The <code class=""chakra-code css-1u83yg1"">projectId</code>, <code class=""chakra-code css-1u83yg1"">modelName</code>, and <code class=""chakra-code css-1u83yg1"">imageData</code> values are passed into the <code class=""chakra-code css-1u83yg1"">client.classify_image()</code> method for analysis. The return value from the method is stored in a variable named <code class=""chakra-code css-1u83yg1"">results</code>.</p>\n</blockquote>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">The <code class=""chakra-code css-1u83yg1"">client.classify_image()</code> method returns a results object, which includes an array of predicted objects identified in the image. To access the predictions, you use the <code class=""chakra-code css-1u83yg1"">results.predictions</code> value. For example, to loop through each prediction you would do something similar to the following:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>prediction </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> results</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>predictions</span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(2, 124, 124);"">0</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>tag_name\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'The predicted classification for this image is {}\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span class=""token"" style=""color: rgb(0, 121, 162);"">format</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>tag_name</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">In this case, we are grabbing the first prediction and printing out the predicted classification tag.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>A.P.</strong>: Provides a measure of a classification model\'s accuracy by summarizing the precision and recall values.</li><li class=""css-cvpopp""><strong>Multiclass classification</strong>: A classification type that allows a single image to contain one or more tags (e.g., the photo includes cows, horses, and pigs).</li><li class=""css-cvpopp""><strong>Multilabel classification</strong>: A classification type for which images can only be assigned one tag (e.g., an image can contain a cow, a horse, or a pig, but not simultaneously).</li><li class=""css-cvpopp""><strong>Precision</strong>: Indicates the percentage of predicted classifications that were correct. This number indicates how likely your classification model is to be correct if it predicts a tag.</li><li class=""css-cvpopp""><strong>Probability threshold</strong>: The minimum probability score for a prediction to be valid when calculating precision and recall.</li><li class=""css-cvpopp""><strong>Recall:</strong> The percentage of image tags that your model should have correctly predicted that were actually tagged.</li><li class=""css-cvpopp""><strong>Supervised learning</strong>: An approach to machine learning which trains models by providing example inputs, such as images, and pairing those with expected outputs, such as classification tags. Also referred to as <strong>supervised machine learning</strong>.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional resources</h3>\n<p class=""chakra-text css-o3oz8b"">Here are some links you can use to learn more about the following topics:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-categorizing-images"">Image categorization in Azure Computer Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/computer-vision/category-taxonomy"">Computer Vision 86-category taxonomy<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://westcentralus.dev.cognitive.microsoft.com/docs/services/computer-vision-v3-1-ga/operations/56f91f2e778daf14a499f21b"">Computer Vision Analyze Image API<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/Custom-Vision-Service/"">Azure Custom Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://southcentralus.dev.cognitive.microsoft.com/docs/services/Custom_Vision_Training_3.3/operations/5eb0bcc6548b571998fddebd"">Custom Vision Training API<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://southcentralus.dev.cognitive.microsoft.com/docs/services/Custom_Vision_Prediction_3.1/operations/5eb37d24548b571998fde5f3"">Custom Vision Prediction API<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-customvision/azure.cognitiveservices.vision.customvision.prediction.operations.customvisionpredictionclientoperationsmixin?view=azure-python#classify-image-project-id--published-name--image-data--application-none--custom-headers-none--raw-false----operation-config-"">Azure Custom Vision Prediction client classify_image() method<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://en.wikipedia.org/wiki/Supervised_learning"">Supervised machine learning<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>']",[],https://www.youtube.com/embed/xKefkBVkHLg
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.11  Quiz: Image Classification,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Image classification models characterize an image based on its primary subject matter. Which items represent the output of an object detection model not provided by image classification models?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">A list individual objects within an image and labels for each</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">A confidence score</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The pixel coordinates locating each entity within the image</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The count of all similar objects</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The label of primary subject matter of the image</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The image below displays the performance metrics for the first iteration of a new image classification model you are building. Referencing the values displayed within the image graphs, what percentage of the classification tags assigned to images by your model are correct?</p>\n<img alt=""Model performance metrics charts"" src=""https://video.udacity-data.com/topher/2021/February/6036f58d_model-performance/model-performance.png"" class=""chakra-image css-0"" node=""[object Object]""></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">80.0%</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">66.7%</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">91.1%</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Match the performance-related terms below to the correct definitions:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Precision</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Probability threshold</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Recall</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">AP</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Accuracy</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The likelihood that your classification model is correct if it predicts a tag</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Minimum probability score for a prediction to be valid</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Percentage of tags a model should have correctly predicted by your model that actually were tagged</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Measure of model accuracy</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What is the expected output of an image classification model?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">A rectangle defining a bounding box and confidence score</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">A probability or confidence score of each tag and a prediction label</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">A confidence score, prediction tag, and rectangle with object position in the image</p></div>']",['https://video.udacity-data.com/topher/2021/February/6036f58d_model-performance/model-performance.png'],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.12  Exercise: Create An Image Classification Solution,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Introduction</h2>\n<p class=""chakra-text css-o3oz8b"">You have now learned about the capabilities of the Computer Vision service and how it can be used to create metadata and categorize images. In many business cases, organizations need to take things one step further and using images of their own to train a machine learning model. Often, companies do not have data scientists or the amount of data required to build, train, and custom machine learning models. Azure Custom Vision was created for this scenario. With Custom Vision, you can use the Computer Vision machine learning models developed by Microsoft and train them with images you provide, giving you the best of both worlds.</p>\n<p class=""chakra-text css-o3oz8b"">In this exercise, you use the Azure Custom Vision service to train a custom image classification model, and then deploy the model to a prediction service for consumption. You will then use a Jupyter notebook in Azure Machine Learning Studio to send images into your prediction service for categorization and examine the results.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Requirements</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">You must have an Azure subscription to complete this exercise. If you don\'t have one, you can sign up for a free trial at <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free"">https://azure.microsoft.com/free<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li><li class=""css-cvpopp"">You will not be using a Udacity workspace to complete this exercise.</li><li class=""css-cvpopp"">You must have completed the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://coco.udacity.com/mocha/programs/nd099_t/en-us/1.0.0/courses/5ddef5bf-04d1-4b9d-94af-dac454840080/lessons/839a81b8-95d8-44df-b88f-49a5202d9152/pages/005ec524-d2e5-41cd-a752-fb4a9d04f96b"">Exercise Resource Setup<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in order to complete this exercise.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Instructions</h2>\n<p class=""chakra-text css-o3oz8b"">After completing all of the tasks outlined in the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://coco.udacity.com/mocha/programs/nd099_t/en-us/1.0.0/courses/5ddef5bf-04d1-4b9d-94af-dac454840080/lessons/839a81b8-95d8-44df-b88f-49a5202d9152/pages/005ec524-d2e5-41cd-a752-fb4a9d04f96b"">Exercise Resource Setup guide<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, complete the tasks listed below to create, train, test, and publish a custom image classification model. Then, follow the steps within the indicated Jupyter notebook to consume the deployed model using Python code.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Navigate to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.customvision.ai/"">Computer Vision portal<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in a web browser and sign in using the account associated with your Azure subscription.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the Custom Vision portal, create a new project with the following settings:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Name</strong>: Enter <code class=""chakra-code css-1u83yg1"">Image classifier</code>.</li><li class=""css-cvpopp""><strong>Resource</strong>: Associate the project with your Custom Vision training resource.</li><li class=""css-cvpopp""><strong>Project Type</strong>: Choose <code class=""chakra-code css-1u83yg1"">Classification</code>.</li><li class=""css-cvpopp""><strong>Classification Type</strong>: Select <code class=""chakra-code css-1u83yg1"">Multiclass</code>.</li><li class=""css-cvpopp""><strong>Domain</strong>: Use the <code class=""chakra-code css-1u83yg1"">General</code> domain.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Download the image classification exercise training images from <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/image-classification/training-images.zip?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/image-classification/training-images.zip?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Upload the <strong>cat</strong> images in the <code class=""chakra-code css-1u83yg1"">training-images/cats</code> folder you downloaded and tag them <code class=""chakra-code css-1u83yg1"">cat</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Upload the <strong>dog</strong> images in the <code class=""chakra-code css-1u83yg1"">training-images/dogs</code> folder you download and tag them <code class=""chakra-code css-1u83yg1"">dog</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Train the custom image classification model using the Quick Training method.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Test the model using these image URLs:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/image-classification/test-images/cat-test-01.jpg?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/image-classification/test-images/cat-test-01.jpg?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/image-classification/test-images/dog-test-01.jpg?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/image-classification/test-images/dog-test-01.jpg?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Publish the model to your Custom Vision\'s prediction resource with the name <code class=""chakra-code css-1u83yg1"">image-classification-exercise</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Retrieve the <code class=""chakra-code css-1u83yg1"">Project Id</code> for your deployed object detection model.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Retrieve the key and endpoint values for your Custom Vision prediction resource.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Open <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">Azure Machine Learning Studio<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, locate the <code class=""chakra-code css-1u83yg1"">udacity-ml-vm</code> compute you created during the exercise resource setup, and open its associated <code class=""chakra-code css-1u83yg1"">Jupyter</code> environment.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the Jupyter file system, navigate to the <code class=""chakra-code css-1u83yg1"">AI_fundamentals</code> folder under your user folder, and then drill down to the <code class=""chakra-code css-1u83yg1"">image-classification-exercise.ipynb</code> notebook in the <code class=""chakra-code css-1u83yg1"">computer-vision/exercises/image-classification</code> folder. Follow the instructions within the notebook to complete the rest of the exercise.</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.13  Solution: Create An Image Classification Solution,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Solution: Create An Image Classification Solution</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The tasks below provide step-by-step instructions to walk you through the solution to the object detection solution exercise. Following these steps, you will upload images to the Custom Vision service and use them for training an object detection model. You will then publish the trained model to your prediction Cognitive service and consume the model from a Jupyter notebook in Azure Machine Learning.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 1: Log into the Custom Vision portal</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Open a web browser and go to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.customvision.ai"">https://www.customvision.ai<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select <strong>Sign In</strong> and enter your Azure portal credentials if prompted.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Custom Vision portal</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">You will be directed to the Custom Vision portal\'s home page.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Custom Vision portal</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 2: Create a new image classification project</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">In the Custom Vision portal, select <strong>New Project</strong>.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">New project</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">In the Create new project dialog, enter the following:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Name</strong>: Enter <code class=""chakra-code css-1u83yg1"">Image classifier</code>.</li><li class=""css-cvpopp""><strong>Description</strong>: Enter a short description, such as <code class=""chakra-code css-1u83yg1"">Image classification exercise</code>.</li><li class=""css-cvpopp""><strong>Resource</strong>: Select your training Cognitive service.</li><li class=""css-cvpopp""><strong>Project Types</strong>: Choose <code class=""chakra-code css-1u83yg1"">Classification</code>.</li><li class=""css-cvpopp""><strong>Classification Types</strong>: Choose <code class=""chakra-code css-1u83yg1"">Multiclass</code>.</li><li class=""css-cvpopp""><strong>Domains</strong>: Choose <code class=""chakra-code css-1u83yg1"">General</code>.</li></ul>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create new Custom Vision Project</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Create project</strong>.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 3: Download training images</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select the link provided and download the ZIP file containing training images.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/training-images.zip?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/object-detection/training-images.zip?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Extract the download ZIP file to a local folder, such as <code class=""chakra-code css-1u83yg1"">C:\\udacity\\training-images\\</code>.</p>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b""><strong>Note</strong>: The project will categorize images as containing either a cat or dog, so the training images are grouped into two folders, <code class=""chakra-code css-1u83yg1"">cats</code> and <code class=""chakra-code css-1u83yg1"">dogs</code>.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 4: Upload and tag images</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Return to your newly created Custom Vision project and select <strong>Add images</strong> from the toolbar.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Add images</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Browse to the directory where you extracted the training images you downloaded above. Select the <strong>cats</strong> folder, select all of the images in the folder, and select <strong>Open</strong> in the Open file dialog.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">In the <strong>Image upload</strong> dialog, enter <code class=""chakra-code css-1u83yg1"">cat</code> into the <strong>My Tags</strong> field and then select <strong>Upload 15 files</strong>.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Image tagging and upload</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""4"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select <strong>Done</strong> when the upload completes.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Repeat the previous step, this time selecting all of the photos in the <strong>dogs</strong> folder and entering <code class=""chakra-code css-1u83yg1"">dog</code> into the <strong>My Tags</strong> field. There should be 15 dog images uploaded.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Image tagging and upload</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""6"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Done</strong> when the upload completes.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 5: Train the image classification model</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>Train</strong> in the Custom Vision header bar to select a training type.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Custom Vision toolbar</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Choose the <code class=""chakra-code css-1u83yg1"">Quick training</code> training type and select <strong>Train</strong>.</p>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b""><strong>Note</strong>: Training may take a few minutes to complete.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Choose training type</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 6: Test the model</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Test your model selecting <strong>Quick Test</strong> on the toolbar.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Custom Vision toolbar</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Use the following image URLs for testing by copying and pasting each one into the <strong>Image URL</strong> box and selecting the box containing an arrow:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/image-classification/test-images/cat-test-01.jpg?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/image-classification/test-images/cat-test-01.jpg?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/image-classification/test-images/dog-test-01.jpg?raw=true"">https://github.com/udacity/AI_fundamentals/blob/main/computer-vision/exercises/image-classification/test-images/dog-test-01.jpg?raw=true<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Quick Test</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">Close the <strong>Quick Test</strong> dialog when you are done testing.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 7: Publish the custom image classification model</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">From the image classification project in Custom Vision studio, select <strong>Publish</strong> on your model\'s <strong>Performance</strong> tab.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Publish</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Name the model <code class=""chakra-code css-1u83yg1"">image-classification-exercise</code>, select your <strong>-Prediction</strong> resource from the list, and then select <strong>Publish</strong>.</p>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">This will deploy your image classification model to the prediction cognitive services resource that was created when you provisioned the Custom Vision service.</p>\n</blockquote>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Publish Model</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 8: Retrieve the Project Id for the deployed model</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Open your Custom Vision project\'s <code class=""chakra-code css-1u83yg1"">Settings</code> page by selecting the gear icon on the toolbar</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Project settings</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">Locate the <strong>Project Id</strong> field and then leave the page open for the next task. You will need the <code class=""chakra-code css-1u83yg1"">Project Id</code> value to complete the steps contained within the notebook below.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Project settings</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 9: Retrieve the key and endpoint for the Custom Vision prediction resource</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Open a new browser window, return to the Azure portal, and navigate back to the <code class=""chakra-code css-1u83yg1"">udacity-exercises</code> resource group. Within the resource group, select the <strong>-Prediction</strong> Cognitive service.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Resource group</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">On the prediction cognitive service blade, select <strong>Keys and Endpoints</strong> from the left-hand menu and leave this page open. You will need the <code class=""chakra-code css-1u83yg1"">Key</code> and <code class=""chakra-code css-1u83yg1"">Endpoint</code> values from your prediction cognitive service resource.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Keys and Endpoint</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 10: Open Azure Machine Learning Studio</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">In a web browser, navigate to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">https://ml.azure.com/<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> and sign in, if prompted.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select the <strong>Compute</strong> item in the left-hand navigation menu.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">On the compute tab, locate the <code class=""chakra-code css-1u83yg1"">udacity-ml-vm</code> compute resource and select the <strong>Jupyter</strong> application URI.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Jupyter link</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 11: Open the image-classification-exercise Jupyter notebook</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Navigate down to the <code class=""chakra-code css-1u83yg1"">AI_fundamentals/computer-vision/exercises/image-classification</code> folder.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select the <code class=""chakra-code css-1u83yg1"">image-classification-exercise.ipynb</code> notebook to open it.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 12: Paste the copied values into the notebook</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Scroll down to the variables cell in the notebook.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Variables cell</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Paste the Project Id value copied from the Custom Vision portal into the value for the <code class=""chakra-code css-1u83yg1"">projectId</code> variable.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Paste the <code class=""chakra-code css-1u83yg1"">Key 1</code> value copied from the prediction resource\'s Keys and Endpoint page in the Azure portal into the <code class=""chakra-code css-1u83yg1"">key</code> variable\'s value.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Paste the <code class=""chakra-code css-1u83yg1"">Endpoint</code> value copied from the prediction resource\'s Keys and Endpoint page in the Azure portal into the <code class=""chakra-code css-1u83yg1"">endpoint</code> variable\'s value.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Verify the <code class=""chakra-code css-1u83yg1"">modelName</code> variable matches the name you entered when publishing your model. This should be <code class=""chakra-code css-1u83yg1"">image-classification-exercise</code>.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Your updated variables cell should look similar to the following:</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Updated variables</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 13: Instantiate a CustomVisionPredictionClient object</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">For the TODO in this cell, you will enter <code class=""chakra-code css-1u83yg1"">CustomVisionPredictionClient(endpoint=endpoint, credentials=credentials)</code> on this line to create a new <code class=""chakra-code css-1u83yg1"">CustomVisionPredictionClient</code> object. The final line will look like:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>client </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> CustomVisionPredictionClient</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> credentials</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>credentials</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Task 14: Complete the call to the classify_image method</h2>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">To call the <code class=""chakra-code css-1u83yg1"">classify_image</code> method from Python is as follows.</li></ol>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>results </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>classify_image</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>projectId</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> modelName</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> imageData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">You can learn more by reading the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-customvision/azure.cognitiveservices.vision.customvision.prediction.operations.customvisionpredictionclientoperationsmixin?view=azure-python#classify-image-project-id--published-name--image-data--application-none--custom-headers-none--raw-false----operation-config-"">classify_image documentation<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</p>\n</blockquote></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Classify image</p></div>']","['https://video.udacity-data.com/topher/2021/March/604d1326_custom-vision-sign-in/custom-vision-sign-in.png', 'https://video.udacity-data.com/topher/2021/March/604d1349_custom-vision-home/custom-vision-home.png', 'https://video.udacity-data.com/topher/2021/March/604d13b3_custom-vision-new-project-tile/custom-vision-new-project-tile.png', 'https://video.udacity-data.com/topher/2021/March/604d487d_custom-vision-create-new-project/custom-vision-create-new-project.png', 'https://video.udacity-data.com/topher/2021/March/604d14e9_custom-vision-add-images/custom-vision-add-images.png', 'https://video.udacity-data.com/topher/2021/March/604d4923_upload-cat-images/upload-cat-images.png', 'https://video.udacity-data.com/topher/2021/March/604d492c_upload-dog-images/upload-dog-images.png', 'https://video.udacity-data.com/topher/2021/March/604d16ed_train-classification-model/train-classification-model.png', 'https://video.udacity-data.com/topher/2021/March/604d170b_choose-training-type/choose-training-type.png', 'https://video.udacity-data.com/topher/2021/March/604d1a62_toolbar-quick-test/toolbar-quick-test.png', 'https://video.udacity-data.com/topher/2021/March/604d4a28_quick-test-cat/quick-test-cat.png', 'https://video.udacity-data.com/topher/2021/March/604d4a7d_publish-button/publish-button.png', 'https://video.udacity-data.com/topher/2021/March/604d4a97_publish-model/publish-model.png', 'https://video.udacity-data.com/topher/2021/March/604d1bd4_project-settings/project-settings.png', 'https://video.udacity-data.com/topher/2021/March/604d4af7_image-classification-project-settings/image-classification-project-settings.png', 'https://video.udacity-data.com/topher/2021/March/604d1c27_resource-group/resource-group.png', 'https://video.udacity-data.com/topher/2021/March/604d1c47_prediction-keys-and-endpoints/prediction-keys-and-endpoints.png', 'https://video.udacity-data.com/topher/2021/March/604d1c77_jupyter-link/jupyter-link.png', 'https://video.udacity-data.com/topher/2021/March/604d4ba6_image-classification-variables-cell/image-classification-variables-cell.png', 'https://video.udacity-data.com/topher/2021/March/604d4bbb_image-classification-variables/image-classification-variables.png', 'https://video.udacity-data.com/topher/2021/March/604d4bfe_client-classify-image/client-classify-image.png']",
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.14  Facial Recognition,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Facial Recognition</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Facial detection and recognition are perhaps the computer vision capabilities with which many of us are most familiar. These capabilities are designed to analyze and extract information about human faces and can be broken down into two broad categories:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">Face detection</code>, which is about discovering and analyzing faces in images. Faces identified within an image are analyzed to detect numerous attributes, such as age, gender, hair color, and various other traits.</li><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">Facial recognition</code> is concerned with performing comparison operations on two faces to determine if they belong to the same person or share enough similar features to be labeled as a possible match.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The Azure Computer Vision service provides limited face detection functionality, so most developers will want to use the Azure Face service to take advantage of a much richer set of capabilities in their applications. The Face service performs facial detection using a 27-point landmark mapping and analyzes faces to identify many attributes, including age, gender, and emotion, to name a few. Facial recognition operations are also available using the Face service and use several objects to compare and persist identities associated with faces.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Demo: Detect and Analyze Faces With the Azure Face Service</h3>\n<p class=""chakra-text css-o3oz8b"">A guided demo of how to detect and analyze faces using the Azure Face service.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Demo: Recognize and Identify Faces With the Azure Face Service</h3>\n<p class=""chakra-text css-o3oz8b"">A guided demo of how to perform facial recognition functions using the Azure Face service.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Azure Cognitive Service Face client operations (Python SDK)</h3>\n<p class=""chakra-text css-o3oz8b"">In the demo, we uses multiple Face client operations to analyze and recognize faces. To access the Face client from a Jupyter notebook, we used the native Python SDK to access the Azure Cognitive Services Face library. The steps below detail how to use the Face library and the operations on available with the Face client.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">The first step required to use the Face Client, is to install it in the Jupyter environment we are using to execute a notebook. You install the Custom Vision library using a <code class=""chakra-code css-1u83yg1"">pip install</code> command, such as the following:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-bash"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>pip </span><span class=""token"" style=""color: rgb(153, 0, 0); font-weight: bold;"">install</span><span> azure-cognitiveservices-vision-face</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Next, import the required components from the Face library.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> azure</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>cognitiveservices</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>vision</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>face </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> FaceClient\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> msrest</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>authentication </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> CognitiveServicesCredentials</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Instantiate a new Face Client, referencing the endpoint of your Cognitive Services account and a credential object using the key for your Cognitive Service account.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>key </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'008bc0c83d6840b3badbf662f040925d\'</span><span> </span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#\'COGNITIVE_SERVICES_KEY\'</span><span>\n</span><span>endpoint </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'https://udacity-cog-services.cognitiveservices.azure.com/\'</span><span> </span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#\'COGNITIVE_SERVICES_ENDPOINT\'</span><span>\n</span><span>client </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> FaceClient</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> CognitiveServicesCredentials</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>key</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">With your <code class=""chakra-code css-1u83yg1"">client</code> object created, you can call various <code class=""chakra-code css-1u83yg1"">Face</code> operations available with the Face client. These methods include:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">detect_with_stream</code> detects and analyzes faces using a binary stream of image data.</li><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">detect_with_url</code> is similar to <code class=""chakra-code css-1u83yg1"">detect_with_stream</code>, but detects and analyzes an image at a specified URL.</li><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">find_similar</code> compares faces in multiple images to indicate with the faces belong to the same person or share similar attributes, if not the same person.</li><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">group</code> divides faces into groups based on face similarity.</li><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">identify</code> performs a one-to-one identification to find the matches for a face provided in other images.</li><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">verify_face_to_face</code> compares faces in two images to determine if they are the same face.</li><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">verify_face_to_person</code> compares a saved person\'s face to the face in an image to decide if they are the same person.</li></ul>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">To provide an example, we can detect faces and retrieve attributes about those faces by doing the following:</p>\n</li></ol>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Open an image</span><span>\n</span><span>imagePath </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> os</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>path</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>join</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'images\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'people.jpg\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Detect faces and specified facial attributes</span><span>\n</span><span>attributes </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'age\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'gender\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'emotion\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Detect faces</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">with</span><span> </span><span class=""token"" style=""color: rgb(0, 121, 162);"">open</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>imagePath</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> mode</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""rb""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">as</span><span> imageData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>  faceResults </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>face</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>detect_with_stream</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>imageData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> return_face_attributes</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>attributes</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> face </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> faceResults</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>  r </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> face</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>face_rectangle\n</span><span>  bounding_box </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>r</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>left</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> r</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>top</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>r</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>left </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">+</span><span> r</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>width</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> r</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>top </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">+</span><span> r</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>height</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span>  draw </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> ImageDraw</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>Draw</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>img</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span>  draw</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>rectangle</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>bounding_box</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> outline</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'magenta\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> width</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(2, 124, 124);"">5</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">The code above opens an image named <code class=""chakra-code css-1u83yg1"">people.jpg</code> from the <code class=""chakra-code css-1u83yg1"">images</code> folder as a stream, named <code class=""chakra-code css-1u83yg1"">imageData</code>. The <code class=""chakra-code css-1u83yg1"">imageData</code> value is passed into the <code class=""chakra-code css-1u83yg1"">client.face.detect_with_stream()</code> method for analysis. In addition, the <code class=""chakra-code css-1u83yg1"">return_face_attributes</code> argument is passed in, containing an array of facial attributes to return, including age, gender, and emotion. The return value from the method is stored in a variable named <code class=""chakra-code css-1u83yg1"">faceResults</code>. Use the <code class=""chakra-code css-1u83yg1"">faceResults</code> value, we can loop through the faces returned and in the code above, we are drawing a bounding box around each face using its <code class=""chakra-code css-1u83yg1"">face_rectangle</code> property.</p>\n</blockquote></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Face detection</strong>: Using computer vision-related machine learning models to find and analyze faces in images. Faces identified within an image are analyzed to detect numerous attributes, such as age, gender, hair color, and various other traits.</li><li class=""css-cvpopp""><strong>Facial recognition</strong>: The processing of using AI to compare two faces to determine if they belong to the same person or share enough similar features to be labeled as a possible match.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional resources</h3>\n<p class=""chakra-text css-o3oz8b"">Here are some links you can use to learn more about the following topics:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/face/overview"">Azure Face service<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-detecting-faces"">Detecting faces with Azure Computer Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395236"">Face API<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/face/quickstarts/client-libraries?tabs=visual-studio&amp;pivots=programming-language-csharp"">Using the Face client library<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/python/api/azure-cognitiveservices-vision-face/azure.cognitiveservices.vision.face.operations.faceoperations?view=azure-python#detect-with-stream-image--return-face-id-true--return-face-landmarks-false--return-face-attributes-none--recognition-model--recognition-01---return-recognition-model-false--detection-model--detection-01---custom-headers-none--raw-false--callback-none----operation-config-"">Azure Cognitive Services Face client detect_with_stream() method documentation<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>']",[],https://www.youtube.com/embed/FXgTNlLF0XI
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.15  Quizzes: Facial Recognition,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Quizzes: Facial Recognition</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following types of information does the face detection capability of <strong>Azure Computer Vision</strong> return?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Age</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Emotion</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Gender</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Hair color</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Smile</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following attributes is the <strong>Azure Face service</strong> is capable of identifying?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Age</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Happiness</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Race</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Gender</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Intelligence</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Match the Azure Face service\'s facial recognition operations to their correct functionality.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Recognize</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Find similar</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Identify</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Group</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Detect</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Verify</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Compare faces to determine if they belong to the same person</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Compares a face against known faces to indicate to whom the face belongs</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Compares a face against known faces and returns faces that share comparable traits</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Cluster faces of the same person together and allowing combining them on like features</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You\'ve learned about the various Face operations available using the Python SDK\'s Face library. Assuming you\'ve created a Face client named <code class=""chakra-code css-1u83yg1"">client</code>, which of the following are valid methods for detecting faces in images using the Face client?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">client.face.analyze()</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">client.face.detect()</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">client.face.detect_with_stream()</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">client.face.detect_with_url()</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">client.face.detect_with_attributes()</code></p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.16  Exercise: Detect And Analyze Faces With The Face Service,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Introduction</h2>\n<p class=""chakra-text css-o3oz8b"">You have now learned about the capabilities of the Azure Face service and how it can be used to detect, analyze, and recognize faces. In this exercise, you use the Azure Face service to explore the detect and analyze capabilities by using a Jupyter notebook in Azure Machine Learning Studio to send images into the Face service.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Requirements</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">You must have an Azure subscription to complete this exercise. If you don\'t have one, you can sign up for a free trial at <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free"">https://azure.microsoft.com/free<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li><li class=""css-cvpopp"">You will not be using a Udacity workspace to complete this exercise.</li><li class=""css-cvpopp"">You have completed the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://coco.udacity.com/mocha/programs/nd099_t/en-us/1.0.0/courses/5ddef5bf-04d1-4b9d-94af-dac454840080/lessons/839a81b8-95d8-44df-b88f-49a5202d9152/pages/005ec524-d2e5-41cd-a752-fb4a9d04f96b"">Exercise Resource Setup<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in order to complete this exercise.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Instructions</h2>\n<p class=""chakra-text css-o3oz8b"">After completing all of the tasks outlined in the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://coco.udacity.com/mocha/programs/nd099_t/en-us/1.0.0/courses/5ddef5bf-04d1-4b9d-94af-dac454840080/lessons/839a81b8-95d8-44df-b88f-49a5202d9152/pages/005ec524-d2e5-41cd-a752-fb4a9d04f96b"">Exercise Resource Setup guide<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, follow the steps within the indicated Jupyter notebook to use various Face service operations using Python code.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Retrieve the key and endpoint values for your Cognitive Services resource.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Open <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">Azure Machine Learning Studio<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, locate the <code class=""chakra-code css-1u83yg1"">udacity-ml-vm</code> compute you created during the exercise resource setup, and open its associated <code class=""chakra-code css-1u83yg1"">Jupyter</code> environment.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the Jupyter file system, navigate to the <code class=""chakra-code css-1u83yg1"">AI_fundamentals</code> folder under your user folder, and then drill down to the <code class=""chakra-code css-1u83yg1"">detect-faces-exercise.ipynb</code> notebook in the <code class=""chakra-code css-1u83yg1"">computer-vision/exercises/detect-faces</code> folder. Follow the instructions within the notebook to complete the rest of the exercise.</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.17  Solution: Detect And Analyze Faces With The Face Service,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Exercise Solution: Detect and Analyze Faces With the Face Service</h2>\n<p class=""chakra-text css-o3oz8b"">The tasks below provide step-by-step instructions to walk you through the solution to the Detect and Analyze Faces With the Face Service exercise. Following these steps, you will connect a Jupyter notebook running in Azure Machine Learning Studio to your Azure Cognitive Services account, and then analyze faces using the Azure Face Service.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 1: Retrieve the key and endpoint for the Custom Vision prediction resource</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Open the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com"">Azure portal<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in a new browser window, and navigate to the <code class=""chakra-code css-1u83yg1"">udacity-exercises</code> resource group. Within the resource group, select the Azure Cognitive Services resource.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Resource group</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">On the cognitive services blade, select <strong>Keys and Endpoints</strong> from the left-hand menu and leave this page open. You will need the <code class=""chakra-code css-1u83yg1"">Key</code> and <code class=""chakra-code css-1u83yg1"">Endpoint</code> values from your prediction cognitive service resource.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Keys and Endpoint</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 2: Open Azure Machine Learning Studio</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">In a web browser, navigate to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">https://ml.azure.com/<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> and sign in, if prompted.</li><li class=""css-cvpopp"">Select the <strong>Compute</strong> item in the left-hand navigation menu.</li><li class=""css-cvpopp"">On the compute tab, locate the <code class=""chakra-code css-1u83yg1"">udacity-ml-vm</code> compute resource and select the <strong>Jupyter</strong> application URI.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Jupyter link</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 3: Open the detect-faces-exercise Jupyter notebook</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Navigate down to the <code class=""chakra-code css-1u83yg1"">AI_fundamentals/computer-vision/exercises/detect-faces</code> folder.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select the <code class=""chakra-code css-1u83yg1"">detect-faces-exercise.ipynb</code> notebook to open it.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 4: Install the Azure Computer Vision Service Computer Vision Library</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">To install the Azure Computer Vision Service Computer Vision Library, the <code class=""chakra-code css-1u83yg1"">pip install</code> line should be updated to read:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-bash"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>pip </span><span class=""token"" style=""color: rgb(153, 0, 0); font-weight: bold;"">install</span><span> azure-cognitiveservices-vision-computervision</span></code></div></div></pre>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 5: Set variables in the notebook</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Scroll down to the variables cell in the notebook.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Variables cell</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Paste the <code class=""chakra-code css-1u83yg1"">Key 1</code> value copied from the Cognitive Services resource\'s Keys and Endpoint page in the Azure portal into the <code class=""chakra-code css-1u83yg1"">key</code> variable\'s value.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Paste the <code class=""chakra-code css-1u83yg1"">Endpoint</code> value copied from the Cognitive Services resource\'s Keys and Endpoint page in the Azure portal into the <code class=""chakra-code css-1u83yg1"">endpoint</code> variable\'s value.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Your updated variables cell should look similar to the following:</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Updated variables</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 6: Create a Computer Vision Client</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">For the TODO in this cell, you will enter <code class=""chakra-code css-1u83yg1"">ComputerVisionClient(endpoint, credentials)</code> on this line to create a new <code class=""chakra-code css-1u83yg1"">CustomVisionPredictionClient</code> object. The final line will look like:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>client </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> ComputerVisionClient</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> credentials</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 7: Detect faces</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">In the <strong>detect faces</strong> code cell, you want to use the <code class=""chakra-code css-1u83yg1"">detect_with_stream</code> method, passing in the <code class=""chakra-code css-1u83yg1"">imageData</code> stream. The completed line should look like:</li></ol>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>faceResults </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>face</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>detect_with_stream</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>imageData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 8: Analyze facial attributes</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">There are two cells under <strong>Analyze facial attributes</strong> that have <code class=""chakra-code css-1u83yg1"">TODOs</code>. Both of these cells require the same code update.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">In the <strong>analyze facial attributes</strong> code cells, you want to use the <code class=""chakra-code css-1u83yg1"">detect_with_stream</code> method as you did above, passing in the <code class=""chakra-code css-1u83yg1"">imageData</code> stream. To retrieve attributes from the Face service, we also need to include the <code class=""chakra-code css-1u83yg1"">return_face_attributes</code> argument and set it to the <code class=""chakra-code css-1u83yg1"">attributes</code> list that was defined in the cell. The completed line should look like:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>faceResults </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>face</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>detect_with_stream</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>imageData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> return_face_attributes</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>attributes</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 9: Find similar faces</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">There are two cells under <strong>Find similar faces</strong> that have <code class=""chakra-code css-1u83yg1"">TODOs</code>. Both of these cells require the same code update.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">In the <strong>find similar faces</strong> code cells, you want to use the <code class=""chakra-code css-1u83yg1"">find_similar</code> method of the <code class=""chakra-code css-1u83yg1"">face</code> object. Into that method, you will pass a <code class=""chakra-code css-1u83yg1"">face_id</code> argument that is set to the Face Id of the face analyzed in the first image. You also need to send a second argument named <code class=""chakra-code css-1u83yg1"">face_ids</code> containing a list of the Face Ids detected in the second image analyzed. The completed line should look like:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>similarFaces </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>face</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>find_similar</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>face_id</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>faceOne</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>face_id</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> face_ids</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>imageTwoFaceIds</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li></ol></div>']","['https://video.udacity-data.com/topher/2021/March/604e606a_resource-group-cog-services/resource-group-cog-services.png', 'https://video.udacity-data.com/topher/2021/March/604e6086_cog-services-keys-and-endpoints/cog-services-keys-and-endpoints.png', 'https://video.udacity-data.com/topher/2021/March/604d1c77_jupyter-link/jupyter-link.png', 'https://video.udacity-data.com/topher/2021/March/604e636c_detect-faces-variables-cell/detect-faces-variables-cell.png', 'https://video.udacity-data.com/topher/2021/March/604e6374_detect-faces-variables/detect-faces-variables.png']",
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.18  Read Text,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Read Text</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Optical Character Recognition</strong> or OCR is a primary focus of computer vision. The vast amount of latent information stored within opaque files, such as PDFs or JPEG images, has driven the development of OCR capabilities within images. The goal of OCR is to extract printed and handwritten text from images and scanned documents and render it as digital text. Once the text has been extracted from a document, it can be indexed, searched, and analyzed.</p>\n<p class=""chakra-text css-o3oz8b"">The Computer Vision Read API is the preferred method for reading text within printed and handwritten documents in Azure. The Read API leverages an asynchronous approach, allowing it to be optimized for reading considerable amounts of text from images and PDF documents.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Azure Computer Vision Read API Operations (Python SDK)</h3>\n<p class=""chakra-text css-o3oz8b"">The <code class=""chakra-code css-1u83yg1"">Read API</code> is the preferred way of performing OCR functions using Computer Vision. While the <code class=""chakra-code css-1u83yg1"">OCR API</code> is still available, it is more limited in its capabilities and is in the process of being deprecated. To help provide a better understanding of the <code class=""chakra-code css-1u83yg1"">Read API</code>, let\'s look at some of the operations available through the Python SDK <code class=""chakra-code css-1u83yg1"">ComputerVisionClient</code> object.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">The first step required to use the Computer Vision Client, is to install it in the Jupyter environment we are using to execute a notebook. You install the Custom Vision library using a <code class=""chakra-code css-1u83yg1"">pip install</code> command, such as the following:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-bash"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>pip </span><span class=""token"" style=""color: rgb(153, 0, 0); font-weight: bold;"">install</span><span> azure-cognitiveservices-vision-computervision</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Next, import the required components from the Computer Vision library.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> azure</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>cognitiveservices</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>vision</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>computervision </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> ComputerVisionClient\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> azure</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>cognitiveservices</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>vision</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>computervision</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>models </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> OperationStatusCodes\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> msrest</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>authentication </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> CognitiveServicesCredentials</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Instantiate a new Computer Vision Client, referencing the endpoint of your Cognitive Services account and a credential object using the key for your Cognitive Service account.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>key </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'008bc0c83d6840b3badbf662f040925d\'</span><span> </span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#\'COGNITIVE_SERVICES_KEY\'</span><span>\n</span><span>endpoint </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'https://udacity-cog-services.cognitiveservices.azure.com/\'</span><span> </span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;"">#\'COGNITIVE_SERVICES_ENDPOINT\'</span><span>\n</span><span>client </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> ComputerVisionClient</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> CognitiveServicesCredentials</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>key</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">With a <code class=""chakra-code css-1u83yg1"">client</code> object created, you can call various <code class=""chakra-code css-1u83yg1"">Read</code> operations available with the Computer Vision client. These methods include:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">read_in_stream</code> sends an asynchronous request to read text within an image or document.</li><li class=""css-cvpopp""><code class=""chakra-code css-1u83yg1"">get_read_result</code> provides the interface for retrieving the status and results of a read request.</li></ul>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">The status is check by referencing an <code class=""chakra-code css-1u83yg1"">operationId</code> value, which is returned in the header of response from the <code class=""chakra-code css-1u83yg1"">read_in_stream</code> method. Once you receive a <code class=""chakra-code css-1u83yg1"">get_read_result</code> status of <code class=""chakra-code css-1u83yg1"">succeeded</code>, and can then use the <code class=""chakra-code css-1u83yg1"">analyze_result</code> object on the returned value to read the results of the OCR operation by using the <code class=""chakra-code css-1u83yg1"">read_results</code> property.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Let\'s look at a full example of reading text from a document.</p>\n</li></ol>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Read an image file into a stream</span><span>\n</span><span>path </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> os</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>path</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>join</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'docs\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'whitepaper.pdf\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Send an async request to read text within the image</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">with</span><span> </span><span class=""token"" style=""color: rgb(0, 121, 162);"">open</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>path</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""rb""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">as</span><span> docData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>  operation </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>read_in_stream</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>docData</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> raw</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">True</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Extract the operation ID from the response headers</span><span>\n</span><span>locationHeader </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> operation</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>headers</span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Operation-Location""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span>\n</span><span>operationId </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> locationHeader</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>split</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""/""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">-</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># Wait for the asynchronous operation to complete</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">while</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">True</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>  result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>get_read_result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>operationId</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span>  </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">if</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>status </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">not</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span>OperationStatusCodes</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>running</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">break</span><span>\n</span><span>  time</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>sleep</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(2, 124, 124);"">1</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span>\n<span></span><span class=""token"" style=""color: rgb(113, 113, 101); font-style: italic;""># When the operation has completed successfully, print each line of text returned to the output</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">if</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>status </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">==</span><span> OperationStatusCodes</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>succeeded</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>  </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> res </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>analyze_result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>read_results</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> line </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> res</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>lines</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>      </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>line</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>text</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">The code above opens a PDF document named <code class=""chakra-code css-1u83yg1"">whitepaper.pdf</code> from the <code class=""chakra-code css-1u83yg1"">docs</code> folder as a stream, named <code class=""chakra-code css-1u83yg1"">docData</code>. The <code class=""chakra-code css-1u83yg1"">docData</code> value is passed into the <code class=""chakra-code css-1u83yg1"">client.read_in_stream()</code> method for asynchronous analysis. In addition to the document data stream, we also passed <code class=""chakra-code css-1u83yg1"">raw=True</code> into the <code class=""chakra-code css-1u83yg1"">read_in_stream</code> method. This instructs the API to return a direct response back to the client, which allows us to access the operation headers in the response. The <code class=""chakra-code css-1u83yg1"">Operation-Location</code> headers are then extracted from the <code class=""chakra-code css-1u83yg1"">read_in_stream</code> response, and the <code class=""chakra-code css-1u83yg1"">operationId</code> is retrieved from the headers. We then send status check requests to the <code class=""chakra-code css-1u83yg1"">Read API</code> using the <code class=""chakra-code css-1u83yg1"">get_read_results</code> operation, checking for a status of <code class=""chakra-code css-1u83yg1"">succeeded</code>. When the OCR operation completes, we can use the <code class=""chakra-code css-1u83yg1"">result.analyze_result.read_results</code> function to read each line of text returned by the read operation and print them to the output pane in a Jupyter notebook.</p>\n</blockquote></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Opaque</strong>: In data science, opaque refers to data types that do not define a concrete data structure, are not easily searchable, and do not enable simple extraction of metadata or the content contained within them. Opaque documents are things like binary image or PDF files, containing vast amounts of information, but it is not discoverable using tradition search methodologies.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional resources</h3>\n<p class=""chakra-text css-o3oz8b"">Here are some links you can use to learn more about the following topics:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://en.wikipedia.org/wiki/Opaque_data_type"">Opaque data type<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-recognizing-text"">Optical character recognition in Azure Computer Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://westcentralus.dev.cognitive.microsoft.com/docs/services/computer-vision-v3-1-ga/operations/56f91f2e778daf14a499f20d"">OCR API<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://westcentralus.dev.cognitive.microsoft.com/docs/services/computer-vision-v3-1-ga/operations/5d986960601faab4bf452005"">Read API<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/computer-vision/quickstarts-sdk/client-library?tabs=visual-studio&amp;pivots=programming-language-csharp#read-printed-and-handwritten-text"">Reading text with Azure Computer Vision<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>']",[],https://www.youtube.com/embed/02Pbn3EwGq0
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.19  Quizzes: Read Text,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Quizzes: Read Text</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You\'ve learned about the various <code class=""chakra-code css-1u83yg1"">Read API</code> operations required to perform asynchronous OCR activities using the API. Which of the following has the order of operations in the correct order to send a request, await completion of the operation, and then access the results?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Start the operation by sending a read request to <code class=""chakra-code css-1u83yg1"">read_in_stream</code>, followed by calling <code class=""chakra-code css-1u83yg1"">analyze_result.read_result_status</code> to check for the result status, and end by extracting the read results using <code class=""chakra-code css-1u83yg1"">get_read_result</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Start the operation by sending a read request to <code class=""chakra-code css-1u83yg1"">read_in_stream</code>, followed by checking for the result status by calling <code class=""chakra-code css-1u83yg1"">get_read_result</code>, and end by extracting the read results using <code class=""chakra-code css-1u83yg1"">analyze_result.read_results</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Start the operation by sending a read request to <code class=""chakra-code css-1u83yg1"">analyze_image</code>, followed by checking for the result status by calling <code class=""chakra-code css-1u83yg1"">get_read_result</code>, and end by extracting the read results using <code class=""chakra-code css-1u83yg1"">read_in_stream</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Start the operation by sending a read request to <code class=""chakra-code css-1u83yg1"">read_in_stream</code>, followed by calling <code class=""chakra-code css-1u83yg1"">get_read_result</code> to check for the result status, and finish by extracting the read results using <code class=""chakra-code css-1u83yg1"">analyze_read_results</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following are valid APIs for reading text from images using Azure Computer Vision?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Analyze Image API</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Detect Text API</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">OCR API</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Read API</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Identify API</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following file formats is NOT supported by the Azure Computer Vision OCR API?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">BMP</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">GIF</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">JPEG</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">PDF</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">PNG</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.20  Exercise: Read Text With Computer Vision Service,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Introduction</h2>\n<p class=""chakra-text css-o3oz8b"">Throughout the last segment of this lesson, you learned about the Optical Character Recognition (OCR) capabilities of Azure Computer Vision. Using the <strong>Read Text</strong> functionality of Computer Vision, you can accurately extract both printed and handwritten text from images and documents. In this exercise, you will explore some of these capabilities using a Jupyter notebook running in Azure Machine Learning Studio.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Requirements</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">You must have an Azure subscription to complete this exercise. If you don\'t have one, you can sign up for a free trial at <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/free"">https://azure.microsoft.com/free<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li><li class=""css-cvpopp"">You will not be using a Udacity workspace to complete this exercise.</li><li class=""css-cvpopp"">You have completed the<a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://coco.udacity.com/mocha/programs/nd099_t/en-us/1.0.0/courses/5ddef5bf-04d1-4b9d-94af-dac454840080/lessons/839a81b8-95d8-44df-b88f-49a5202d9152/pages/005ec524-d2e5-41cd-a752-fb4a9d04f96b"">Exercise Resource Setup<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in order to complete this exercise.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Instructions</h2>\n<p class=""chakra-text css-o3oz8b"">After completing all of the tasks outlined in the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://coco.udacity.com/mocha/programs/nd099_t/en-us/1.0.0/courses/5ddef5bf-04d1-4b9d-94af-dac454840080/lessons/839a81b8-95d8-44df-b88f-49a5202d9152/pages/005ec524-d2e5-41cd-a752-fb4a9d04f96b"">Exercise Resource Setup guide<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, follow the steps within the indicated Jupyter notebook to read text from images and documents using Python code.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Retrieve the key and endpoint values for your Cognitive Services resource.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Open <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">Azure Machine Learning Studio<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>, locate the <code class=""chakra-code css-1u83yg1"">udacity-ml-vm</code> compute you created during the exercise resource setup, and open its associated <code class=""chakra-code css-1u83yg1"">Jupyter</code> environment.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the Jupyter file system, navigate to the <code class=""chakra-code css-1u83yg1"">AI_fundamentals</code> folder under your user folder, and then drill down to the <code class=""chakra-code css-1u83yg1"">read-text-exercise.ipynb</code> notebook in the <code class=""chakra-code css-1u83yg1"">computer-vision/exercises/read-text</code> folder. Follow the instructions within the notebook to complete the rest of the exercise.</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.21  Solution: Read Text With Computer Vision Service,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Exercise Solution: Read Text With Azure Computer Vision</h2>\n<p class=""chakra-text css-o3oz8b"">The tasks below provide step-by-step instructions to walk you through the solution to the Read Text With Computer Vision exercise. Following these steps, you will connect a Jupyter notebook running in Azure Machine Learning Studio to your Azure Cognitive Services account, and then read and analyze text in images and documents using the <code class=""chakra-code css-1u83yg1"">OCR API</code> and the <code class=""chakra-code css-1u83yg1"">Read API</code>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 1: Retrieve the key and endpoint for the Custom Vision prediction resource</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Open the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com"">Azure portal<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in a new browser window, and navigate to the <code class=""chakra-code css-1u83yg1"">udacity-exercises</code> resource group. Within the resource group, select the Azure Cognitive Services resource.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Resource group</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">On the cognitive services blade, select <strong>Keys and Endpoints</strong> from the left-hand menu and leave this page open. You will need the <code class=""chakra-code css-1u83yg1"">Key</code> and <code class=""chakra-code css-1u83yg1"">Endpoint</code> values from your prediction cognitive service resource.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Keys and Endpoint</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 2: Open Azure Machine Learning Studio</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">In a web browser, navigate to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://ml.azure.com/"">https://ml.azure.com/<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> and sign in, if prompted.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select the <strong>Compute</strong> item in the left-hand navigation menu.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">On the compute tab, locate the <code class=""chakra-code css-1u83yg1"">udacity-ml-vm</code> compute resource and select the <strong>Jupyter</strong> application URI.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Jupyter link</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 3: Open the read-text-exercise Jupyter notebook</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Navigate down to the <code class=""chakra-code css-1u83yg1"">AI_fundamentals/computer-vision/exercises/read-text</code> folder.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Select the <code class=""chakra-code css-1u83yg1"">read-text-exercise.ipynb</code> notebook to open it.</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 4: Install the Azure Computer Vision Service Computer Vision Library</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">To install the Azure Computer Vision Service Computer Vision Library, the <code class=""chakra-code css-1u83yg1"">pip install</code> line should be updated to read:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-bash"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>pip </span><span class=""token"" style=""color: rgb(153, 0, 0); font-weight: bold;"">install</span><span> azure-cognitiveservices-vision-computervision</span></code></div></div></pre>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 5: Set variables in the notebook</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Scroll down to the variables cell in the notebook.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Variable cell</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Paste the <code class=""chakra-code css-1u83yg1"">Key 1</code> value copied from the Cognitive Services resource\'s Keys and Endpoint page in the Azure portal into the <code class=""chakra-code css-1u83yg1"">key</code> variable\'s value.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Paste the <code class=""chakra-code css-1u83yg1"">Endpoint</code> value copied from the Cognitive Services resource\'s Keys and Endpoint page in the Azure portal into the <code class=""chakra-code css-1u83yg1"">endpoint</code> variable\'s value.</p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Your updated variables cell should look similar to the following:</p>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Updated variables</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 6: Create a Computer Vision Client</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">For the TODO in this cell, you will enter <code class=""chakra-code css-1u83yg1"">ComputerVisionClient(endpoint, credentials)</code> on this line to create a new <code class=""chakra-code css-1u83yg1"">CustomVisionPredictionClient</code> object. The final line will look like:</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>client </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> ComputerVisionClient</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> credentials</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 7: Read scanned documents</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">In the <strong>read scanned document</strong> code cell, there are two <code class=""chakra-code css-1u83yg1"">TODOs</code> that must be addressed.\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">The <code class=""chakra-code css-1u83yg1"">operation = client.</code> line should be completed using the <code class=""chakra-code css-1u83yg1"">read_in_stream()</code> method, passing in the image stream and setting the <code class=""chakra-code css-1u83yg1"">raw</code> argument to <code class=""chakra-code css-1u83yg1"">True</code>. The final line should look like:</li></ul>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>  operation </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>read_in_stream</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>stream</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> raw</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">True</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">To complete the <code class=""chakra-code css-1u83yg1"">result = client.</code> line, make a call to the <code class=""chakra-code css-1u83yg1"">get_read_result</code> operation, which allows you to check on the status of the asynchronous operation started by the <code class=""chakra-code css-1u83yg1"">read_in_stream</code> call.</li></ul>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>  result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>get_read_result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>operationId</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 8: Read PDF documents</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">In the <strong>read PDF documents</strong> code cell, there are three <code class=""chakra-code css-1u83yg1"">TODOs</code> that must be addressed.\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">The <code class=""chakra-code css-1u83yg1"">operation = client.</code> line should be completed using the <code class=""chakra-code css-1u83yg1"">read_in_stream()</code> method, passing in the image stream and setting the <code class=""chakra-code css-1u83yg1"">raw</code> argument to <code class=""chakra-code css-1u83yg1"">True</code>. The final line should look like:</li></ul>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>  operation </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>read_in_stream</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>stream</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> raw</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">True</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">To complete the <code class=""chakra-code css-1u83yg1"">result = client.</code> line, make a call to the <code class=""chakra-code css-1u83yg1"">get_read_result</code> operation, which allows you to check on the status of the asynchronous operation started by the <code class=""chakra-code css-1u83yg1"">read_in_stream</code> call. The updated code should look like:</li></ul>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>  result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>get_read_result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>operationId</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">On the <code class=""chakra-code css-1u83yg1"">for res in result.analyze_result</code> line, you can access the results of the read operation using the <code class=""chakra-code css-1u83yg1"">read_results</code> property. The final line will look like:</li></ul>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>  </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> res </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>analyze_result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>read_results</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span></code></div></div></pre>\n</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Task 9: Read handwritten text</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">In the <strong>read handwritten text</strong> code cell, there are three <code class=""chakra-code css-1u83yg1"">TODOs</code> that must be addressed.\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">The <code class=""chakra-code css-1u83yg1"">operation = client.</code> line should be completed using the <code class=""chakra-code css-1u83yg1"">read_in_stream()</code> method, passing in the image stream and setting the <code class=""chakra-code css-1u83yg1"">raw</code> argument to <code class=""chakra-code css-1u83yg1"">True</code>. The final line should look like:</li></ul>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>  operation </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>read_in_stream</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>stream</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> raw</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">True</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">To complete the <code class=""chakra-code css-1u83yg1"">result = client.</code> line, make a call to the <code class=""chakra-code css-1u83yg1"">get_read_result</code> operation, which allows you to check on the status of the asynchronous operation started by the <code class=""chakra-code css-1u83yg1"">read_in_stream</code> call. The updated code should look like:</li></ul>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>  result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>get_read_result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>operationId</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">On the <code class=""chakra-code css-1u83yg1"">for res in result.analyze_result</code> line, you can access the results of the read operation using the <code class=""chakra-code css-1u83yg1"">read_results</code> property. The final line will look like:</li></ul>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-python"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>  </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> res </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>analyze_result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>read_results</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span></code></div></div></pre>\n</li></ol></div>']","['https://video.udacity-data.com/topher/2021/March/604e606a_resource-group-cog-services/resource-group-cog-services.png', 'https://video.udacity-data.com/topher/2021/March/604e6086_cog-services-keys-and-endpoints/cog-services-keys-and-endpoints.png', 'https://video.udacity-data.com/topher/2021/March/604d1c77_jupyter-link/jupyter-link.png', 'https://video.udacity-data.com/topher/2021/March/604e612e_read-text-variables-cell/read-text-variables-cell.png', 'https://video.udacity-data.com/topher/2021/March/604e6135_read-text-variables/read-text-variables.png']",
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.22  Process Forms,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Process Forms</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The <strong>Azure Form Recognizer</strong> is an extension of the OCR capabilities of Azure Computer Vision capable of extracting key-value pairs and tables from documents. The output from Form Recognizer is data structured to indicate the relationship between the label and value for each form field.</p>\n<p class=""chakra-text css-o3oz8b"">The machine learning model underlying the functionality of the Layout API uses <code class=""chakra-code css-1u83yg1"">unsupervised learning</code> by default to analyze forms and determine the relationship between form labels and value fields. If you are developing an application that deals with more complicated form documents, you also have the option of training the model using the forms and labels you provide, following a <code class=""chakra-code css-1u83yg1"">supervised machine learning</code> approach to improve model performance.</p>\n<p class=""chakra-text css-o3oz8b"">The Azure Form Recognizer provides services to accommodate three different approaches to dealing with form data extraction.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">The <strong>Layout API</strong>, which is the default approach to analyzing forms,</li><li class=""css-cvpopp""><strong>prebuilt models</strong> designed to extract information from invoices, receipts, and business cards, and</li><li class=""css-cvpopp""><strong>custom document processing models</strong>, which you can train by providing manually labeled forms.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">New terms</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Unsupervised learning</strong>: A machine learning pattern in which algorithms analyze unlabeled data to learn patterns and discover information.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/form-recognizer/"">Form Recognizer<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://westus2.dev.cognitive.microsoft.com/docs/services/form-recognizer-api-v2/operations/AnalyzeLayoutAsync"">Form Recognizer API<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/form-recognizer/concept-layout"">Form Recognizer Layout API<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/azure/cognitive-services/form-recognizer/quickstarts/client-library?tabs=preview%2Cv2-1&amp;pivots=programming-language-csharp"">Use the Form Recognizer client library or REST API<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>']",[],https://www.youtube.com/embed/qyftLGec0pI
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.23  Quizzes: Process Forms,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Quizzes: Process Forms</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following forms is NOT handled by the prebuilt models available in Azure Form Recognizer?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Business cards</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Invoices</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Tax forms</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Receipts</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What is the minimum number of input forms required to train a custom Form Recognizer model with your documents and data?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">1</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">2</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">5</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">10</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">20</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following are services that make up the Azure Form Recognizer?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Analyze Form API</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Custom Models</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Extract Text API</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Layout API</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Prebuilt Models</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.24  Lesson Review,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Lesson Review</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Great work! We\'ve covered a lot, so let\'s wrap up with a brief review. This lesson covered numerous services and how to use them to build custom vision solutions. We have looked at how the Azure Cognitive Services components Computer Vision, Custom Vision, Face service, and Form Recognizer help developers build solutions leveraging the fascinating field of computer vision.</p>\n<p class=""chakra-text css-o3oz8b"">With the content you\'ve covered throughout this lesson, you should be able to use the Azure Cognitive Services computer vision components to:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Extract insights about the visual features, objects, and characteristics contained within images</li><li class=""css-cvpopp"">Build, deploy, and consume a custom image classification model</li><li class=""css-cvpopp"">Detect, analyze, and recognize faces using the Face service</li><li class=""css-cvpopp"">Analyze images containing text using optical character recognition</li><li class=""css-cvpopp"">Read typed and handwritten documents using the Read API of Computer Vision</li><li class=""css-cvpopp"">Extract structured data from forms using the Form Recognizer</li></ul></div>']",[],https://www.youtube.com/embed/wIDR94v4aoM
AI Fundamentals,AI Fundamentals,Lesson 5: Computer Vision,5.25  Glossary,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Glossary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">For your reference, here are all the new terms we introduced in this lesson:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>A.P.</strong>: Stands for average precision and provides a measure of a classification model\'s accuracy by summarizing the precision and recall values.</li><li class=""css-cvpopp""><strong>Bounding box</strong>: Depictions an object\'s location in an image, including the X and Y coordinates of the entity and its width and height, designated by a <code class=""chakra-code css-1u83yg1"">rectangle</code> object.</li><li class=""css-cvpopp""><strong>Computer vision</strong>: The area of artificial intelligence focused on processing and understanding visual inputs through complex machine learning algorithms.</li><li class=""css-cvpopp""><strong>Face detection</strong>: Using computer vision-related machine learning models to find and analyze faces in images. Faces identified within an image are analyzed to detect numerous attributes, such as age, gender, hair color, and various other traits.</li><li class=""css-cvpopp""><strong>Facial recognition</strong>: The processing of using AI to compare two faces to determine if they belong to the same person or share enough similar features to be labeled as a possible match.</li><li class=""css-cvpopp""><strong>Multiclass classification</strong>: A classification type that allows a single image to contain one or more tags (e.g., the photo includes cows, horses, and pigs).</li><li class=""css-cvpopp""><strong>Multilabel classification</strong>: A classification type for which images can only be assigned one tag (e.g., an image can contain a cow, a horse, or a pig, but not simultaneously).</li><li class=""css-cvpopp""><strong>Opaque</strong>: In data science, opaque refers to data types that do not define a concrete data structure, are not easily searchable, and do not enable simple extraction of metadata or the content contained within them. Opaque documents are things like binary image or PDF files, containing vast amounts of information, but it is not discoverable using tradition search methodologies.</li><li class=""css-cvpopp""><strong>Optical character recognition (OCR)</strong>: The conversion of printed or handwritten text contained within images, such as scanned documents, into a digital representation of the text characters.</li><li class=""css-cvpopp""><strong>Precision</strong>: Indicates the percentage of predicted classifications that were correct. This number indicates how likely your classification model is to be right if it predicts a tag.</li><li class=""css-cvpopp""><strong>Probability threshold</strong>: The minimum probability score for a prediction to be valid when calculating precision and recall.</li><li class=""css-cvpopp""><strong>Recall</strong>: The percentage of image tags that your model should have correctly predicted that were actually tagged.</li><li class=""css-cvpopp""><strong>Semantic segmentation</strong>: The process of using complicated machine learning models to examine an image pixel-by-pixel and then cluster together all the parts or pixels that belong to the same object. Semantic segmentation classifies every pixel of an image, using that information to provide a very low-level understanding of the information in an image and feed other aspects of computer vision.</li><li class=""css-cvpopp""><strong>Supervised learning</strong>: An approach to machine learning which trains models by providing example inputs, such as images, and pairing those with expected outputs, such as classification tags. Also referred to as <strong>supervised machine learning</strong>.</li><li class=""css-cvpopp""><strong>Unsupervised learning</strong>: A machine learning pattern in which algorithms analyze unlabeled data to learn patterns and discover information.</li></ul></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.1  Introduction and Lesson Overview,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Lesson Overview</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Summary</h3>\n<p class=""chakra-text css-o3oz8b"">Natural Language is another way to communicate with computers. Personal assistants in your mobile phone and home appliances are good examples. Natural Language Processing (NLP) research started around the 1950s.</p>\n<p class=""chakra-text css-o3oz8b"">Our focus in this course will be on the cognition part of NLP that relates to comprehension of natural language.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">We will look into <strong>Azure Cognitive Services</strong> and drill down to its capabilities such as\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Text Analytics</li><li class=""css-cvpopp"">Text Translation</li><li class=""css-cvpopp"">Speech</li><li class=""css-cvpopp"">Language Understanding</li></ul>\n</li><li class=""css-cvpopp"">At the end of the lesson, you will implement NLP functionalities into your applications using Azure Cognitive Services.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Our focus for this lesson will be Azure Cognitive Services, which has an extensive list of capabilities  implementing various natural language features that we can use in our solutions.</p>\n<p class=""chakra-text css-o3oz8b"">At the end of the lesson, you will be able to use Azure Cognitive Services in your solution to:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">detect the language of documents and text</li><li class=""css-cvpopp"">extract key phrases and entities from text</li><li class=""css-cvpopp"">understand the sentiment carried out in a text</li><li class=""css-cvpopp"">translate text and audio to a set of languages</li><li class=""css-cvpopp"">extract intent from text</li><li class=""css-cvpopp"">recognize speech and transcribe it</li><li class=""css-cvpopp"">synthesize speech and get your applications talking</li></ul>\n<p class=""chakra-text css-o3oz8b"">We will look into implementation details and how these services can be used in the solutions you build.</p></div>']",[],https://www.youtube.com/embed/K5w3AB-VWqw
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.2  Core Workloads in NLP,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">Some critical core NLP workloads are:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Key phrase extraction</li><li class=""css-cvpopp"">Entity recognition</li><li class=""css-cvpopp"">Sentiment analysis</li><li class=""css-cvpopp"">Language modeling</li><li class=""css-cvpopp"">Speech recognition and synthesis</li><li class=""css-cvpopp"">Language translation</li></ul>\n<h3 class=""chakra-heading css-k57syw"">Matching Azure Cognitive Services</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Azure Text Analytics</strong> can be used for key phrase extraction, entity detection, and sentiment analysis.</li><li class=""css-cvpopp""><strong>Azure Speech</strong> can be used for speech recognition and synthesize speech, and translating spoken languages.</li><li class=""css-cvpopp""><strong>Translator Text service</strong> can be used for Text Language Translation.</li><li class=""css-cvpopp""><strong>Language Understanding (LUIS)</strong> can be used to train a language model that can understand spoken or text-based commands.</li></ul>\n<p class=""chakra-text css-o3oz8b"">In addition to provisioning the above services individually, you can provision an <strong>Azure Cognitive service</strong> resource if you are planning to use these services in combination. This is the path we will take during our lesson exercises.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">New terms:</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Intent Extraction</strong>: Analyzing text to understand that would connect the text with a predefined action. Intent extraction is a popular approach in chatbot development.</li><li class=""css-cvpopp""><strong>Entity</strong>: An item of a particular type or a category, for example, a location or organization.</li><li class=""css-cvpopp""><strong>Speech synthesis</strong>: the artificial production of human speech</li><li class=""css-cvpopp""><strong>Key Phrase Extraction</strong>: Identifying terms in a text that best describes the subject of the document.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional resources</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">To consider all the options of handling NLP workloads including topics we don\'t cover here, read this article about <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/natural-language-processing"">choosing a natural language processing technology in Azure<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li><li class=""css-cvpopp"">Interested in R? <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/r-developers-guide"">R developer\'s guide to Azure<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> can be your starting point.</li></ul></div>']",[],https://www.youtube.com/embed/HL5xQ6XbdOo
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.3  Quizzes: Core Workloads in NLP,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s strengthen some of the ideas we just covered on core workloads and NLP!</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Below are some of the Azure services we just discussed. Can you match each of them with the correct description?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">LUIS</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Text Analytics</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Text Analytics</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Speech</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Key Phrase Extraction</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech Synthesize</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Sentiment Analysis</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Understanding spoken language</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following is considered a core NLP workload?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Downsizing images</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Sentiment Analysis</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Finding Objects In Videos</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Consider you have a list of tweets. What core workloads would apply to <strong>extract</strong> information from your data with the help of NLP? (Select all that apply)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Entity Recognition</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Key Phrase Extraction</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Sentiment Analysis</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Text Translation</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.4  Preparing Your Workspace,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">On this page, we will help you set up the Azure environment for the lesson. More specifically, we will walk you through how to create:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">an Azure Cognitive Service account</li><li class=""css-cvpopp"">an Azure Machine Learning Environment with a compute node</li><li class=""css-cvpopp"">a Jupyter notebook</li></ul>\n<p class=""chakra-text css-o3oz8b"">At the end of the page, there is a checklist for you to check if you are ready to continue to the next section. If you feel confident to set up your environment independently, feel free to skip the tutorial and jump right to the checklist.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 1 - Creating Your Azure Cognitive Services Account</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Log in to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com"">Azure Portal <span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>with your credentials, and navigate to you resource group.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select <strong>Add (1)</strong> to navigate to Azure Marketplace. Search for <strong>Cognitive Service</strong> in the marketplace.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the <strong>Cognitive Services (1)</strong> listing item, select <strong>Create (2)</strong></p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Make sure the correct <strong>Azure Subscription (1)</strong> and <strong>Azure Resource Group (1)</strong> are selected.</li><li class=""css-cvpopp"">Select a <strong>region (2)</strong> of your choice for the deployment target.</li><li class=""css-cvpopp"">Give your Cognitive Service a <strong>name (3).</strong> The name has to be <em class=""chakra-text css-o3oz8b"">globally unique</em> across Azure and will have no visibility outside your code.</li><li class=""css-cvpopp"">Select <strong>Standard S0 (4)</strong> for your pricing tier.</li><li class=""css-cvpopp"">Read the instructions and <strong>approve the checkboxes (5)</strong> if appropriate.</li><li class=""css-cvpopp"">Finally, select <strong>Review + create (6)</strong> to continue.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once you pass the <strong>validation (1)</strong> select <strong>Create (2)</strong> to start the provisioning of your service.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">When you receive <strong>Your deployment is complete</strong> message <strong>(1)</strong> select <strong>Go to resource (2)</strong> to navigate to your newly created Azure Cognitive Services account.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Now that you have your Cognitive Services account, head over to <strong>Keys and Endpoint (1)</strong> to grab access keys and some more information we will need during the exercises. Select <strong>Show Keys (2)</strong> to get the keys to appear. Open a text editor such as Notepad to note the values you will copy from this screen. Copy <strong>Key 1 (3)</strong>, <strong>Endpoint (4)</strong>, and <strong>Location (5)</strong> and past all values one by one to your editor of choice for future use.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 2 - Creating Your Azure Machine Learning Environment</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Log in to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com"">Azure Portal <span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>with your credentials, and navigate to you resource group.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select <strong>Add (1)</strong> to navigate to Azure Marketplace. Search for <strong>Machine Learning</strong> in the marketplace.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select <strong>Create (2)</strong> for the Machine Learning <strong>(1)</strong> listing item.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Make sure the correct <strong>Azure Subscription (1)</strong> and <strong>Azure Resource Group (1)</strong> are selected.</li><li class=""css-cvpopp"">Select a <strong>region (3)</strong> of your choice for the deployment target.</li><li class=""css-cvpopp"">Give your Azure Machine Learning workspace a <strong>name (2).</strong></li><li class=""css-cvpopp"">Finally, select <strong>Review + create (4)</strong> to continue.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once you pass the <strong>validation (1)</strong> select <strong>Create (2)</strong> to start the provisioning of your service.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">When you receive <strong>Your deployment is complete</strong> message <strong>(1)</strong> select <strong>Go to resource (2)</strong> to navigate to your newly created Azure Machine Learning service.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Now that you are on your Azure Machine Learning service page, you can select <strong>Studio web URL (1)</strong> to navigate to your Azure Machine Learning Studio.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Within the Azure Machine Learning Studio, switch to the <strong>Notebooks (1)</strong> section and select <strong>Terminal (2)</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You will receive an error message that says <strong>The terminal is not connected to a running compute (1)</strong>. This is because we did not create a compute node <strong>(2)</strong> for our workspace. Now select the <strong>+ (3)</strong> button to create one.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select <strong>Standard_DS2_v2 (1)</strong>, or any appropriate virtual machine size and select <strong>Next (2)</strong> to continue.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Give your virtual machine a name <strong>(1)</strong> and select <strong>Create (2)</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">When you are back in Notebooks, you will notice that the compute node is now being created <strong>(1)(3)</strong>. Wait until the progress indicator <strong>(2)</strong> is gone and you are logged in to the terminal.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 3 - Creating a Jupyter Notebook in Azure Machine Learning Workspace</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">During the upcoming lessons, you can use a Jupyter environment to run the code snippets shared with you. In this task, we will see how to create a single notebook in Azure Machine Learning Workspace. You can create multiple notebooks, change their names and manage notebooks in folders in the Azure ML workspace user interface.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Ensure you have the <strong>Notebooks (1)</strong> section selected and the <strong>Files (2)</strong> section open.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select the location <strong>(1)</strong> where you want to create the file. Select the plus <strong>""+""</strong> icon from the menu <strong>(2)</strong> and click <strong>Create new file (3).</strong></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select <strong>Notebook (1)</strong> as your file type. Type in a file name of your choice <strong>(2)</strong> and select <strong>Create (3)</strong> to create the notebook.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once your file is created, the web interface will open it up for you. To get a better view of the notebook, you might <strong>hide (1) the files tab</strong>. If you see an authentication warning, select <strong>Authenticate (2)</strong> to authenticate with the Azure portal. This will require you to enter your Azure Subscription credentials to give your notebook access to your compute node.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the notebook editor, you can start writing code into a cell <strong>(1)</strong>, run the code in a cell by clicking the play button on the right side of the cell <strong>(2)</strong>, and see the result underneath the cell <strong>(3)</strong>. You can write multiple lines of code in a single cell if needed.</p>\n<p class=""chakra-text css-o3oz8b"">As you can see, in our case, we are getting an error back because we did not write valid Python code.</p>\n<p class=""chakra-text css-o3oz8b"">You can add more cells into a notebook by clicking the ""+"" button underneath a cell <strong>(4)</strong> and choose to add a new <strong>Code Cell or a Markdown Cell (5)</strong> as needed. You can have as many cells as you want.</p>\n<p class=""chakra-text css-o3oz8b"">Finally, make sure you save your notebooks by selecting the toolbar\'s save button <strong>(6)</strong> before leaving the environment.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s check if you have everything you need to start with your lab.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Cognitive Service account</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Machine Learning Workspace</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Compute node for Azure Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Jupyter Notebook</p></div>']","['https://video.udacity-data.com/topher/2021/February/6033826c_add-new-resource/add-new-resource.png', 'https://video.udacity-data.com/topher/2021/February/60338352_select-azure-cognitice-service-marketplace/select-azure-cognitice-service-marketplace.png', 'https://video.udacity-data.com/topher/2021/February/603383d3_create-new-cognitive-service/create-new-cognitive-service.png', 'https://video.udacity-data.com/topher/2021/February/60338538_create-new-cognitive-service-validation/create-new-cognitive-service-validation.png', 'https://video.udacity-data.com/topher/2021/February/603385c8_create-new-cognitive-service-done/create-new-cognitive-service-done.png', 'https://video.udacity-data.com/topher/2021/February/60338684_cognitive-services-keys/cognitive-services-keys.png', 'https://video.udacity-data.com/topher/2021/February/603388fe_add-new-resource/add-new-resource.png', 'https://video.udacity-data.com/topher/2021/February/60338947_create-machine-learning/create-machine-learning.png', 'https://video.udacity-data.com/topher/2021/February/603389d4_create-machine-learning-settings/create-machine-learning-settings.png', 'https://video.udacity-data.com/topher/2021/February/60338a5f_create-new-azureml-validation/create-new-azureml-validation.png', 'https://video.udacity-data.com/topher/2021/February/60338bc0_create-new-azureml-done/create-new-azureml-done.png', 'https://video.udacity-data.com/topher/2021/February/60338c1c_navigate-azureml-studio/navigate-azureml-studio.png', 'https://video.udacity-data.com/topher/2021/February/60338dd7_azureml-goto-terminal/azureml-goto-terminal.png', 'https://video.udacity-data.com/topher/2021/February/60338e46_azureml-no-compute-found/azureml-no-compute-found.png', 'https://video.udacity-data.com/topher/2021/February/60338ee2_azureml-select-vm-size/azureml-select-vm-size.png', 'https://video.udacity-data.com/topher/2021/February/60338f60_azureml-name-your-vm/azureml-name-your-vm.png', 'https://video.udacity-data.com/topher/2021/February/60338fad_azureml-vm-creating/azureml-vm-creating.png', 'https://video.udacity-data.com/topher/2021/March/603cf583_azureml-notebooks-section/azureml-notebooks-section.png', 'https://video.udacity-data.com/topher/2021/March/603cf61b_azureml-create-new-file/azureml-create-new-file.png', 'https://video.udacity-data.com/topher/2021/March/603cf6ae_azureml-create-notebook/azureml-create-notebook.png', 'https://video.udacity-data.com/topher/2021/March/603cf75a_azure-ml-notebook-authenticate/azure-ml-notebook-authenticate.png', 'https://video.udacity-data.com/topher/2021/March/603cf860_azureml-notebook-walkthrough/azureml-notebook-walkthrough.png']",
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.5  Analyzing Text - Part 1,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">For text analytics, our focus will be on <strong>language detection, sentiment analysis, key phrase detection, and entity recognition.</strong></p>\n<p class=""chakra-text css-o3oz8b"">On this page of part 1, we will go over the <strong>language detection and sentiment analysis.</strong> On the next page of part 2, we will talk about <strong>key phrase detection and entity recognition.</strong></p>\n<p class=""chakra-text css-o3oz8b"">Let\'s go one by one and see how we can use Azure Cognitive Services to enable these functionalities in our applications.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s check if you have everything you need to start with your lesson. If you are missing any of the items make sure you go back to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://classroom.udacity.com/nanodegrees/nd099_t/parts/5ddef5bf-04d1-4b9d-94af-dac454840080/modules/3ac10164-797f-4bac-a4f1-a86466c7e9dd/lessons/1032ba0c-0152-4b45-9e4c-38bb6dd2d670/concepts/42bf8680-fc7a-4a1c-a3bc-117918655251"">Preparing Your Workspace page<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Cognitive Service account</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Machine Learning Workspace</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Compute node for Azure Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Jupyter Notebook</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Preparation</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a notebook in your Azure Machine Learning workspace and name it <code class=""chakra-code css-1u83yg1"">l5-analyze-text.ipynb</code>. You can use your notebook to run the code snippets shared below.</p>\n<p class=""chakra-text css-o3oz8b"">In order to use Azure Cognitive Services APIs from Python, you have to install the Phyton SDK. The code below will make sure you have the required SDK installed.</p>\n<p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">pip install azure-ai-textanalytics</code></p>\n<p class=""chakra-text css-o3oz8b"">Access to Azure Cognitive Services requires three variables that can be accessed through the Azure Portal. During the workspace preparation task, you have copied these values into a text editor of your choice. Now it is time to use them. The values are:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Service Access Key</li><li class=""css-cvpopp"">Service Access Endoint</li><li class=""css-cvpopp"">Service Region</li></ul>\n<p class=""chakra-text css-o3oz8b"">Only two of these variables are needed by the Language Detection APIs. To use these values, we pass those to variables to be used in future steps. <code class=""chakra-code css-1u83yg1"">&lt;YourKey&gt;</code> and <code class=""chakra-code css-1u83yg1"">&lt;YourEndpoint&gt;</code> are placeholders for the actual values to be captured from the Azure Portal.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>cognitive_key </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'&lt;YourKey&gt;\'</span><span>\n</span><span>cognitive_endpoint </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'&lt;YourEndpoint&gt;\'</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">A <code class=""chakra-code css-1u83yg1"">TextAnalyticsClient</code> class wraps all access to the APIs. To create one, we will need an <code class=""chakra-code css-1u83yg1"">AzureKeyCredential</code> object. Both classes can be used quickly after importing the two namespaces shown below.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> azure</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>core</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>credentials </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> AzureKeyCredential\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> azure</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>ai</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>textanalytics </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> TextAnalyticsClient</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Next is to get the <code class=""chakra-code css-1u83yg1"">AzureKeyCredential</code> object ready. This object only needs the access key we previously stored in a variable. The code snippet below creates the object by passing the access key and returning the credential object back to a <code class=""chakra-code css-1u83yg1"">credential</code> variable to be used later.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>credential </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> AzureKeyCredential</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>cognitive_key</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Finally, it is time to create the <code class=""chakra-code css-1u83yg1"">TextAnalyticsClient</code> object that will help us access the APIs directly. The code below is creating the <code class=""chakra-code css-1u83yg1"">TextAnalyticsClient</code> object by passing two parameters. The first <code class=""chakra-code css-1u83yg1"">endpoint</code> parameter is the <code class=""chakra-code css-1u83yg1"">cognitive_endpoint</code> variable we previously defined by passing the endpoint value given on the Azure Portal. The second <code class=""chakra-code css-1u83yg1"">credential</code> parameter is the <code class=""chakra-code css-1u83yg1"">AzureKeyCredential</code> object we have prepared in the previous step.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>text_analytics_client </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> TextAnalyticsClient</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>cognitive_endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> credential</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>credential</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Now, we have a <code class=""chakra-code css-1u83yg1"">TextAnalyticsClient</code> instance named <code class=""chakra-code css-1u83yg1"">text_analytics_client</code> that we can use to call different APIs to access Text Analytics capabilities of Azure Cognitive Services.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s see if you have all the objects ready to keep coding.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Did you install the SDK?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Defined the variables to hold the access key and endpoint</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created the credential object holding the access key</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created the <code class=""chakra-code css-1u83yg1"">TextAnalyticsClient</code> object with the credentials and enpoint</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Language Detection</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Language detection</strong> provides us the capability of detecting the language of a text or document. We can submit multiple documents and texts in a batch if needed. For every piece of text we submit, we will get back:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">The name of the detected language</li><li class=""css-cvpopp"">The ISO 6391 language code</li><li class=""css-cvpopp"">A confidence score indicating how confident the machine learning algorithm is about the result. This will be a value between 0 to 1. One being the highest confidence level.</li></ul>\n<p class=""chakra-text css-o3oz8b"">Here is a sample array of text assigned to a variable named <code class=""chakra-code css-1u83yg1"">documents</code>. Our goal is to submit this array for language detection. We can tell that the first text is in English, the second in French, and the third in German.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>documents </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""This is written in English.""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Ceci est écrit en Français.""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Dies ist in deutscher Sprache geschrieben.""</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">In the code example below, we are using the <code class=""chakra-code css-1u83yg1"">text_analytics_client</code> we previously created to call the <code class=""chakra-code css-1u83yg1"">detect_language</code> method from Azure Cognitive Services. In this case, the only parameter we need to pass is the array of documents.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>language_analysis </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> text_analytics_client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>detect_language</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>documents</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Once we get the <code class=""chakra-code css-1u83yg1"">language_analysis</code> object back, we filter out those that do not have an error. Assign the successful documents to a <code class=""chakra-code css-1u83yg1"">result</code> variable.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span>doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> language_analysis </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">if</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">not</span><span> doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>is_error</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span></code></div></div></pre>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">As an alternative documents having an error can be further analysed to investigate the root cause of the error by looking into the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azuresdkdocs.blob.core.windows.net/$web/python/azure-ai-textanalytics/latest/azure.ai.textanalytics.html#azure.ai.textanalytics.TextAnalyticsError"">DocumentError<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> object.</p>\n</blockquote>\n<p class=""chakra-text css-o3oz8b"">Finally, we loop through the results we have received and access the language name from <code class=""chakra-code css-1u83yg1"">primary_language.name</code> property, the language code from <code class=""chakra-code css-1u83yg1"">primary_language.iso6391_name</code> property, and the confidence score from <code class=""chakra-code css-1u83yg1"">primary_language.confidence_score</code> property.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Language detected: {}""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span class=""token"" style=""color: rgb(0, 121, 162);"">format</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>primary_language</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>name</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""ISO6391 name: {}""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span class=""token"" style=""color: rgb(0, 121, 162);"">format</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>primary_language</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>iso6391_name</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Confidence score: {}\\n""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span class=""token"" style=""color: rgb(0, 121, 162);"">format</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>primary_language</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>confidence_score</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the screenshot above, you can see the result of the execution of our code. Look at those confidence scores!</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What number represents the highest confidence score?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">100</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">1</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">10</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">100%</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Sentiment Analysis</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Sentiment Analysis</strong> helps us evaluate text and extract a positive or negative sentiment out of it. This is especially popular analyzing social media, reviews, and customer feedback.</p>\n<p class=""chakra-text css-o3oz8b"">Azure Cognitive Services comes with a pre-built machine learning classification model that can give sentiment scores between 0 to 1. One is considered <em class=""chakra-text css-o3oz8b"">positive</em>, and zero is considered <em class=""chakra-text css-o3oz8b"">negative</em>. A 0.5 score is interpreted as <em class=""chakra-text css-o3oz8b"">neutral</em>. Separate confidence scores about each sentiment can be found in the range of 0 to 1 as well.</p>\n<p class=""chakra-text css-o3oz8b"">Here is a sample array of text assigned to a variable named <code class=""chakra-code css-1u83yg1"">documents</code>. Our goal is to submit this array for sentiment analysis.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>documents </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""I\'m in love with Azure Text Analytics""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""AI is going to eat our jobs. We will all end up homeless.""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Machine Learning skills are always valuable.""</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">In the code example below, we are using the <code class=""chakra-code css-1u83yg1"">text_analytics_client</code> we previously created to call the <code class=""chakra-code css-1u83yg1"">analyze_sentiment</code> method from Azure Cognitive Services. The first parameter we need to pass is the array of documents. The second parameter called <code class=""chakra-code css-1u83yg1"">language</code> defines the document\'s language submitted as part of the <code class=""chakra-code css-1u83yg1"">documents</code> array.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>response </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> text_analytics_client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>analyze_sentiment</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>documents</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> language</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""en""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Once we get the <code class=""chakra-code css-1u83yg1"">analyze_sentiment</code> object back, we filter out those that do not have an error. Assign the successful documents to a <code class=""chakra-code css-1u83yg1"">result</code> variable.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span>doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> response </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">if</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">not</span><span> doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>is_error</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Finally, we loop through the results we have received and access the overall sentiment value from <code class=""chakra-code css-1u83yg1"">doc.sentiment</code> property. Additionally, we access <code class=""chakra-code css-1u83yg1"">confidence_scores</code> for every sentiment type, including <code class=""chakra-code css-1u83yg1"">positive</code>, <code class=""chakra-code css-1u83yg1"">neutral</code> and <code class=""chakra-code css-1u83yg1"">negative</code>.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Overall sentiment: {}""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span class=""token"" style=""color: rgb(0, 121, 162);"">format</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>sentiment</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Scores: positive={}; neutral={}; negative={} \\n""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span class=""token"" style=""color: rgb(0, 121, 162);"">format</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>\n</span><span>        doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>confidence_scores</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>positive</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>        doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>confidence_scores</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>neutral</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>        doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>confidence_scores</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>negative</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the screenshot above, you can see that in the case of the second sentence, the algorithm is not fully confident about the statements negativity. However, the final sentiment is negative, thanks to a 0.65 confidence score on negative compared to a 0.33 confidence score on neutral.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Here is what we covered so far:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">how to download and use the Azure Cognitive Services Text Analytics Phyton SDK</li><li class=""css-cvpopp"">a round of Language Detection</li><li class=""css-cvpopp"">a round of Sentiment Analysis</li></ul>\n<p class=""chakra-text css-o3oz8b"">On the next page, we will learn about key phrase extraction and entity recognition.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">New terms:</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>SDK:</strong> Software Development Kit. In our case. We used Python SDKs for Azure Cognitive Services to access REST APIs\' functionality without worrying about the raw HTTP implementations.</li><li class=""css-cvpopp""><strong>Text Analytics:</strong> The umbrella service in Azure that provides NLP functionalities for text processing. Text Analytics is part of Azure Cognitive Services. Text Analytics can be used stand-alone or used as part of an Azure Cognitive Services resource in the Azure Portal.</li><li class=""css-cvpopp""><strong>Confidence Score</strong>: The confidence score can have a different meaning for various AI implementations. The overall idea is about how confident the algorithm is about its result or decision.</li><li class=""css-cvpopp""><strong>Service Access Endpoint:</strong> The HTTP endpoint where an SDK can access the cognitive service.</li><li class=""css-cvpopp""><strong>Service Region:</strong> The deployment location of the services in terms of Azure Data Center locations. Some SDKs use the region instead of the endpoint and prefer to build their endpoint structure based on the region metadata.</li></ul></div>']","['https://video.udacity-data.com/topher/2021/March/603d1a5e_language-detection-sample-result/language-detection-sample-result.png', 'https://video.udacity-data.com/topher/2021/March/603d184b_sentiment-analysis-sample-result/sentiment-analysis-sample-result.png']",https://www.youtube.com/embed/_vqIhdWxL_E
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.6  Analyzing Text - Part 2,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">On the previous page, we discussed <strong>language detection and sentiment analysis.</strong> Let\'s continue the journey of analyzing text by looking at the rest of the functionalities: <strong>key phrase detection and entity recognition.</strong></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s check if you have everything you need to start with your lesson. If you are missing any of the items make sure you go back to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://classroom.udacity.com/nanodegrees/nd099_t/parts/5ddef5bf-04d1-4b9d-94af-dac454840080/modules/3ac10164-797f-4bac-a4f1-a86466c7e9dd/lessons/1032ba0c-0152-4b45-9e4c-38bb6dd2d670/concepts/42bf8680-fc7a-4a1c-a3bc-117918655251"">Preparing Your Workspace page<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Cognitive Service account</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Machine Learning Workspace</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Compute node for Azure Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Jupyter Notebook</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s see if you have all the objects ready to keep coding. You can view the preparation section on the previous page ""<a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://classroom.udacity.com/nanodegrees/nd099_t/parts/5ddef5bf-04d1-4b9d-94af-dac454840080/modules/3ac10164-797f-4bac-a4f1-a86466c7e9dd/lessons/1032ba0c-0152-4b45-9e4c-38bb6dd2d670/concepts/a43b30ed-3a57-46c7-8700-cb14ed319541"">Analyzing Text - Part 1<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>"" about the setup.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Did you install the SDK?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Defined the variables to hold the access key and endpoint</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created the credential object holding the access key</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created the <code class=""chakra-code css-1u83yg1"">TextAnalyticsClient</code> object with the credentials and enpoint</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Key Phrase Extraction</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Key phrase extraction</strong> looks into a text and extracts the primary talking points. Once combined with Sentiment Analysis, Key phrase extraction can help us understand what the sentiment relates to. For example, customer feedback might be detected as negative, and key phrase detection can enrich the understanding by adding more metadata, such as a product mentioned in a sentence.</p>\n<p class=""chakra-text css-o3oz8b"">Here is a sample array of text assigned to a variable named <code class=""chakra-code css-1u83yg1"">documents</code>. Our goal is to submit this array for key phrase extraction. Do you have any guesses?</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>documents </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Udacity is the perfect place to learn by practice and exercise.""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""I might eat a burger tonight.""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""The answer to everything in life is 42.""</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">In the code example below, we are using the <code class=""chakra-code css-1u83yg1"">text_analytics_client</code> we previously created to call the <code class=""chakra-code css-1u83yg1"">extract_key_phrases</code> method from Azure Cognitive Services. The first parameter we need to pass is the array of documents. The second parameter called <code class=""chakra-code css-1u83yg1"">language</code> defines the document\'s language submitted as part of the <code class=""chakra-code css-1u83yg1"">documents</code> array.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>response </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> text_analytics_client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>extract_key_phrases</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>documents</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> language</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""en""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Once we get the response back, we filter out documents that do not have an error. Assign the successful documents to a <code class=""chakra-code css-1u83yg1"">result</code> variable.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span>doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> response </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">if</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">not</span><span> doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>is_error</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Finally, we loop through the results we have received and access all the key phrases simply in the <code class=""chakra-code css-1u83yg1"">key_phrases</code> of each document.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>key_phrases</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">When you look at the screenshot above, you can see the results returned from the API. Please take a look at it, and try different sentences of your own.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">""This new headset design is not looking good in green""<br>\nWhat option below look like a good candidate for a key phrase?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">new headset design</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">good</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">green</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">headset</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Entity Recognition</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Entity recognition</strong> extracts items from text such as category names, person names, organizations, locations, quantity values, date-times, and many others. You can find a complete list of recognized entities by Azure Cognitive Services in the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/named-entity-types?tabs=general"">Supported entity categories in the Text Analytics API v3\n<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> document. Let\'s see how we can use entity recognition.</p>\n<p class=""chakra-text css-o3oz8b"">Here is a sample array of text assigned to a variable named <code class=""chakra-code css-1u83yg1"">documents</code>. Our goal is to extract entities from the documents.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>documents </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""David woke up very early.""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""When she was in Paris, Diana was a happy girl.""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""He found a job thanks to Udacity.""</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">In the code example below, we are using the <code class=""chakra-code css-1u83yg1"">text_analytics_client</code> we previously created to call the <code class=""chakra-code css-1u83yg1"">recognize_entities</code> method from Azure Cognitive Services. The first parameter we need to pass is the array of documents. The second parameter called <code class=""chakra-code css-1u83yg1"">language</code> defines the document\'s language submitted as part of the <code class=""chakra-code css-1u83yg1"">documents</code> array.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>response </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> text_analytics_client</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>recognize_entities</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>documents</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> language</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""en""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Once we get the response back, we filter out documents that do not have an error and assign the successful documents to a <code class=""chakra-code css-1u83yg1"">result</code> variable.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span>doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> response </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">if</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">not</span><span> doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>is_error</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Finally, we loop through the results and loop through the entities for each document to access <code class=""chakra-code css-1u83yg1"">entity.text</code>, <code class=""chakra-code css-1u83yg1"">entity.category</code> and <code class=""chakra-code css-1u83yg1"">entity.confidence_score</code>.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> doc </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">for</span><span> entity </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">in</span><span> doc</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>entities</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span>\n</span><span>        </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Entity: \\t""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> entity</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>text</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""\\tCategory: \\t""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> entity</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>category</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>              </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""\\tConfidence Score: \\t""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> entity</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>confidence_score</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the screenshot above, you can see that <strong>David</strong> is extracted as a <strong>Person</strong> type entity with a <strong>0.95</strong> confidence score. In the last sentence, <strong>Udacity</strong> is extracted as an <strong>Organization</strong> with a confidence score of 0.84. These are high confidence scores and good results so far.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Here is what we covered so far:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">how to download and use the Azure Cognitive Services Text Analytics Phyton SDK</li><li class=""css-cvpopp"">a round of Language Detection</li><li class=""css-cvpopp"">a round of Sentiment Analysis</li><li class=""css-cvpopp"">a round of Key Phrase Extraction</li><li class=""css-cvpopp"">a round of Entity Recognition</li></ul>\n<p class=""chakra-text css-o3oz8b"">You can combine all of the functionalities listed above in a single solution when incorporating these features into your applications. For example, to pass the text language to Sentiment Analysis, you can use Language Detection first to detect the document\'s language.</p>\n<p class=""chakra-text css-o3oz8b"">With that, you are now the master of Text Analytics. Congratulations.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Playground Demo</h2>\n<p class=""chakra-text css-o3oz8b"">Visit Microsoft\'s <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aidemos.microsoft.com/text-analytics"">Text Analytics playground<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> and try Sentiment &amp; Key Phrases and Entity Linking with your own texts.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional Resources</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Explore some more example user scenarios for the Text Analytics API <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/text-analytics-user-scenarios"">here<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li><li class=""css-cvpopp"">In case you receive errors from the SDK getting into the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-call-api?tabs=synchronous"">Text Analytics REST API Documentation<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> can help investigate issues.</li><li class=""css-cvpopp"">Opinion Mining is a feature of Sentiment Analysis, starting in the preview of version 3.1. Also known as Aspect-based Sentiment Analysis in Natural Language Processing (NLP). <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-sentiment-analysis?tabs=version-3-1#opinion-mining"">Interested? Take a look at it here.<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>']","['https://video.udacity-data.com/topher/2021/March/603d1c85_keyphrase-extraction-sample-result/keyphrase-extraction-sample-result.png', 'https://video.udacity-data.com/topher/2021/March/603d2220_entity-recognition-sample-result/entity-recognition-sample-result.png']",https://www.youtube.com/embed/W2uNIPf1nEg
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.7  Quizzes: Analyzing Text,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s get some practice with some of the text analysis concepts we just discussed! 😊</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You are given a <code class=""chakra-code css-1u83yg1"">text_analytics_client</code> object that comes from <code class=""chakra-code css-1u83yg1"">azure.ai.textanalytics</code>.</p>\n<p class=""chakra-text css-o3oz8b"">Which of the following would run a sentiment analysis for English language documents?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">text_analytics_client.analyze_sentiment(documents, language=""en"")</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">text_analytics_client.extract_sentiment(documents, language=""english"")</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">text_analytics_client.getSentiment(documents, language=""en"")</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">text_analytics_client.analyzeSentiment(documents, language.english)</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You are doing an entity recognition operation for a list of documents. What type of entities should you expect? (Select all that apply)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Person</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Location</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Organization</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Planet</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which one is <strong>NOT</strong> a sentiment you can extract from Azure Cognitive Services Sentiment Analysis?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Positive</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Negative</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Neutral</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Happy</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which one is <strong>NOT</strong> a requirement to use Azure Cognitive Services APIs?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Access Key</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Endpoint URL</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Region</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Trained Model File</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.8  Getting Prepared for the Exercise,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">On this page, we will help you set up the Azure environment for the exercises. More specifically, we will walk you through how to <em class=""chakra-text css-o3oz8b"">access Lab materials from GitHub</em>. If you did not prepare your Azure Cognitive Services and Azure Machine Learning environment previously, make sure you follow the steps described in the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://classroom.udacity.com/nanodegrees/nd099_t/parts/5ddef5bf-04d1-4b9d-94af-dac454840080/modules/3ac10164-797f-4bac-a4f1-a86466c7e9dd/lessons/1032ba0c-0152-4b45-9e4c-38bb6dd2d670/concepts/42bf8680-fc7a-4a1c-a3bc-117918655251"">""Preparing Your Workspace"" <span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>section first.</p>\n<p class=""chakra-text css-o3oz8b"">At the end of the page, there is a checklist for you to check if you are ready to work on the exercise. If you feel confident to set up your environment independently, feel free to skip the tutorial and jump right to the checklist.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Accessing Lab Materials From GitHub</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Launch a terminal window in your Azure Machine Learning Studio. You can keep using the one you have from the previous exercise.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once you are in the terminal run <code class=""chakra-code css-1u83yg1"">git clone https://github.com/udacity/AI_fundamentals.git</code> <strong>(1)</strong> to download Lab assets to your Azure ML Studio. When cloning is done select <strong>Refresh (2)</strong> to see the list of files <strong>(3)</strong> in the file browser.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s check if you have everything you need to start with your lab. Make sure you have followed the instructions in a previous section titled <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://classroom.udacity.com/nanodegrees/nd099_t/parts/5ddef5bf-04d1-4b9d-94af-dac454840080/modules/3ac10164-797f-4bac-a4f1-a86466c7e9dd/lessons/1032ba0c-0152-4b45-9e4c-38bb6dd2d670/concepts/42bf8680-fc7a-4a1c-a3bc-117918655251"">Preparing Your Workplace<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Have an Azure Cognitive Services Account</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Have an Azure Machine Learning Workspace</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Have a Compute node for Azure Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Cloned Lab Files from GitHub</p></div>']",['https://video.udacity-data.com/topher/2021/February/60339160_azureml-github-clone/azureml-github-clone.png'],
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.9  Exercise: Analyze Text With The Text Analytics Service,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You have learned what Azure Text Analytics can do for you. Now it is time to get things running. In this exercise, you will be given some sample text and asked to <em class=""chakra-text css-o3oz8b"">detect the texts\' language, analyze the sentiments they carry, extract key phrases, and recognize the entities mentioned in the texts</em>. The exercise will help your practice with Azure Text Analytics and see it in action.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s check if you have everything you need to start with your lab. If you are missing any of the items make sure you go back to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://classroom.udacity.com/nanodegrees/nd099_t/parts/5ddef5bf-04d1-4b9d-94af-dac454840080/modules/3ac10164-797f-4bac-a4f1-a86466c7e9dd/lessons/1032ba0c-0152-4b45-9e4c-38bb6dd2d670/concepts/8ce0f94a-9084-4860-83d9-e77a95de0979"">Getting Prepared page<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in this lesson and complete the tasks.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Cognitive Service account</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Machine Learning Workspace</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Compute node for Azure Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Cloned Lab Files from GitHub</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Working on the Lab</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Log in to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com"">Azure Portal <span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>with your credentials, and navigate to your Azure ML Studio. Make sure you have your access key and endpoint from the <strong>Keys and Endpoints</strong> section of your Azure Cognitive Service. You will need those during the exercise. Open <strong>Notebooks (1)</strong> and launch <code class=""chakra-code css-1u83yg1"">textanalytics.ipynb</code> <strong>(3)</strong> under <strong>lesson-5 (2)</strong> and follow instructions in the Jupyter notebook.</p></div>']",['https://video.udacity-data.com/topher/2021/February/60339273_azureml-textanalytics/azureml-textanalytics.png'],
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.10  Solution: Analyze Text With The Text Analytics Service,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 1 - Language Detection and Sentiment Analysis</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 2 - Key Phrase Extraction and Entity Recognition</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Good work! Now, open the <code class=""chakra-code css-1u83yg1"">textanalytics_solution.ipynb</code> <strong>(3)</strong> notebook to see our implementation. You can access the file in the <strong>Notebooks (1)</strong> section in your Azure ML Studio under <strong>lesson-5 (2)</strong> folder. If you are still holding your Azure Cognitive Service Access Key and Endpoint information, fill those in and run the notebook cells to see those in action.</p></div>']",['https://video.udacity-data.com/topher/2021/February/60339348_azureml-textanalytics-solution/azureml-textanalytics-solution.png'],https://www.youtube.com/embed/Hp4Oeqmwew8
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.11  Extracting Intent,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">At the end of this lesson, you will implement a language understanding model to extract intent from spoken language.</p>\n<p class=""chakra-text css-o3oz8b"">Microsoft\'s <strong>Language Understanding Intelligent Service (LUIS)</strong> can be used to train a language model that can understand spoken or text-based commands. For LUIS there are three different aspects of language understanding:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Intents: An intent represents the action the user wants to execute.</li><li class=""css-cvpopp"">Utterances: An utterance is a single input from the user that has to be evaluated.</li><li class=""css-cvpopp"">Entities: An entity represents a key phrase inside the utterance that we want to extract in relation to the intent.</li></ul>\n<p class=""chakra-text css-o3oz8b"">Here is an example: <em class=""chakra-text css-o3oz8b"">Could you help me book two flight tickets to Rome?</em>\nIn this case, this utterance\'s intent can be represented with an action called <strong>BookFlight</strong>. <strong>Rome</strong> can be extracted as an entity in the category of a <strong>Location</strong>. Finally, <strong>two</strong> is another entity that represents the quantity.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 1 - Creating Your LUIS Services Account</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Our first task is to create a <strong>LUIS</strong> service resource to access the capabilities of LUIS from Azure.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Log in to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com"">Azure Portal <span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>with your credentials, and navigate to your resource group. Add an Azure marketplace by selecting <strong>+Add.</strong></li><li class=""css-cvpopp"">Search for <strong>LUIS</strong> and select <strong>Create &gt; Language Understanding.</strong> This will start the process of creating a LUIS service resource.</li><li class=""css-cvpopp"">Fill in the information\n<ol role=""list"" class=""css-124kmyc""><li class=""css-cvpopp"">Select your subscription and resource group</li><li class=""css-cvpopp"">Give a unique name to your service</li><li class=""css-cvpopp"">Select an <strong>Authoring</strong> and <strong>Prediction</strong> region. The authoring resource is for training to create, edit, train, and publish. The prediction resource is to send user\'s text and receive a prediction about intent and entities.</li><li class=""css-cvpopp"">Select the <strong>Free F0</strong> Tier for both services</li><li class=""css-cvpopp"">Select <strong>Review + create</strong> to continue</li></ol>\n</li><li class=""css-cvpopp"">Select <strong>Create</strong> to start the provisioning for the service.</li><li class=""css-cvpopp"">Once you get the message ""<em class=""chakra-text css-o3oz8b"">Your deployment is complete</em>"", select <strong>Go to resource</strong> to navigate to the LUIS resource page.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Now that you have your LUIS service, head over to <strong>Keys and Endpoint (1)</strong> to grab access keys and some more information you will need accessing the service. Select <strong>Show Keys (2)</strong> to get the keys to appear. Open a text editor such as Notepad to note the values you will copy from this screen. Copy <strong>Key 1 (3)</strong>, <strong>Endpoint (4)</strong>, and <strong>Location (5)</strong> and past all values one by one to your editor of choice for future use.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What is the difference between the authoring and prediction resources in LUIS?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">They are the same</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Prediction is about predicting confidence scores.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Authoring is all you need. No need for prediction.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The authoring is for training and prediction to predict intent and entities.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 2 - Creating Your First LUIS Application</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select one of the following URLs, and open it in a new browser window.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">North America: <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.luis.ai/"">https://www.luis.ai/<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp"">Europe: <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://eu.luis.ai/"">https://eu.luis.ai/<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp"">Australia: <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://au.luis.ai/"">https://au.luis.ai/<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul>\n<p class=""chakra-text css-o3oz8b"">There are three LUIS websites, based on region. Make sure you select the one that matches the region you picked for your authoring service. In our case, it is Europe.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">If it\'s your first time using LUIS, you may need to log in and select or create an authoring resource. Follow the instruction below to set it up.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select <strong>Login / Sign up</strong> to continue. When asked make sure you enter the same account credentials you used to access the Azure Portal.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select your Azure Subscription <strong>(1)</strong> and click <strong>Select or create an authoring resource (2)</strong></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Select your Authoring resource <strong>(1)</strong> and click <strong>Done (2)</strong></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">When you are ready, follow the steps to creat a LUIS application.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>+New app</strong> to create a new LUIS application. You can have multiple logical applications using the same LUIS service resource.</li><li class=""css-cvpopp"">Fill the information\n<ol role=""list"" class=""css-124kmyc""><li class=""css-cvpopp"">Give your application a name</li><li class=""css-cvpopp"">Select the culture your application will use. This is the culture/<strong>language</strong> your application will understand through the use of LUIS. In our case. it is English.</li><li class=""css-cvpopp"">Select the <strong>Prediction Resource</strong> you previously created.</li><li class=""css-cvpopp"">Select <strong>Done</strong> to continue.</li></ol>\n</li></ol>\n<p class=""chakra-text css-o3oz8b"">Once you are done with the setup, you will get a beautiful wizard in the portal introducing you to LUIS concepts. Please take a look at it, read the sections, and dismiss the wizard once you are done.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">How does selecting a culture for an application helps?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">It helps LUIS understanding cultural idioms in text.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">It helps LUIS run matching NLP models for the language chosen.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">It helps LUIS respond to questions in the matching culture.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Culture is about the client application and has nothing to do with LUIS.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 3 - Extracting Intent</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Our first goal will be to understand the intent of a text, or message.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Switch to the <strong>Intents</strong> tab and select <strong>+Create</strong> to create a new intent</li><li class=""css-cvpopp"">Write <code class=""chakra-code css-1u83yg1"">BookFlight</code> as the intent name and select <strong>Done</strong> to continue.</li><li class=""css-cvpopp"">Write <code class=""chakra-code css-1u83yg1"">Can you book three flight tickets to Paris?</code> as your first utterance and hit Enter from your keyboard. For now, we will go with a single utterance. More utterance will definitely help LUIS find the matching intent when you have various intents.</li><li class=""css-cvpopp"">Select <strong>Train</strong> to start training LUIS with our utterance.</li></ol>\n<p class=""chakra-text css-o3oz8b"">Once training is complete, we can click <strong>Test</strong> to test it out.</p>\n<p class=""chakra-text css-o3oz8b"">In the test window, write <code class=""chakra-code css-1u83yg1"">could you help me book two flight tickets to Rome?</code> to see if LUIS will catch the right intent.</p>\n<p class=""chakra-text css-o3oz8b"">It looks like the <code class=""chakra-code css-1u83yg1"">BookFlight</code> intent is matched with a <strong>0.779</strong> confidence. Perfect. Once you are done testing different utterances make sure you close the test panel.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">How do more utterances help LUIS?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">It is used to train the model with more data and affects the confidence level when matching intents.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Utterances are used when LUIS responds to questions.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Utterances are only for testing.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">More utterances are never helpful.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 4 - Extracting Entities</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Notice that we have an intent called <strong>None</strong>. This is a fallback intent for LUIS in case it can not detect an intent for the utterance.</p>\n<p class=""chakra-text css-o3oz8b"">Now that we have successfully run our first intent extraction let\'s see how we can extract entities from utterances.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Select <strong>BookFlight</strong> from the list.</li></ol>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">On the BookFlight intent page, you will see our single utterance listed. While you are there, you can take a look at the confidence score for the utterance.</p>\n</blockquote>\n<ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">In this utterance, hover your mouse over Paris and click to select it. From the context menu that pops up, select <strong>Open prebuilt entity...</strong></li></ol>\n<blockquote class=""css-6jmydm"">\n<p class=""chakra-text css-o3oz8b"">Prebuilt entities are part of prebuilt models in LUIS for quickly adding common, conversational user scenarios. It is a fast way to add abilities to your applications without building the models yourself.</p>\n</blockquote>\n<ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">From the list of built-in entities, look for <strong>geographyV2</strong>. This prebuilt entity will help us detect location entities in utterances.</li><li class=""css-cvpopp"">Select <strong>Done</strong> to continue.</li><li class=""css-cvpopp"">Train our LUIS model again by selecting <strong>Train</strong></li><li class=""css-cvpopp"">Select <strong>Test</strong> when training is complete</li></ol>\n<p class=""chakra-text css-o3oz8b"">In the test window, write <code class=""chakra-code css-1u83yg1"">Get me seven flight tickets to Tokyo?</code> for a new round of tests. Once the result is in, select <strong>Inspect</strong> to open an additional tab that has more details about the LUIS result. Notice that <strong>Tokyo</strong> is captured as a geography entity.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">There is a <strong>number</strong> prebuilt entity as well. Considering the BookFlight intent, we discussed above <code class=""chakra-code css-1u83yg1"">Get me seven flight tickets to Tokyo?</code>, how could that help us?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">We can use it to count the number of cities in the utterance.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">We can use it to report the confidence number.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">We can use it to extract the number of tickets the user wants to purchase.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">There is no use for it.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Prebuilt Domains</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Previously, we looked into an example of a prebuilt entity type. Prebuilt entity models and types are beneficial for getting things started without building your model. <strong>Prebuilt domains</strong> provide built-in capabilities to enhance around specific business domains.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You can navigate to <strong>Prebuilt Domains (1)</strong> to see a list of domains available, ready to go with LUIS. These models include their specific list of entities and intents you can use on your applications. If you want to add a prebuilt domain to your application, you can select <strong>Add domain (2)</strong> for the matching item in the list.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Entity Types</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">We saw a prebuilt entity extract the location information from an utterance. You can build your custom entities as well. There are four types of entities you can build in LUIS.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Machine Learned Entity:</strong> These are the top matching entities to train LUIS to extract entities from utterances. Machine-learning entities provide the broadest range of data extraction options.</li><li class=""css-cvpopp""><strong>List Entity:</strong> If you have a fixed list of words with their synonyms you would like to extract from utterances, this is the right entity type.</li><li class=""css-cvpopp""><strong>Regular Expression Entity</strong>: This is an entity type that does regex matching to extract text from utterances. Take a look at the Additional Resources section if you are interested to learn more about Regular Expressions.</li><li class=""css-cvpopp""><strong>Pattern.any Entity:</strong> This type of entity defines a template with placeholders for entities to be extracted. For example: In a template defined as <code class=""chakra-code css-1u83yg1"">What does {WordEntity} mean in {LanguageEntity}[?]</code> The values between curly braces will be extracted as separate entities, and the question mark will be ignored.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What is <strong>NOT</strong> a built-in capability of LUIS?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Prebuilt Domains</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Prebuilt Entities</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Prebuilt Agents</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">None of the above.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Now you know how to extract intents and entities from utterances in LUIS. Not just that, you know that LUIS has built-in domains that can help you get up to speed pretty fast. Moreover, if you are looking to customize and enhance LUIS\'s built-in models, you have four different types of entities we discussed that could be used to build more functionality.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Playground Demo</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Visit <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.luis.ai/"">LUIS.ai<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> to run some of the sample language understanding demos and see what LUIS returns as a result.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">New terms:</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Intents</strong>: An intent represents the action the user wants to execute</li><li class=""css-cvpopp""><strong>Utterances</strong>: An utterance is a single input from the user that has to be evaluated</li><li class=""css-cvpopp""><strong>Entities</strong>: An entity represents a key phrase inside the utterance that we want to extract in relation to the intent</li><li class=""css-cvpopp""><strong>LUIS Authoring Resource</strong>: A resource used for training to create, edit, train, and publish LUIS models</li><li class=""css-cvpopp""><strong>LUIS Prediction Resource</strong>: A resource used to send user\'s text and receive a prediction about intent and entities</li><li class=""css-cvpopp""><strong>Prebuilt Entity</strong>: Prebuilt entities are part of prebuilt models in LUIS for quickly adding common, conversational user scenarios. It is a fast way to add abilities to your applications without building the models yourself</li><li class=""css-cvpopp""><strong>Prebuilt Domains</strong>: Prebuilt models that include a specific list of entities and intents you can use on your applications</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional Resources</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Prebuilt domain support can change based on the culture of your LUIS application. <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-reference-prebuilt-domains#Calendar"">Here is a list you can look at<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> to see what domain is supported by which culture.</li><li class=""css-cvpopp"">For those who want to explore the Regular Expression Language, <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference"">here is a starting point<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp"">Before you start building your applications using LUIS make sure you take a look at <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-best-practices"">the best practices.<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>']","['https://video.udacity-data.com/topher/2021/March/603e3194_luis-access-keys/luis-access-keys.png', 'https://video.udacity-data.com/topher/2021/March/603e32f3_luis-login/luis-login.png', 'https://video.udacity-data.com/topher/2021/March/603e3381_luis-select-subscription/luis-select-subscription.png', 'https://video.udacity-data.com/topher/2021/March/603e33e2_luis-authoring-selected/luis-authoring-selected.png', 'https://video.udacity-data.com/topher/2021/March/603f4cf1_luis-prebuilt-models/luis-prebuilt-models.png']",https://www.youtube.com/embed/x2PGdTTQ47o
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.12  Quizzes: Extracting Intent,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Time for some questions to freshen up!</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which one looks like an utterance?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">FindForm</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">ApplyForJob</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Where is invoice number 4500?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">GetInvoice</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What is the name of the fallback intent?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Default</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Main</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">None</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Empty</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What is <strong>not</strong> a type of entity?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">List</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Array</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Regular Expression</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which one is not a concept related to LUIS?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Utterances</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Entities</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Intents</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Authors</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.13  Recognize and Synthesize Speech,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><strong>Speech synthesis</strong> and <strong>speech recognition</strong> are essential workloads in the NLP space. Suppose you have ever used one of the assistants available both on desktop and mobile operating systems. In that case, you might have already experienced the beauty of communicating with machines through voice.</p>\n<p class=""chakra-text css-o3oz8b"">This lesson will look into speech recognition to transcribe audio into text and speech synthesis to convert text to audio.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s check if you have everything you need to start with your lesson. If you are missing any of the items make sure you go back to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://classroom.udacity.com/nanodegrees/nd099_t/parts/5ddef5bf-04d1-4b9d-94af-dac454840080/modules/3ac10164-797f-4bac-a4f1-a86466c7e9dd/lessons/1032ba0c-0152-4b45-9e4c-38bb6dd2d670/concepts/42bf8680-fc7a-4a1c-a3bc-117918655251"">Preparing Your Workspace page<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Cognitive Service account</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Machine Learning Workspace</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Compute node for Azure Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Jupyter Notebook</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Preparation</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a notebook in your Azure Machine Learning workspace and name it <code class=""chakra-code css-1u83yg1"">l5-speech.ipynb</code>. You can use your notebook to run the code snippets shared below.</p>\n<p class=""chakra-text css-o3oz8b"">In order to use Azure Cognitive Services Speech APIs from Python, you have to install the matching Phyton SDK. The code below will make sure you have the required SDK installed.</p>\n<p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">pip install azure.cognitiveservices.speech</code></p>\n<p class=""chakra-text css-o3oz8b"">Access to Azure Cognitive Services requires three variables that can be accessed through the Azure Portal. During the workspace preparation task, you have copied these values into a text editor of your choice. Now it is time to use them. The values are:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Service Access Key</li><li class=""css-cvpopp"">Service Access Endoint</li><li class=""css-cvpopp"">Service Region</li></ul>\n<p class=""chakra-text css-o3oz8b"">Only two of these variables are needed by the Speech APIs. To use these values, we pass those to variables to be used in future steps. <code class=""chakra-code css-1u83yg1"">&lt;YourKey&gt;</code> and <code class=""chakra-code css-1u83yg1"">&lt;YourRegion&gt;</code> are placeholders for the actual values to be captured from the Azure Portal.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>cognitive_key </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'&lt;YourKey&gt;\'</span><span>\n</span><span>cognitive_service_region </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'&lt;YourRegion&gt;\'</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">We have to import a couple of namespaces before we move forward. <code class=""chakra-code css-1u83yg1"">SpeechConfig</code>, <code class=""chakra-code css-1u83yg1"">SpeechRecognizer</code>, <code class=""chakra-code css-1u83yg1"">AudioConfig</code> are objects and classes we will use to communicate with Speech REST APIs. The <code class=""chakra-code css-1u83yg1"">os</code> and <code class=""chakra-code css-1u83yg1"">IPython</code> imports are for file system access and audio playback in the notebook.  These two imports are not related to Speech SDK, but will help us experience the speech operations in our notebook.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> os\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> IPython\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">from</span><span> azure</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>cognitiveservices</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>speech </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> SpeechConfig</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> SpeechRecognizer</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> AudioConfig\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> azure</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>cognitiveservices</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>speech </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">as</span><span> speechsdk</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">The first step is to get the <code class=""chakra-code css-1u83yg1"">SpeechConfig</code> object ready. This object needs the access key and region name we previously stored in two variables. The code snippet below creates the object by passing the access key and region name, returning the <code class=""chakra-code css-1u83yg1"">SpeechConfig</code> object back to a <code class=""chakra-code css-1u83yg1"">speech_config</code> variable to be used later.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>speech_config </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> SpeechConfig</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>cognitive_key</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> cognitive_service_region</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s see if you have all the objects ready to keep coding.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Did you install the SDK?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Defined the variables to hold the access key and region name</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created the speech_config object holding the access key and the region name</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Speech Recognition</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech Recognition is about transcribing audio to text. One of the advantages of having text is that it is easier to process when it comes to sentiment analysis and keyphrase extraction. Another advantage is that text takes less storage space when it comes to extensive archives of content.</p>\n<p class=""chakra-text css-o3oz8b"">In the case of Azure Cognitive Services Speech APIs, we have multiple options to implement speech recognition. The spoken content can either come in the form of a recorded audio file or tap into a live stream from a microphone. In the background, an acoustic model converts audio into phonemes that represent specific sounds, and following a language, the model converts the sounds into particular words.</p>\n<p class=""chakra-text css-o3oz8b"">In our example below, we will use a prerecorded audio file. Now let\'s dive in.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Open a browser window and navigate got the address provided below.</p>\n<p class=""chakra-text css-o3oz8b""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/lesson-5/speech-sample.wav""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/lesson-5/speech-sample.wav"">https://github.com/udacity/AI_fundamentals/blob/main/lesson-5/speech-sample.wav<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Make sure you see the <code class=""chakra-code css-1u83yg1"">speech-sample.wav</code> and download the file to your computer to a folder of your choice.</li><li class=""css-cvpopp"">Go back to your notebook and upload <code class=""chakra-code css-1u83yg1"">sample-speech.wav</code> file you previously downloaded.</li><li class=""css-cvpopp"">Once the upload is complete, go back to your <code class=""chakra-code css-1u83yg1"">l5-speech.ipynb</code> notebook to continue building the solution.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Our next step will be to define a variable to hold the audio file name. Here we have a variable named <code class=""chakra-code css-1u83yg1"">audio_file</code> that is assigned our \'speech-sample.wav\' file\'s name.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>audio_file </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'speech-sample.wav\'</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Out of the file name, we will create an <code class=""chakra-code css-1u83yg1"">AudioConfig</code> object by passing our <code class=""chakra-code css-1u83yg1"">audio_file</code> into its <code class=""chakra-code css-1u83yg1"">filename</code> parameter.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>audio_config </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> AudioConfig</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>filename</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>audio_file</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span> </span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">So far we have an <code class=""chakra-code css-1u83yg1"">AudioConfig</code> object that has the file name and the <code class=""chakra-code css-1u83yg1"">SpeechConfig</code> object that holds the access key and the region of the Azure Cognitive Services resource.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>speech_recognizer </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> speechsdk</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>SpeechRecognizer</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>\n</span><span>    speech_config</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>speech_config</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> language</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""en-US""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> audio_config</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>audio_config</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">This is where we create our <code class=""chakra-code css-1u83yg1"">SpeechRecognizer</code> to be used in the next step. The <code class=""chakra-code css-1u83yg1"">SpeechRecognizer</code> needs the <code class=""chakra-code css-1u83yg1"">speech_config</code>, the <code class=""chakra-code css-1u83yg1"">audio_config</code> and the language of the audio file passed into it\'s <code class=""chakra-code css-1u83yg1"">language</code> parameter.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> speech_recognizer</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>recognize_once</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Finally, we are calling the <code class=""chakra-code css-1u83yg1"">recognize_once()</code> method of the <code class=""chakra-code css-1u83yg1"">SpeechRecognizer</code> to execute the speech recognition process.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">When the result is back, we will print it out and see what we have.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the screenshot above, you can see the result of the execution of our code. The <code class=""chakra-code css-1u83yg1"">text</code> field of the result has the full transcription of our audio file.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What is <strong>NOT</strong> a needed parameter to create a <code class=""chakra-code css-1u83yg1"">SpeechRecognizer</code> instance?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">speech_config</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">language</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">audio_config</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">audio_font</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Speech Synthesis</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech synthesis is the opposite of speech recognition. It is about reading the text and converting it into an audio file. This is helpful when a machine has to react to its user in the form of audio. The process usually starts by tokenizing the text into words and selects phonetic sounds for each token. Once that is complete, the content is broken into phrases to be synthesized as audio by implementing a voice.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Our first step is to define the voice font to be used in the speech synthesis process. The list of voice fonts (voice names) supported by Azure Cognitive Services Speech Service can be found <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/language-support"">here<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>. In our case, we will use <code class=""chakra-code css-1u83yg1"">en-US, BenjaminRUS</code>.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>voice </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Microsoft Server Speech Text to Speech Voice (en-US, BenjaminRUS)""</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">The next step involves defining the text to be synthesized and the name of the output file that will be generated once the audio is ready. In this case, <code class=""chakra-code css-1u83yg1"">text</code> variable will hold our sample text, and <code class=""chakra-code css-1u83yg1"">file_name</code> will hold the file\'s name to be generated.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>text </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""Using Azure Speech to Text Capability""</span><span>\n</span><span>file_name </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""outputaudio.wav""</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Previously, we created a <code class=""chakra-code css-1u83yg1"">SpeechConfig</code> object that holds the <code class=""chakra-code css-1u83yg1"">cognitive_key</code> and <code class=""chakra-code css-1u83yg1"">cognitive_service_region</code>. In addition to these values, we have to assign our <code class=""chakra-code css-1u83yg1"">voice</code> object that holds the <strong>voice name</strong> to the <code class=""chakra-code css-1u83yg1"">speech_synthesis_voice_name</code> property of the <code class=""chakra-code css-1u83yg1"">SpeechConfig</code> object.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>speech_config </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> speechsdk</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>SpeechConfig</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>subscription</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>cognitive_key</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> region</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>cognitive_service_region</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span>speech_config</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>speech_synthesis_voice_name </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> voice</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">As a next step, we will assign <code class=""chakra-code css-1u83yg1"">file_name</code> to an <code class=""chakra-code css-1u83yg1"">AudioOutputConfig</code> and have it ready in our <code class=""chakra-code css-1u83yg1"">file_config</code> variable.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>file_config </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> speechsdk</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>audio</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>AudioOutputConfig</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>filename</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>file_name</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Now, we have an <code class=""chakra-code css-1u83yg1"">AudioOutputConfig</code> that has the WAV file\'s name to be created and a <code class=""chakra-code css-1u83yg1"">SpeechConfig</code> that has the access key and region to Azure Cognitive Services and the voice name to be used during audio synthesis. Finally, will pass these object as parameters to a <code class=""chakra-code css-1u83yg1"">SpeechSynthesizer</code> as shown below.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>speech_synthesizer </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> speechsdk</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>SpeechSynthesizer</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>speech_config</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>speech_config</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> audio_config</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>file_config</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Our <code class=""chakra-code css-1u83yg1"">SpeechSynthesizer</code> is ready for use. We can call the <code class=""chakra-code css-1u83yg1"">speak_text_async</code> method passing our <code class=""chakra-code css-1u83yg1"">text</code> and call <code class=""chakra-code css-1u83yg1"">get</code> to receive the result.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> speech_synthesizer</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>speak_text_async</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>text</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>get</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">The result object can contain error information if the process fails. Otherwise, the file will be created in the file system with the name we provided in our <code class=""chakra-code css-1u83yg1"">file_name</code> variable. You can use the code below to play the file on your notebook environment.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>IPython</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>display</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>display</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>IPython</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>display</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>Audio</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>file_name</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> autoplay</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">True</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the screenshot above, you can see a two-second long audio file generated and ready for playback.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Now you transcribed a sample audio file into text and synthesized text into audio. Imagine how these NLP capabilities can enhance your application\'s interaction with its users. Keep working on your skills by trying out various texts and audio files you can create on your computer by simply recording WAV files with your microphone.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Playground Demo</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Visit <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text/"">Microsoft\'s Speech-to-Text web site<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> and use the demo to speak through a microphone and observe the text output.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">New terms:</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Voice Name and Styles:</strong> Voice names and styles define the tone and the emotions of a speech.  You can listen to a couple of examples <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://speech.microsoft.com/customvoice"">here<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional Resources</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Interested in creating your own custom voice? Take a look at the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://speech.microsoft.com/customvoice"">Custom Voice service<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li><li class=""css-cvpopp"">Want to improve speech-to-text accuracy for your applications and products? <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/custom-speech-overview"">Custom Speech Implementation<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> can help.</li><li class=""css-cvpopp"">Having some questions in your head? Make sure you take a look at the official <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/faq-stt"">Speech to Text FAQ<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li></ul></div>']","['https://video.udacity-data.com/topher/2021/March/603f748f_azureml-speech-recognition-sample/azureml-speech-recognition-sample.png', 'https://video.udacity-data.com/topher/2021/March/603f7be5_azureml-speech-synthesis-sample/azureml-speech-synthesis-sample.png']",https://www.youtube.com/embed/-YAiMSdNPjY
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.14  Quizzes: Recognize and Synthesize Speech,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Take a breath, now go!</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You are going to execute <code class=""chakra-code css-1u83yg1"">speechsdk.SpeechRecognizer</code>. What do you need to convert an audio file in English to a text? (Select all that apply)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">SpeechConfig</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">AudioConfig</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Language Code String such as ""en-US""</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">LanguageConfig</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You are going to execute <code class=""chakra-code css-1u83yg1"">speechsdk.SpeechSynthesizer</code>. What do you need to synthesize a text into English spoken audio? (Select all that apply)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">speech_synthesis_voice_name</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">AudioOutputConfig</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">speech_config</code></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Sample Audio File</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which one is <strong>NOT</strong> a speech service functionality?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech-to-Text</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Text-to-Speech</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech-to-Speech</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.15  Exercise: Use the Speech Service,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You have learned how to synthesize speech and transcribe text. Now it is time to get things running. In this exercise, you will be given an audio file and asked to translate that into text. To go the other way around, you will type in a text and synthesize speech based on the text. The exercise will help your practice with Azure Speech services and see it in action.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s check if you have everything you need to start with your lab. If you are missing any of the items make sure you go back to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://classroom.udacity.com/nanodegrees/nd099_t/parts/5ddef5bf-04d1-4b9d-94af-dac454840080/modules/3ac10164-797f-4bac-a4f1-a86466c7e9dd/lessons/1032ba0c-0152-4b45-9e4c-38bb6dd2d670/concepts/8ce0f94a-9084-4860-83d9-e77a95de0979"">Getting Prepared page<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in this lesson and complete the tasks.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Cognitive Service account</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Machine Learning Workspace</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Compute node for Azure Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Cloned Lab Files from GitHub</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Working on the Lab</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Log in to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com"">Azure Portal <span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>with your credentials, and navigate to your Azure ML Studio. Make sure you have your access key and endpoint from the <strong>Keys and Endpoints</strong> section of your Azure Cognitive Service. You will need those during the exercise. Open <strong>Notebooks (1)</strong> and launch <code class=""chakra-code css-1u83yg1"">speech.ipynb</code> <strong>(3)</strong> under <strong>lesson-5 (2)</strong> and follow instructions in the Jupyter notebook.</p></div>']",['https://video.udacity-data.com/topher/2021/February/6033a6ba_azureml-speech/azureml-speech.png'],
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.16  Solution: Use the Speech Service,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Good work! Now, open the <code class=""chakra-code css-1u83yg1"">speech_solution.ipynb</code> <strong>(3)</strong> notebook to see our implementation. You can access the file in the <strong>Notebooks (1)</strong> section in your Azure ML Studio under <strong>lesson-5 (2)</strong> folder. If you are still holding your Azure Cognitive Service Access Key and Endpoint information, fill those in and run the notebook cells to see those in action.</p></div>']",['https://video.udacity-data.com/topher/2021/February/6033a584_azureml-translate-solution/azureml-translate-solution.png'],https://www.youtube.com/embed/o_r0UOe50ug
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.17  Translating Text And Speech,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Having global business spanning across geographies, the language barrier is hitting organizations harder than ever. Today, meetings with people from various cultures and regions are a common occurrence. One way of removing the language barrier is to automate the translation process from one language to another. This is where machine translation comes into the picture.</p>\n<p class=""chakra-text css-o3oz8b"">In this lesson, we will <strong>use Azure Cognitive Services to implement both text and speech translation</strong>. Text translation can help translate documents or text messages. Speech translation can do both speech-to-speech and speech-to-text translation. In the course of our lesson, we will focus on text translation and speech-to-text translation.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s check if you have everything you need to start with your lesson. If you are missing any of the items make sure you go back to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://classroom.udacity.com/nanodegrees/nd099_t/parts/5ddef5bf-04d1-4b9d-94af-dac454840080/modules/3ac10164-797f-4bac-a4f1-a86466c7e9dd/lessons/1032ba0c-0152-4b45-9e4c-38bb6dd2d670/concepts/42bf8680-fc7a-4a1c-a3bc-117918655251"">Preparing Your Workspace page<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Cognitive Service account</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Machine Learning Workspace</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Compute node for Azure Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Jupyter Notebook</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Preparation</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a notebook in your Azure Machine Learning workspace, and name it <code class=""chakra-code css-1u83yg1"">l5-translation.ipynb</code>. You can use your notebook to run the code snippets shared below.</p>\n<p class=""chakra-text css-o3oz8b"">In order to use Azure Cognitive Services Translation and Speech APIs from Python, you have to install the Phyton SDK. The code below will make sure you have the required SDK installed.</p>\n<p class=""chakra-text css-o3oz8b""><code class=""chakra-code css-1u83yg1"">pip install azure.cognitiveservices.speech</code></p>\n<p class=""chakra-text css-o3oz8b"">Access to Azure Cognitive Services requires three variables that can be accessed through the Azure Portal. During the workspace preparation task, you have copied these values into a text editor of your choice. Now it is time to use them. The values are:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Service Access Key</li><li class=""css-cvpopp"">Service Access Endoint</li><li class=""css-cvpopp"">Service Region</li></ul>\n<p class=""chakra-text css-o3oz8b"">Only two of these variables are needed by the Translation and Speech APIs. To use these values, we pass those to variables to be used in future steps. <code class=""chakra-code css-1u83yg1"">&lt;YourKey&gt;</code> and <code class=""chakra-code css-1u83yg1"">&lt;YourRegion&gt;</code> are placeholders for the actual values to be captured from the Azure Portal.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>cognitive_key </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'&lt;YourKey&gt;\'</span><span>\n</span><span>cognitive_service_region </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'&lt;YourRegion&gt;\'</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s see if you have all the objects ready to keep coding.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Did you install the SDK?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Defined the variables to hold the access key and region name</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Text Translation</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The globalization of businesses and interactions on social media is making translations more critical than in the past. Being able to translate text helps not only to translate documents but also to translate video subtitles or meeting transcriptions. The Text Translator service in Azure Cognitive Services supports <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/Translator/language-support"">more than 60 languages<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>. In the background, it uses the Neural Machine Translation model that involves examining the semantic context of the document and provides a more realistic and comprehensive translation.</p>\n<p class=""chakra-text css-o3oz8b"">Now let\'s dive in.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">To use the Text Translator service, we have to send RAW HTTP requests as the service does not have a Python SDK. We will start by importing a couple of libraries in python to craft the proper HTTP request and submit it to the Text Analytics endpoint. <code class=""chakra-code css-1u83yg1"">requests</code>, <code class=""chakra-code css-1u83yg1"">uuid</code>, <code class=""chakra-code css-1u83yg1"">json</code> are the libraries we will use. <code class=""chakra-code css-1u83yg1"">requests</code> will help us with the HTTP communication. <code class=""chakra-code css-1u83yg1"">uuid</code> will create a unique identifier when we need it. Finally, <code class=""chakra-code css-1u83yg1"">json</code> will help to parse the API response.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> requests</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> uuid</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> json</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">The endpoint address below is the default base endpoint for the Text Translation service. We will store that in a variable called <code class=""chakra-code css-1u83yg1"">endpoint</code> for later use.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>endpoint </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">""https://api.cognitive.microsofttranslator.com/translate""</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">The first set of information we need to pass to the Text Translation service is the version of API we will use (<code class=""chakra-code css-1u83yg1"">api-version</code>), the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""http://www.loc.gov/standards/iso639-2/php/code_list.php"">ISO 639-1 language code<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> of the source document\'s language (<code class=""chakra-code css-1u83yg1"">from</code>) and the target translation language codes (<code class=""chakra-code css-1u83yg1"">to</code>). In our example below, we target the latest version of the API and ask for a translation from English to German and Italian.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>params </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">{</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'api-version\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'3.0\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'from\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'en\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'to\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'de\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'it\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61);"">}</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">The second set of information we need to pass is going to be in the header of our HTTP request. <code class=""chakra-code css-1u83yg1"">Ocp-Apim-Subscription-Key</code> will hold our access key, <code class=""chakra-code css-1u83yg1"">Ocp-Apim-Subscription-Region</code> will hold the region of our service. Finally, we will specify that we are looking for a JSON response by defining <code class=""chakra-code css-1u83yg1"">Content-type</code> to <code class=""chakra-code css-1u83yg1"">application/json</code> and provide a unique identifier to <code class=""chakra-code css-1u83yg1"">X-ClientTraceId</code> for tracing purposes if needed.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>headers </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">{</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'Ocp-Apim-Subscription-Key\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> cognitive_key</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'Ocp-Apim-Subscription-Region\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> cognitive_service_region</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'Content-type\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'application/json\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'X-ClientTraceId\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> </span><span class=""token"" style=""color: rgb(0, 121, 162);"">str</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>uuid</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>uuid4</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61);"">}</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">It is time to provide our original text. A simple hello world would do it.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>body </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> </span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(15, 43, 61);"">{</span><span>\n</span><span>    </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'text\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">:</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'Hello World!\'</span><span>\n</span><span></span><span class=""token"" style=""color: rgb(15, 43, 61);"">}</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Now that we are ready, we can submit a POST request to the endpoint with all the values we set previously, including <code class=""chakra-code css-1u83yg1"">params</code>, <code class=""chakra-code css-1u83yg1"">headers</code> and the <code class=""chakra-code css-1u83yg1"">body</code> of the request.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>request </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> requests</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>post</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>endpoint</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> params</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>params</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> headers</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>headers</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> json</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>body</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Once we receive a response back, we will read the JSON into a variable called <code class=""chakra-code css-1u83yg1"">response</code>.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>response </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> request</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>json</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">At this point, we have our translations back, and we can use them in our apps. For the sake of a test run, we will output the response.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>json</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>dumps</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>response</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> sort_keys</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">True</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> ensure_ascii</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">False</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> indent</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(2, 124, 124);"">4</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> separators</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\',\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\': \'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In the screenshot above, you can see a running example of our code with the result printed. Pretty cool to have multiple translations at once.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which one is <strong>NOT</strong> a capability of the Text Translation service?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Translation into multiple languages at once.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Translate to more than 60 languages</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Translate to Klingon</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Extract sentiment from text</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Speech Translation</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Using speech is opening up so many opportunities to communicate with devices. On top of that, the ability to translate speech either in real-time or in batch, recorded mode helps to tap into the spoken words and reach out to a wider audience unlimited of the language barrier.</p>\n<p class=""chakra-text css-o3oz8b"">We took a look at the Speech Service in a previous lesson. Now we will focus on its translation capability. The Speech Translation API supports both real-time audio input and recorded audio translations. The service supports the same extensive list of languages backed by the Text Translation service. In our example, we will use a sample audio file and translate it into a different language in a text form.</p>\n<p class=""chakra-text css-o3oz8b"">Let\'s start!</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Open a browser window and navigate got the address provided below.</p>\n<p class=""chakra-text css-o3oz8b""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/lesson-5/speech-sample.wav""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/lesson-5/speech-sample.wav"">https://github.com/udacity/AI_fundamentals/blob/main/lesson-5/speech-sample.wav<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></p></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Make sure you see the <code class=""chakra-code css-1u83yg1"">speech-sample.wav</code> and download the file to your computer to a folder of your choice.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""2"" class=""css-13a5a39""><li class=""css-cvpopp"">Go back to your notebook and upload the <code class=""chakra-code css-1u83yg1"">sample-speech.wav</code> file you previously downloaded.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><ol role=""list"" start=""3"" class=""css-13a5a39""><li class=""css-cvpopp"">Once the upload is complete go back to your <code class=""chakra-code css-1u83yg1"">l5-translation.ipynb</code> notebook to continue building the solution.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">As a first step, we will import the Azure <code class=""chakra-code css-1u83yg1"">speechsdk</code> that we will use to communicate with Speech Translation API.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">import</span><span> azure</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>cognitiveservices</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>speech </span><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">as</span><span> speechsdk</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">The second step is to create a <code class=""chakra-code css-1u83yg1"">SpeechTranslationConfig</code>object that will hold our access key, region name, speech recognition language and the target languages we want to translate to.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>translation_config </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> speechsdk</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>translation</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>SpeechTranslationConfig</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>\n</span><span>        subscription</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>cognitive_key</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> region</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>cognitive_service_region</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>        speech_recognition_language</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'en-US\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span>\n</span><span>        target_languages</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'de\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> </span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'fr\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Our third step is to prepare an <code class=""chakra-code css-1u83yg1"">AudioConfig</code> object that holds the source WAV file\'s name. You can see below that we are creating a <code class=""chakra-code css-1u83yg1"">AudioConfig</code> object and passing the file name to its <code class=""chakra-code css-1u83yg1"">filename</code> property.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>audio_config </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> speechsdk</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>audio</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>AudioConfig</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>filename</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span class=""token"" style=""color: rgb(221, 17, 68);"">""speech-sample.wav""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Now that we have all the configuration objects ready we can create a <code class=""chakra-code css-1u83yg1"">TranslationRecognizer</code> by passing our <code class=""chakra-code css-1u83yg1"">translation_config</code> and <code class=""chakra-code css-1u83yg1"">audio_config</code>. At this point the <code class=""chakra-code css-1u83yg1"">TranslationRecognizer</code> knows our service access key, service endpoint, source language, target languages, and source file name.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>recognizer </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> speechsdk</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>translation</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>TranslationRecognizer</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>\n</span><span>    translation_config</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>translation_config</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> audio_config</span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span>audio_config</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Finally, we execute <code class=""chakra-code css-1u83yg1"">TranslationRecognizer</code>\'s <code class=""chakra-code css-1u83yg1"">recognize_once()</code> method to start the translation process.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span>result </span><span class=""token"" style=""color: rgb(15, 43, 61); background: rgb(255, 255, 255);"">=</span><span> recognizer</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>recognize_once</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span></code></div></div></pre>\n<p class=""chakra-text css-o3oz8b"">Once we have the result, we print the translations out by accessing each translation with its code from the <code class=""chakra-code css-1u83yg1"">result.translations</code> array.</p>\n<pre class=""css-0""><div data-defines-codeblock=""true"" tabindex=""0"" class=""css-1y373wj""><div style=""color: rgb(15, 43, 61); text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><code class=""language-py"" style=""color: rgb(15, 43, 61); background: none; text-shadow: white 0px 1px; font-family: &quot;Fira Code&quot;, Consolas, Monaco, &quot;Andale Mono&quot;, &quot;Ubuntu Mono&quot;, monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; overflow-wrap: normal; line-height: 1.5; tab-size: 4; hyphens: none;""><span class=""token"" style=""color: rgb(15, 43, 61); font-weight: bold;"">print</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span class=""token triple-quoted-string"" style=""color: rgb(221, 17, 68);"">""""""Recognized: {}\n</span><span class=""token triple-quoted-string"" style=""color: rgb(221, 17, 68);"">German translation: {}\n</span><span class=""token triple-quoted-string"" style=""color: rgb(221, 17, 68);"">French translation: {}""""""</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span class=""token"" style=""color: rgb(0, 121, 162);"">format</span><span class=""token"" style=""color: rgb(15, 43, 61);"">(</span><span>\n</span><span>    result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>text</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>translations</span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'de\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">,</span><span> result</span><span class=""token"" style=""color: rgb(15, 43, 61);"">.</span><span>translations</span><span class=""token"" style=""color: rgb(15, 43, 61);"">[</span><span class=""token"" style=""color: rgb(221, 17, 68);"">\'fr\'</span><span class=""token"" style=""color: rgb(15, 43, 61);"">]</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span class=""token"" style=""color: rgb(15, 43, 61);"">)</span><span>`</span></code></div></div></pre></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Above is a screenshot where you can see the sample code execute with the resulting two translations printed.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Wasn\'t that easy? You implemented Text Translation that translated a source text into two different languages. Moreover, you completed a speech translation implementation that recognized an audio file and translated it into two languages. Better, you can do these with more than 60 languages without a sweat.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Playground Demo</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Download one of the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.microsoft.com/en-us/translator/"">Microsoft Translator mobile applications<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> to your phone and test speech translation.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">New terms:</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Klingon</strong>: The language of a fictional species in the science fiction franchise called...(Can you guess it?)</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional Resources</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Here you can find more <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/MicrosoftTranslator"">Translator Code Samples hosted on Github<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp"">If you are looking to translate large documents <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/translator/document-translation/overview"">Document Translation<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> can help.</li></ul></div>']","['https://video.udacity-data.com/topher/2021/March/603f92df_azureml-texttranslation-sample-result/azureml-texttranslation-sample-result.png', 'https://video.udacity-data.com/topher/2021/March/603f9a0b_azureml-speechtranslation-sample-result/azureml-speechtranslation-sample-result.png']",https://www.youtube.com/embed/oXVWgM3Sx-k
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.18  Quizzes: Translating Text And Speech,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What\'s  <strong>not</strong> true about Text Translation with Azure Cognitive Services?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You can translate to multiple language in one HTTP Request</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">The URL endpoint is custom for every service account.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">It\'s API works based on JSON objects.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">It requires the source text\'s language.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You are going to run the <code class=""chakra-code css-1u83yg1"">speechsdk.translation.TranslationRecognizer</code> method. You are preparing a <code class=""chakra-code css-1u83yg1"">SpeechTranslationConfig</code> object. Which one is <strong>not</strong> needed?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Cognitive Service Access Key</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Cognitive Service Region Name</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Recognition Language</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Voice Font Name</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following statements is true?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech Translation supports Real-Time translation of speech.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech Translation requires prerecorded voice files for translations.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech Translation can translate to text only.</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.19  Exercise: Translating Text And Speech,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You have learned how to translate text and speech to different languages. Now it is time to get your hands on some code. In this exercise, you will be given an audio file and some text to translate those into different languages. The exercise will help your practice with Azure Text Translation and Speech Translation services and see it in action.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Let\'s check if you have everything you need to start with your lab. If you are missing any of the items make sure you go back to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://classroom.udacity.com/nanodegrees/nd099_t/parts/5ddef5bf-04d1-4b9d-94af-dac454840080/modules/3ac10164-797f-4bac-a4f1-a86466c7e9dd/lessons/1032ba0c-0152-4b45-9e4c-38bb6dd2d670/concepts/8ce0f94a-9084-4860-83d9-e77a95de0979"">Getting Prepared page<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> in this lesson and complete the tasks.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Cognitive Service account</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created an Azure Machine Learning Workspace</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Created a Compute node for Azure Machine Learning</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Cloned Lab Files from GitHub</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Working on the Lab</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Log in to the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com"">Azure Portal <span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>with your credentials, and navigate to your Azure ML Studio. Make sure you have your access key and endpoint from the <strong>Keys and Endpoints</strong> section of your Azure Cognitive Service. You will need those during the exercise. Open <strong>Notebooks (1)</strong> and launch <code class=""chakra-code css-1u83yg1"">translation.ipynb</code> <strong>(3)</strong> under <strong>lesson-5 (2)</strong> and follow instructions in the Jupyter notebook.</p></div>']",['https://video.udacity-data.com/topher/2021/February/6033a6fe_azureml-translate/azureml-translate.png'],
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.20  Solution: Translating Text And Speech,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Good work! Now, open the <code class=""chakra-code css-1u83yg1"">translation_solution.ipynb</code> <strong>(3)</strong> notebook to see our implementation. You can access the file in the <strong>Notebooks (1)</strong> section in your Azure ML Studio under <strong>lesson-5 (2)</strong> folder. If you are still holding your Azure Cognitive Service Access Key and Endpoint information, fill those in and run the notebook cells to see those in action.</p></div>']",['https://video.udacity-data.com/topher/2021/February/6033a757_azureml-translate-solution/azureml-translate-solution.png'],https://www.youtube.com/embed/V-w-tOnixC8
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.21  Edge Cases,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Language Detection</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In Text Analytics, we discussed language detection to detect the language of documents and texts. However, there might be cases where the text can contain <strong>multiple languages at once</strong>. In this case, the result will reflect the dominant language. The text\'s length in a particular language in the document will affect the decision about the dominant language. The confusion between multiple languages will definitely be reflected in the confidence score in a negative way.</p>\n<p class=""chakra-text css-o3oz8b"">Another edge case with Text Analytics related to texts that only have punctuations. For example, if one tries to detect the language of a text with only a "":D"" in it, the service will return <strong>unknown</strong> for the language and NaN (Not-A-Number) for its confidence level.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Sentiment Analysis</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In Sentiment Analysis, a rare occurrence is a sentiment score of 0.5 that represents complete neutrality. The result might be correct, but it always good to double-check if the language parameter passed for analysis is the correct language for the text. In most cases, when the language parameter does not match the submitted document, sentiment analysis results in neutral sentiment scores.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">When implementing AI concepts, it is critical to test and keep an eye on the results. Some of the services we discussed so far have customizable options where you can contribute data and build your models. Most are linked in the particular lessons page in the additional resources section. If you start feeling that you need more flexibility, make sure you take a look at the supplementary resources provided.</p></div>']",[],https://www.youtube.com/embed/GsYXqnudh2U
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.22  Lesson Review,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">At this moment, you should be able to use Azure Cognitive Services in your solution to</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">detect the language of documents and text</li><li class=""css-cvpopp"">extract key phrases and entities from text</li><li class=""css-cvpopp"">understand the sentiment carried out in a text</li><li class=""css-cvpopp"">translate text and audio to a set of languages</li><li class=""css-cvpopp"">extract intent from text</li><li class=""css-cvpopp"">recognize speech and transcribe it</li><li class=""css-cvpopp"">synthesize speech and get your applications talking</li></ul></div>']",[],https://www.youtube.com/embed/4EnqsIlgLVM
AI Fundamentals,AI Fundamentals,Lesson 6: Natural Language Processing,6.23  Glossary,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">For your reference, here are all the new terms we introduced in this lesson:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Confidence Score</strong>: The confidence score can have a different meaning for various AI implementations. The overall idea is about how confident the algorithm is about its result or decision.</li><li class=""css-cvpopp""><strong>Entity</strong>: An item of a particular type or a category, for example, a location or organization.</li><li class=""css-cvpopp""><strong>Intents</strong>: An intent represents the action the user wants to execute.</li><li class=""css-cvpopp""><strong>Intent Extraction</strong>: Analyzing text to understand that would connect the text with a predefined action. Intent extraction is a popular approach in chatbot development.</li><li class=""css-cvpopp""><strong>Key Phrase Extraction</strong>: Identifying terms in a text that best describes the subject of the document.</li><li class=""css-cvpopp""><strong>Klingon</strong>: The language of a fictional species in the science fiction franchise called..?</li><li class=""css-cvpopp""><strong>LUIS Prediction Resource</strong>: A resource used to send user\'s text and receive a prediction about intent and entities.</li><li class=""css-cvpopp""><strong>LUIS Authoring Resource</strong>: A resource used for training to create, edit, train, and publish LUIS models.</li><li class=""css-cvpopp""><strong>Prebuilt Domains</strong>: Prebuilt models that include a specific list of entities and intents you can use on your applications.</li><li class=""css-cvpopp""><strong>Prebuilt Entity</strong>: Prebuilt entities are part of prebuilt models in LUIS for quickly adding common, conversational user scenarios. It is a fast way to add abilities to your applications without building the models yourself.</li><li class=""css-cvpopp""><strong>SDK:</strong> Software Development Kit. In our case. We used Python SDKs for Azure Cognitive Services to access REST APIs\' functionality without worrying about the raw HTTP implementations.</li><li class=""css-cvpopp""><strong>Service Access Endpoint:</strong> The HTTP endpoint where an SDK can access the cognitive service.</li><li class=""css-cvpopp""><strong>Service Region:</strong> The deployment location of the services in terms of Azure Data Center locations. Some SDKs use the region instead of the endpoint and prefer to build their endpoint structure based on the region metadata.</li><li class=""css-cvpopp""><strong>Speech synthesis</strong>: the artificial production of human speech</li><li class=""css-cvpopp""><strong>Text Analytics:</strong> The umbrella service in Azure that provides NLP functionalities for text processing. Text Analytics is part of Azure Cognitive Services. Text Analytics can be used stand-alone or used as part of an Azure Cognitive Services resource in the Azure Portal.</li><li class=""css-cvpopp""><strong>Utterances</strong>: An utterance is a single input from the user that has to be evaluated.</li><li class=""css-cvpopp""><strong>Voice Name and Styles:</strong> Voice names and styles define the tone and the emotions of a speech.  You can listen to a couple of examples <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://speech.microsoft.com/customvoice"">here<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li></ul></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.1  Introduction & Lesson Overview,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Lesson Overview</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Summary</h3>\n<p class=""chakra-text css-o3oz8b"">Conversational AI workloads discussed in previous lessons</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Speech Recognition</li><li class=""css-cvpopp"">Natural Language Understanding</li><li class=""css-cvpopp"">Text to Speech</li></ul>\n<p class=""chakra-text css-o3oz8b"">Additional capabilities to be covered in this lesson:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Knowledge Base creation</li><li class=""css-cvpopp"">Building a conversational AI chatbot</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Core Azure Services of interest</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Azure QnA Maker</li><li class=""css-cvpopp"">Azure Bot Service.</li></ul>\n<p class=""chakra-text css-o3oz8b"">At the end of this lesson, you will be able to build a chatbot backed by a knowledge base.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Additional Resources</h3>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Take a look at the<a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/bot-service/bot-service-manage-channels?view=azure-bot-service-4.0""> list of bot channels<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> you can connect to a single bot through Azure Bot Service.</li></ul></div>']",[],https://www.youtube.com/embed/Zv3oUJqL9hQ
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,"7.2  Conversational AI, Its Common Characteristics and Use Cases","['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">Some conversational AI workloads are:</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Voice calls</li><li class=""css-cvpopp"">Messaging services</li><li class=""css-cvpopp"">Online chat applications</li><li class=""css-cvpopp"">Email</li><li class=""css-cvpopp"">Social media platforms</li><li class=""css-cvpopp"">Collaborative workplace tools</li></ul>\n<h4 class=""chakra-heading css-1dlhxqh"">Webchat vs. Voice-Enabled Chat or both?</h4>\n<p class=""chakra-text css-o3oz8b"">Once you have a bot deployed to Azure Bot Service, you can use it as a web chatbot, as a voice-enabled chatbot, or both. Both webchat and voice chat is enabled through the use of channels in Azure Bot Service. Channels help bring in more communication pathways for a single bot and communicate with users through various means. For example, in addition to Web Chat and Direct Line of Speech, you can enable Facebook and Slack, and many other social platforms where your bot can integrate.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">New terms</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Bot:</strong> An AI-infused agent that can receive user input and provide an appropriate response.</li><li class=""css-cvpopp""><strong>Channel:</strong> A channel for Azure Bot Service is a bridge between the user and the bot that defines the communication method and the communication path. For example, Direct Line of Speech channel enables speech-based communication, whereas Web Chat channel provides a web interface for users to communicate with the Bot.</li><li class=""css-cvpopp""><strong>Knowledge-base:</strong> A knowledge base is a list of questions and answers that a bot can use to extract information.</li></ul></div>']",[],https://www.youtube.com/embed/yDjYjbmYw_M
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,"7.3  Quizzes: Conversational AI, Its Common Characteristics and Use Cases","['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which of the following can be a conversational AI workload?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Support E-Mails</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Social Media</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Online chat applications</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Voice calls</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What AI capabilities does Direct Line of Speech use?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech Translation</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech Synthesis</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Speech Recognition</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Language Understanding</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What Conversational AI experiences you enjoy during your day-to-day work?</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.4  Question And Answer Conversational Exchanges,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">A bot will need a knowledge base to look up to to get the information users requests. Azure QnA Maker can create a knowledge base out of Questions and Answers data source to be used by a bot.</p>\n<p class=""chakra-text css-o3oz8b"">In order to drive conversational exchanges, we have to build a bot that can do dialog management and using NLP services such as LUIS to understand the intent and extract keywords from incoming messages (utterances)  to craft the right dialog. When it comes to building a bot we have a couple of options.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Bot Framework SDK: Coding the bot manually.</li><li class=""css-cvpopp"">Bot Composer: Building dialog flows through a user interface</li><li class=""css-cvpopp"">An automatic bot that comes with QnA Maker.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Playground Demo</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Head over to <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://aidemos.microsoft.com/luis/demo"">Microsoft\'s LUIS playground<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> and use your voice or text to give commands to a home automation system. Once you are done testing, a couple of different scenarios come back for the quiz.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional Resources</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">If you are interested in building your own dialogs within a user interface visit the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/composer/introduction"">Azure Bot Composer introduction page<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li><li class=""css-cvpopp"">Are you a developer who wants to get their hands on the code? Here are some <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/bot-service/index-bf-sdk?view=azure-bot-service-4.0"">learning paths from Microsoft<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> that can help you get started with the Bot Framework SDK.</li></ul></div>']",[],https://www.youtube.com/embed/wjKykdfp_x8
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.5  Setting Up a Knowledge Base With QnA Maker,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Terms to remember from the previous lesson:</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Confidence Score</strong>: The confidence score can have a different meaning for various AI implementations. The overall idea is about how confident the algorithm is about its result or decision.</li><li class=""css-cvpopp""><strong>Intents</strong>: An intent represents the action the user wants to execute.</li><li class=""css-cvpopp""><strong>Intent Extraction</strong>: Analyzing text to understand that would connect the text with a predefined action. Intent extraction is a popular approach in chatbot development.</li><li class=""css-cvpopp""><strong>Utterances</strong>: An utterance is a single input from the user that has to be evaluated.</li></ul></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Bots taking care of customer support is a common scenario. An organization can use a bot with a backing knowledge-based built out of a list of questions and answers to take care of everyday support questions. The QnA Maker service is a Cognitive Service that can ingest a list of questions and answers from a web page or a document. The beauty of QnA Maker is its use of Natural Language Processing concepts we discussed in previous lessons. QnA Maker can use the questions and answers in its knowledge as a data source and interpret user requests to find the best solution possible by using NLP techniques.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 1 - Set Up A QnA Maker Service</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Navigate to the URL below in a new browser window. The link will land you directly to the QnA Maker Create screen. If asked, authenticate with your Azure Subscription credentials.</p>\n<p class=""chakra-text css-o3oz8b""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com/#create/Microsoft.CognitiveServicesQnAMaker""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://portal.azure.com/#create/Microsoft.CognitiveServicesQnAMaker"">https://portal.azure.com/#create/Microsoft.CognitiveServicesQnAMaker<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> </p>\n<p class=""chakra-text css-o3oz8b"">Following the steps to set up the QnA Maker service:</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Fill in the information\n<ol role=""list"" class=""css-124kmyc""><li class=""css-cvpopp"">Select your subscription and the resource group where you want to deploy the service.</li><li class=""css-cvpopp"">Provide a unique name to the service, in our case, it is named <code class=""chakra-code css-1u83yg1"">drnqna</code>.</li><li class=""css-cvpopp"">Pick <strong>Free F0</strong> pricing tier from the list.</li><li class=""css-cvpopp"">Pick the region where you want your data to be hosted. Preferably, you should deploy all your services in a <strong>single region</strong> to ensure you don\'t pay for data traffic between regions.</li><li class=""css-cvpopp"">Pick <strong>Free F</strong> pricing tier for the Azure Search service.</li><li class=""css-cvpopp"">Provide a unique name for the app and select a deployment location. The location should preferably be the same location as the Azure Search service location in step 4</li></ol>\n</li><li class=""css-cvpopp"">Select <strong>Disabled</strong> for App Insights</li><li class=""css-cvpopp"">Click <strong>Review + Create</strong> to continue</li><li class=""css-cvpopp"">Select <strong>Create</strong> to give final approval to provision all the resources needed to get the QnA Maker up and running. The provisioning can take a couple of minutes.</li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 2 - Set Up A Knowledge Base (KB)</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Navigate to the URL below in your browser. The link will land you directly to the QnA Maker Create screen. If asked, authenticate with your Azure Subscription credentials.</p>\n<p class=""chakra-text css-o3oz8b""><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.qnamaker.ai/Create""><span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a><a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.qnamaker.ai/Create"">https://www.qnamaker.ai/Create<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> </p>\n<p class=""chakra-text css-o3oz8b"">As we already created our QnA service, we will skip Step 1 and head over to <strong>Step 2- connect your QnA service to your KB</strong> (knowledge base).</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h3 class=""chakra-heading css-k57syw"">Step 2: Connect your QnA service to your KB</h3>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">If you have multiple Azure Directory IDs select the one that holds your Azure Subscription.</li><li class=""css-cvpopp"">Select your Azure Subscription and the QnA service you previously created.</li><li class=""css-cvpopp"">From the list of languages, select English listed under <strong>Chit-Chat and extraction available</strong>. Having chit-chat enabled will make your bot more conversational and engaging. We will configure the details around this at a later step.</li></ol>\n<h3 class=""chakra-heading css-k57syw"">Step 3: Name your KB</h3>\n<p class=""chakra-text css-o3oz8b"">Name your KB. This is just for reference and you can change it later if needed.</p>\n<h3 class=""chakra-heading css-k57syw"">Step 4: Populate your KB</h3>\n<p class=""chakra-text css-o3oz8b"">It is time to give the QnA Maker some data to populate the knowledge base.</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">Provide the URL of the sample document listed below. This is a word document with a list of questions and answers.\n<code class=""chakra-code css-1u83yg1"">https://raw.githubusercontent.com/udacity/AI_fundamentals/main/lesson-6/sample_faq.docx</code></p>\n</li><li class=""css-cvpopp"">\n<p class=""chakra-text css-o3oz8b"">In the Chit-Chat section, set the tone of our bot as <strong>Professional.</strong>  This will make sure QnA Maker prepopulates the knowledge base with questions and answers covering around 100 different chit-chat scenarios.</p>\n</li></ol>\n<h3 class=""chakra-heading css-k57syw"">Step 5: Create your KB</h3>\n<p class=""chakra-text css-o3oz8b"">Select <strong>Create your KB</strong> to start creating the knowledge base.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 3 - Train and Test the Knowledge Base</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once you are on the knowledge base page select <strong>Save and train</strong> to start training your model. When training is complete select <strong>Test</strong> to test your bot.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Please try the following two scenarios:</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">As a test, try typing in<code class=""chakra-code css-1u83yg1"">Hello</code>, and you will see that the bot responds by saying <code class=""chakra-code css-1u83yg1"">Hello</code> back even though we did not train it to do that.</li><li class=""css-cvpopp"">As a second test, try typing <code class=""chakra-code css-1u83yg1"">Can you tell me about Nanodegree programs?</code>. The answer will be extracted from the knowledge base.</li></ol>\n<p class=""chakra-text css-o3oz8b"">Take a look at the original question in the knowledge base in the background and notice how different it is compared to the question we submitted. The original question in the knowledge base is <code class=""chakra-code css-1u83yg1"">What is a Nanodegree Program?</code> where our question is <code class=""chakra-code css-1u83yg1"">Can you tell me about Nanodegree programs?</code>. This is where the power of NLP comes in and helps QnA Maker to understand the intent of the question and provide the answer that QnA Maker feels confident.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You can also <strong>inspect</strong> the process:</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Under the question about Udaiciry Nanodegree, open the <strong>inspection</strong> tab by clicking <strong>inspect.</strong></li><li class=""css-cvpopp"">You can see the <strong>confidence score</strong> of this particular intent extraction.</li><li class=""css-cvpopp"">You can type alternative questions to help QnA Maker with additional utterances.</li><li class=""css-cvpopp"">You can provide a better answer and add a new Question and Answer into the knowledge base.</li></ol>\n<p class=""chakra-text css-o3oz8b"">Once you are done with your testing, select the <strong>Test</strong> button again to close the testing panel.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What is <strong>NOT</strong> a feature of QnA Maker?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Ingesting word documents for QnA data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Casual chat besides questions and answers.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Detect user\'s sentiment.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Ingesting PDF documents for QnA data.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">Congratulations! You just set up an Azure QnA Maker service and a Knowledge Base that the bot can look up for answers. In the next section, we will learn how to create a Bot using the Knowledge base you just created.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">New terms:</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Auth Key</strong>: The short version of ""Authorization Key"".</li><li class=""css-cvpopp""><strong>App Service Plan</strong>: A set of virtual machines that can host multiple Azure App Service deployments.</li><li class=""css-cvpopp""><strong>Azure App Service:</strong> A computing platform in Azure that can host various types of applications.</li><li class=""css-cvpopp""><strong>Azure App Insights:</strong> An extensible Application Performance Management (APM) service for developers and DevOps professionals. In our case, it can help to monitor the performance of the QnA Maker hosted in our Azure App Service.</li><li class=""css-cvpopp""><strong>Azure Cognitive Search:</strong> Previously known as Azure Search, Azure Cognitive Search is a cloud search service that can index data and provides rapid search capabilities.</li><li class=""css-cvpopp""><strong>Chit-chat:</strong> An additional dataset of common conversational scenarios that can be added to a knowledge base.</li><li class=""css-cvpopp""><strong>Production Environment</strong>: A deployment of a copy of a dataset of applications that customers use instead of being used by developers for testing.</li></ul></div>']",[],https://www.youtube.com/embed/vO3Ptflr3t4
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.6  Creating a Bot For Your Knowledge Base,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Now that we have a strong knowledge base, it is time to move forward and create a bot.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 1 -  Create a Bot Service</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Our first step will be to publish our model to a production slot.</p>\n<p class=""chakra-text css-o3oz8b"">Please switch to the <strong>Publish</strong> page from the top menu and click <strong>Publish</strong> button to move our knowledge base from testing to production.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once you see the <strong>Success</strong> message, select <strong>Create Bot</strong>. This will navigate you to the Azure Portal directly to the Web App Bot Creating screen. The portal might ask for your credentials. In that case, please use your Azure Subscription credentials to log in to the portal.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Fill in the information:</p>\n<ol role=""list"" class=""css-13a5a39""><li class=""css-cvpopp"">Give a <strong>unique name</strong> to your bot</li><li class=""css-cvpopp"">Select your <strong>subscription</strong> and the <strong>resource group</strong> of your choice.</li><li class=""css-cvpopp"">Select the <strong>location</strong> for the Bot resource deployment. Preferably this should be within the same region as your QnA service.</li><li class=""css-cvpopp"">Set <strong>F0</strong> for the pricing tier.</li><li class=""css-cvpopp"">Set an <strong>application name</strong> for your bot. This is used to create the API endpoint for Bot APIs used by the Bot client applications.</li><li class=""css-cvpopp"">Select C# or Node.js for your <strong>SDK language</strong>. This is useful if you decide to evolve your bode and modify its code. In our case, we are selecting C#.</li><li class=""css-cvpopp""><strong>QnA Auth Key</strong> is how your bot will access your knowledge base. This is set up by default. No action is needed.</li><li class=""css-cvpopp""><strong>App Service Plan</strong> is where your bot application will be hosted. This setting should come preconfigured as default. If not, feel free to select the same App Service Plan you used during the QnA Maker service.</li><li class=""css-cvpopp"">Set Application Insights <strong>Off</strong>. We do not need application performance monitoring for now.</li><li class=""css-cvpopp"">Select <strong>Create</strong></li></ol></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once the deployment is complete, you will see a notification on the Azure Portal on the top bar. Select the notification bell to open the notifications panel. Once you see the ""<strong>Deployment succeeded</strong>"" message, you can go to the resource and test the Bot.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Part 2 -  Test the Bot Service</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Notice the <strong>Deployment succeeded</strong> message, and select <strong>Go to resource</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Once you are on the Web App Bot page, switch to the <strong>Test in Web Chat</strong> section from the left menu. You will notice the bot welcome you!</p>\n<p class=""chakra-text css-o3oz8b"">Let\'s ask <code class=""chakra-code css-1u83yg1"">Hi, I was wondering if I needed to purchase textbooks for your course?</code> and see the response.</p>\n<p class=""chakra-text css-o3oz8b"">Just to compare our question and the answer with the one in the knowledge base here is what we had in the knowledge base:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You can see how different our question is from the one stored in the knowledge base. This is proof that Natural Language Processing is helping our bot to extract intent and response.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">To add multiple delivery channels for your bot, you can switch to the <strong>Channels</strong> section from the left menu. Here, you can see the list of available channels you can use from Azure Bot Service.</p>\n<p class=""chakra-text css-o3oz8b"">Select <strong>Edit</strong> on the top right to get an HTML snipped that you can use to embed your bot to a website.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Copy one of the <strong>secret keys (1)</strong> and paste it into the <strong>YOUR_SECRET_HERE</strong> section in the embed code <strong>(2)</strong>. You can use the final version of the embed code to embed your chatbot to a website of your choice. To manage multiple chatbots on numerous web site, you can create a separate logical website by selecting <strong>+Add new site (3)</strong>. When you are done, select <strong>Done (4)</strong>.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional Resources</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Once you have your initial version of your bot make sure you take a look at the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/qnamaker/concepts/best-practices?tabs=v1"">best practices of a QnA Maker knowledge base<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp"">If you are interested in building more customized Bots visit <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/composer/introduction"">Intro to Bot Composer<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a></li><li class=""css-cvpopp"">You can access the Chit-Chat datasets <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://docs.microsoft.com/en-us/azure/cognitive-services/qnamaker/concepts/best-practices?tabs=v1#choosing-a-personality"">here<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li></ul></div>']","['https://video.udacity-data.com/topher/2021/March/6040a1b6_qna-textbooks-qa/qna-textbooks-qa.png', 'https://video.udacity-data.com/topher/2021/March/6040a46a_bot-configure-web-chat/bot-configure-web-chat.png']",https://www.youtube.com/embed/YuSf7ePQoMI
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.7  Quizzes: Implementing Conversational AI with Azure,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">In what order would you take on the tasks below to create a chatbot? All tasks are needed.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Edit the Knowledge Base</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a QnA Maker Knowledge Base</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Create a Bot for the Knowledge Base</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Train and Test the Knowledge Base</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Access the Bot through a Channel</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Task 1</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Task 2</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Task 3</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Task 4</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Task 5</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What functionality of Azure QnA Maker helps your bot respond to common conversational scenarios?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Search</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Chit-Chat</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">App Insights</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Which one is <strong>NOT</strong> an Azure resource needed to <strong>deploy</strong> your QnA Maker bot in Azure?</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">App Service Plan</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Search</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">QnA Maker</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Cognitive Service</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">What services can be used when building chatbots?</p>\n<p class=""chakra-text css-o3oz8b"">(Select all that apply.)</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Language Understanding</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Speech</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Computer Vision</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Azure Cognitive Search</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.8  Exercise: Create A Bot,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">You have learned how to create a <strong>QnA Maker Knowledge Base</strong> and a <strong>Chatbot</strong>. Now it is time to combine all that. This exercise will help you understand how all the pieces come together and how you can build a chatbot from scratch.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Download the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://github.com/udacity/AI_fundamentals/blob/main/lesson-6/exercise_faq.docx"">exercise FAQ<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a> file and use it as your source to build a bot from scratch, beginning from the knowledge base to the bot service. In the end, test your bot in the Web Chat interface on the Azure Portal.</p></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.9  Solution: Create A Bot,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">This is just amazing!</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">You have created a knowledge base from a FAQ file, trained a model, and published it to production.</li><li class=""css-cvpopp"">You have built a bot and hosted it on Azure.</li><li class=""css-cvpopp"">Finally, you extracted an embed code to be placed anywhere on the web to get your bot to meet real people.</li></ul>\n<p class=""chakra-text css-o3oz8b"">Think about how easy it was and how many NLP capabilities your bot is using.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Before Going Live</h2></div>', '<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Our bot is in a production deployment, which means you are ready to share it with people and host it on your website. However, when you get a lot of visitors, you might need to scale your Azure resources. Remember, when we were asked for a Pricing Tier during our lesson, we have always picked the Free tier. Every Azure service has various limitations for its free tier. Ensure you visit the <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://azure.microsoft.com/en-us/pricing/"">Azure Pricing page <span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>to learn more about how you might need to scale the underlying service to accommodate all your bot users.</p></div>']",[],https://www.youtube.com/embed/zeNGH04S5gI
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.10  Edge Cases,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">At times AI can be a black box where we do not know what it does and how it does it. We know the question we have at hand, and we can acknowledge the result, but we do not see the path it took AI to get to the result. The most common causes for a black box are artificial neural networks such as Deep Learning that comes with many hidden layers in their decision-making process that we can\'t follow. In this lesson, we are using a knowledge base to feed our chatbot, so we are safe.</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Additional Resources</h2>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Take a read about <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/"">the learnings of Microsoft <span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>from the Tay experiment.</li><li class=""css-cvpopp"">Interested in learning more about Microsoft\'s Responsible AI principles? <a target=""_blank"" rel=""noopener noreferrer"" class=""chakra-link css-190botj"" href=""https://www.microsoft.com/en-us/ai/responsible-ai"">Visit the dedicated Responsible AI site<span class=""chakra-text css-1lktits"">(opens in a new tab)</span></a>.</li></ul></div>']",[],https://www.youtube.com/embed/LHESkgIM-0Q
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.11  Lesson Review,"['<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Summary</h2>\n<p class=""chakra-text css-o3oz8b"">Isn\'t it amazing to be able to build a support chatbot in such a short time? Remember the number of AI technologies that can be used to evolve your chatbot.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">Speech Recognition</li><li class=""css-cvpopp"">Natural Language Understanding</li><li class=""css-cvpopp"">Text-to-Speech</li></ul>\n<p class=""chakra-text css-o3oz8b"">We looked into every one of these technologies, and our chatbot is using many of them. Don\'t leave your chatbot alone just because the lesson is finished. Take it forward, and add more capabilities to your chatbot to make it more intelligent with every improvement. Enjoy!</p></div>']",[],https://www.youtube.com/embed/18Nc8fCI3iw
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.12  Glossary,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">For your reference, here are all the new terms we introduced in this lesson:</p></div>', '<div class=""ureact-markdown css-tc5hjw""><ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp""><strong>Auth Key</strong>: The short version of ""Authorization Key"".</li><li class=""css-cvpopp""><strong>App Service Plan</strong>: A set of virtual machines that can host multiple Azure App Service deployments.</li><li class=""css-cvpopp""><strong>Azure App Service:</strong> A computing platform in Azure that can host various types of applications.</li><li class=""css-cvpopp""><strong>Azure App Insights:</strong> An extensible Application Performance Management (APM) service for developers and DevOps professionals. In our case, it can help to monitor the performance of the QnA Maker hosted in our Azure App Service.</li><li class=""css-cvpopp""><strong>Azure Cognitive Search:</strong> Previously known as Azure Search, Azure Cognitive Search is a cloud search service that can index data and provides rapid search capabilities.</li><li class=""css-cvpopp""><strong>Bot:</strong> An AI-infused agent that can receive user input and provide an appropriate response.</li><li class=""css-cvpopp""><strong>Channel:</strong> A channel for Azure Bot Service is a bridge between the user and the bot that defines the communication method and the communication path. For example, Direct Line of Speech channel enables speech-based communication, whereas Web Chat channel provides a web interface for users to communicate with the Bot.</li><li class=""css-cvpopp""><strong>Chit-chat:</strong> An additional dataset of common conversational scenarios that can be added to a knowledge base.</li><li class=""css-cvpopp""><strong>Knowledge-base:</strong> A knowledge base is a list of questions and answers that a bot can use to extract information.</li><li class=""css-cvpopp""><strong>Production Environment</strong>: A deployment of a copy of a dataset of applications that customers use instead of being used by developers for testing.</li></ul></div>']",[],
AI Fundamentals,AI Fundamentals,Lesson 7: Conversational AI,7.13  Course Conclusion,"['<div class=""ureact-markdown css-tc5hjw""><p class=""chakra-text css-o3oz8b"">Course outline</p></div>', '<div class=""ureact-markdown css-tc5hjw""><h2 class=""chakra-heading css-fz7yxd"">Congratulations!</h2>\n<p class=""chakra-text css-o3oz8b"">You have done it. You have complete all the lessons. Here is a list of skills and capabilities you have added to your AI and ML skill set.</p>\n<ul role=""list"" class=""css-19qh3zo""><li class=""css-cvpopp"">You can now evaluate responsibility requirements for AI and ML workloads in the context of specific use cases. Remember the relationship between AI and ML and how Responsible AI fits in.</li><li class=""css-cvpopp"">You are now able to train and deploy ML Models with Azure ML no code experiences. AutoML and Azure ML Designer were the highlights.</li><li class=""css-cvpopp"">Choosing the proper Azure tool for various Computer Vision problems is another skill you mastered. Computer Vision, Custom Vision, Face Service, and Form Recognizer were the tools we experimented with.</li><li class=""css-cvpopp"">Picking the NLP services from Azure to do Text Analytics, implementing Language Understanding, recognizing speech, synthesizing speech, and translating speech come naturally now.</li><li class=""css-cvpopp"">You can now use the QnA Maker service and Azure Bot service to address common Conversational AI problems, including web chatbots, voice bots, and digital assistants.</li></ul>\n<p class=""chakra-text css-o3oz8b"">Now go ahead, and start using your new skills implementing new AI and ML capabilities into your solutions!</p></div>']",['https://video.udacity-data.com/topher/2021/March/6047bfbd_screen-shot-2021-03-09-at-10.33.58-am/screen-shot-2021-03-09-at-10.33.58-am.png'],https://www.youtube.com/embed/QjG5K-vF_DI
